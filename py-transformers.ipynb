{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet \"pytorch-lightning >=2.0,<2.6\" \"matplotlib\" \"torch >=1.8.1,<2.8\" \"seaborn\" \"torchmetrics >=1.0,<1.8\" \"numpy <3.0\" \"torchvision\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import math\n",
    "import os\n",
    "import urllib.request\n",
    "from functools import partial\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "plt.set_cmap(\"cividis\")\n",
    "%matplotlib inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_orig()\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q:torch.Tensor, k:torch.Tensor, v:torch.Tensor, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attn_logits, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "pl.seed_everything(42)\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3 * embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x:torch.Tensor, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv:torch.Tensor = self.qkv_proj(x)\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)  # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3)  # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"EncoderBlock.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Dimensionality of the input\n",
    "            num_heads: Number of heads to use in the attention block\n",
    "            dim_feedforward: Dimensionality of the hidden layer in the MLP\n",
    "            dropout: Dropout probability to use in the dropout layers\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim),\n",
    "        )\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # MLP part\n",
    "        linear_out = self.linear_net(x)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for layer in self.layers:\n",
    "            _, attn_map = layer.self_attn(x, mask=mask, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = layer(x)\n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"Positional Encoding.\n",
    "\n",
    "        Args:\n",
    "            d_model: Hidden dimensionality of the input.\n",
    "            max_len: Maximum length of a sequence to expect.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer(\"pe\", pe, persistent=False)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "d_model = 32\n",
    "pe = torch.zeros(max_len, d_model)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "pe = pe.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe[:, :20].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPredictor(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        model_dim,\n",
    "        num_classes,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        lr,\n",
    "        warmup,\n",
    "        max_iters,\n",
    "        dropout=0.0,\n",
    "        input_dropout=0.0,\n",
    "    ):\n",
    "        \"\"\"TransformerPredictor.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Hidden dimensionality of the input\n",
    "            model_dim: Hidden dimensionality to use inside the Transformer\n",
    "            num_classes: Number of classes to predict per sequence element\n",
    "            num_heads: Number of heads to use in the Multi-Head Attention blocks\n",
    "            num_layers: Number of encoder blocks to use.\n",
    "            lr: Learning rate in the optimizer\n",
    "            warmup: Number of warmup steps. Usually between 50 and 500\n",
    "            max_iters: Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
    "            dropout: Dropout to apply inside the model\n",
    "            input_dropout: Dropout to apply on the input features\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self._create_model()\n",
    "\n",
    "    def _create_model(self):\n",
    "        # Input dim -> Model dim\n",
    "        self.input_net = nn.Sequential(\n",
    "            nn.Dropout(self.hparams.input_dropout), nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n",
    "        )\n",
    "        # Positional encoding for sequences\n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(\n",
    "            num_layers=self.hparams.num_layers,\n",
    "            input_dim=self.hparams.model_dim,\n",
    "            dim_feedforward=2 * self.hparams.model_dim,\n",
    "            num_heads=self.hparams.num_heads,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "        # Output classifier per sequence element\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
    "            nn.LayerNorm(self.hparams.model_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.hparams.dropout),\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input features of shape [Batch, SeqLen, input_dim]\n",
    "            mask: Mask to apply on the attention outputs (optional)\n",
    "            add_positional_encoding: If True, we add the positional encoding to the input.\n",
    "                                      Might not be desired for some tasks.\n",
    "        \"\"\"\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        x = self.output_net(x)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
    "        \"\"\"Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
    "\n",
    "        Input arguments same as the forward pass.\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
    "        return attention_maps\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "        # We don't return the lr scheduler because we need to apply it per iteration, not per epoch\n",
    "        self.lr_scheduler = CosineWarmupScheduler(\n",
    "            optimizer, warmup=self.hparams.warmup, max_iters=self.hparams.max_iters\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def optimizer_step(self, *args, **kwargs):\n",
    "        super().optimizer_step(*args, **kwargs)\n",
    "        self.lr_scheduler.step()  # Step per iteration\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDataset(data.Dataset):\n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "\n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        labels = torch.flip(inp_data, dims=(0,))\n",
    "        return inp_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = partial(ReverseDataset, 10, 16)\n",
    "train_loader = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader = data.DataLoader(dataset(1000), batch_size=128)\n",
    "test_loader = data.DataLoader(dataset(10000), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_data, labels = train_loader.dataset[0]\n",
    "print(\"Input data:\", inp_data)\n",
    "print(\"Labels:    \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversePredictor(TransformerPredictor):\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        # Fetch data and transform categories to one-hot vectors\n",
    "        inp_data, labels = batch\n",
    "        inp_data = F.one_hot(inp_data, num_classes=self.hparams.num_classes).float()\n",
    "\n",
    "        # Perform prediction and calculate loss and accuracy\n",
    "        preds = self.forward(inp_data, add_positional_encoding=True)\n",
    "        loss = F.cross_entropy(preds.view(-1, preds.size(-1)), labels.view(-1))\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        # Logging\n",
    "        self.log(f\"{mode}_loss\", loss)\n",
    "        self.log(f\"{mode}_acc\", acc)\n",
    "        return loss, acc\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reverse(**kwargs):\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"mps\",\n",
    "        devices=1,\n",
    "        max_epochs=10,\n",
    "        gradient_clip_val=5,\n",
    "    )\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "    \n",
    "    model = ReversePredictor(max_iters=trainer.max_epochs * len(train_loader), **kwargs)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    model = model.to(device)\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_model, reverse_result = train_reverse(\n",
    "    input_dim=train_loader.dataset.num_categories,\n",
    "    model_dim=32,\n",
    "    num_heads=1,\n",
    "    num_classes=train_loader.dataset.num_categories,\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    lr=5e-4,\n",
    "    warmup=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Val accuracy:  %4.2f%%\" % (100.0 * reverse_result[\"val_acc\"]))\n",
    "print(\"Test accuracy: %4.2f%%\" % (100.0 * reverse_result[\"test_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"datasets/ml-32m\"\n",
    "\n",
    "ratings_path = os.path.join(folder, 'ratings.csv')\n",
    "movies_path = os.path.join(folder, 'movies.csv')\n",
    "tags_path = os.path.join(folder, 'tags.csv')\n",
    "\n",
    "rating_column_names = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "movies_column_names = ['movieId', 'title', 'genres']\n",
    "tags_column_names = ['userId', 'movieId', 'tag', 'timestamp']\n",
    "\n",
    "df_ratings = pd.read_csv(ratings_path, sep=',', names=rating_column_names, dtype={'userId':'int32', 'movieId':'int32', 'rating':float, 'timestamp':'int64'}, header=0)\n",
    "df_movies = pd.read_csv(movies_path, sep=',', names=movies_column_names, dtype={'movieId':'int32', 'title':'object', 'genres':'object'}, header=0)\n",
    "df_tags = pd.read_csv(tags_path, sep=',', names=tags_column_names, dtype={'userId':'int32', 'movieId':'int32', 'tag':'object', 'timestamp':'int64'}, header=0)\n",
    "\n",
    "df_ratings.dropna(inplace=True, subset=['userId', 'movieId', 'rating'])\n",
    "df_movies.dropna(inplace=True, subset=['movieId', 'title', 'genres'])\n",
    "df_tags.dropna(inplace=True, subset=['userId', 'movieId', 'tag'])\n",
    "df_tags.drop(columns=[\"userId\",\"timestamp\"], inplace=True)\n",
    "\n",
    "# Extract movie genres\n",
    "df_movies['genres'] = df_movies['genres'].apply(lambda x: x.lower().split('|'))\n",
    "\n",
    "# Extract movie year from title\n",
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "def remove_stop(x):\n",
    "    out = []\n",
    "    for y in x:\n",
    "        if len(y) > 0 and y not in stopwords:\n",
    "            out += [y]\n",
    "    return out\n",
    "\n",
    "def flatten_lists(x):\n",
    "    out = set()\n",
    "    for y in x:\n",
    "        out.update(y.split(\" \"))\n",
    "    out = list(out)\n",
    "    return out\n",
    "\n",
    "df_movies['movie_year'] = df_movies['title'].str.extract(r'\\((\\d{4})\\)').fillna(\"2025\").astype('int')\n",
    "\n",
    "df_movies['title'] = df_movies['title'].str.replace(r'\\((\\d{4})\\)', '', regex=True)\n",
    "df_movies['title'] = df_movies['title'].str.replace(r'[^a-zA-Z0-9\\s]+', '', regex=True)\n",
    "df_movies['title'] = df_movies['title'].apply(lambda x: x.strip().lower().split(\" \"))\n",
    "df_movies['title'] = df_movies['title'].apply(lambda x: remove_stop(x))\n",
    "\n",
    "df_tags['tag'] = df_tags['tag'].str.replace(r'[^a-zA-Z0-9\\s]+', '', regex=True)\n",
    "df_tags['tag'] = df_tags['tag'].apply(lambda x: x.strip().lower())\n",
    "df_tags = df_tags.groupby(\"movieId\").agg(set).reset_index()\n",
    "df_tags['tag'] = df_tags['tag'].apply(list)\n",
    "df_tags['tag'] = df_tags['tag'].apply(lambda x: flatten_lists(x))\n",
    "df_tags['tag'] = df_tags['tag'].apply(lambda x: remove_stop(x))\n",
    "df_tags['tag'] = df_tags['tag'].astype(\"object\")\n",
    "\n",
    "df_movies = df_movies.merge(df_tags, on=['movieId'], how='left')\n",
    "df_movies[\"tag\"] = df_movies[\"tag\"].fillna({i: [\"\"] for i in df_movies.index})\n",
    "df_movies[\"description\"] = df_movies[\"title\"] + df_movies[\"tag\"]\n",
    "df_movies.drop(columns=[\"tag\"], inplace=True)\n",
    "df_movies.drop(columns=[\"title\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>944249077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>944250228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>943230976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>5.0</td>\n",
       "      <td>944249077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>5.0</td>\n",
       "      <td>943228858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>1944</td>\n",
       "      <td>2.0</td>\n",
       "      <td>943231120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1952</td>\n",
       "      <td>4.0</td>\n",
       "      <td>944253272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>1960</td>\n",
       "      <td>1.0</td>\n",
       "      <td>943231236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>944250182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>1965</td>\n",
       "      <td>3.0</td>\n",
       "      <td>943228697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  rating  timestamp\n",
       "0        1       17     4.0  944249077\n",
       "1        1       25     1.0  944250228\n",
       "2        1       29     2.0  943230976\n",
       "3        1       30     5.0  944249077\n",
       "4        1       32     5.0  943228858\n",
       "..     ...      ...     ...        ...\n",
       "95       1     1944     2.0  943231120\n",
       "96       1     1952     4.0  944253272\n",
       "97       1     1960     1.0  943231236\n",
       "98       1     1961     1.0  944250182\n",
       "99       1     1965     3.0  943228697\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>genres</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[adventure, animation, children, comedy, fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[toy, story, jumping, toolbox, almost, sunligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[adventure, children, fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[jumanji, female, libra, beach, film, laundry,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[comedy, romance]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[grumpier, old, men, duringcreditsstinger, act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[comedy, drama, romance]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[waiting, exhale, single, relationship, novel,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[comedy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[father, bride, part, ii, gynecologist, 2021, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>97</td>\n",
       "      <td>[crime, drama]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[hate, haine, la, arabian, oneself, hood, doll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>98</td>\n",
       "      <td>[action, thriller]</td>\n",
       "      <td>1994</td>\n",
       "      <td>[shopping, law, directorial, debut, jude, want]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>99</td>\n",
       "      <td>[documentary]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[heidi, fleiss, hollywood, madam, narration, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>100</td>\n",
       "      <td>[drama, thriller]</td>\n",
       "      <td>1996</td>\n",
       "      <td>[city, hall, drug, investigation, clv, police,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>101</td>\n",
       "      <td>[adventure, comedy, crime, romance]</td>\n",
       "      <td>1996</td>\n",
       "      <td>[bottle, rocket, wistful, robbery, psychiatric...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    movieId                                             genres  movie_year  \\\n",
       "0         1  [adventure, animation, children, comedy, fantasy]        1995   \n",
       "1         2                     [adventure, children, fantasy]        1995   \n",
       "2         3                                  [comedy, romance]        1995   \n",
       "3         4                           [comedy, drama, romance]        1995   \n",
       "4         5                                           [comedy]        1995   \n",
       "..      ...                                                ...         ...   \n",
       "95       97                                     [crime, drama]        1995   \n",
       "96       98                                 [action, thriller]        1994   \n",
       "97       99                                      [documentary]        1995   \n",
       "98      100                                  [drama, thriller]        1996   \n",
       "99      101                [adventure, comedy, crime, romance]        1996   \n",
       "\n",
       "                                          description  \n",
       "0   [toy, story, jumping, toolbox, almost, sunligh...  \n",
       "1   [jumanji, female, libra, beach, film, laundry,...  \n",
       "2   [grumpier, old, men, duringcreditsstinger, act...  \n",
       "3   [waiting, exhale, single, relationship, novel,...  \n",
       "4   [father, bride, part, ii, gynecologist, 2021, ...  \n",
       "..                                                ...  \n",
       "95  [hate, haine, la, arabian, oneself, hood, doll...  \n",
       "96    [shopping, law, directorial, debut, jude, want]  \n",
       "97  [heidi, fleiss, hollywood, madam, narration, p...  \n",
       "98  [city, hall, drug, investigation, clv, police,...  \n",
       "99  [bottle, rocket, wistful, robbery, psychiatric...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ratings(df:pd.DataFrame):\n",
    "    df2 = df[[\"userId\", \"rating\"]].groupby(by=[\"userId\"]).agg(mean_user_rating=('rating', 'mean'), std_user_rating=('rating', 'std'))\n",
    "    df = df.merge(df2, on=[\"userId\"], how=\"inner\")\n",
    "    df[\"normalized_rating\"] = (df[\"rating\"] - df[\"mean_user_rating\"])/df[\"std_user_rating\"]\n",
    "    df[\"normalized_rating\"] = df[\"normalized_rating\"].fillna(df[\"rating\"])\n",
    "    df.drop(columns=[\"mean_user_rating\", \"std_user_rating\", \"rating\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = normalize_ratings(df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings['label'] = [0 for _ in range(len(df_ratings))]\n",
    "df_ratings['label'] = np.where(df_ratings[\"normalized_rating\"] > 0, 1, df_ratings['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>normalized_rating</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>944249077</td>\n",
       "      <td>0.304372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>944250228</td>\n",
       "      <td>-1.646377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>943230976</td>\n",
       "      <td>-0.996127</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>944249077</td>\n",
       "      <td>0.954622</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>943228858</td>\n",
       "      <td>0.954622</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>1944</td>\n",
       "      <td>943231120</td>\n",
       "      <td>-0.996127</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1952</td>\n",
       "      <td>944253272</td>\n",
       "      <td>0.304372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>1960</td>\n",
       "      <td>943231236</td>\n",
       "      <td>-1.646377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1961</td>\n",
       "      <td>944250182</td>\n",
       "      <td>-1.646377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>1965</td>\n",
       "      <td>943228697</td>\n",
       "      <td>-0.345878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  timestamp  normalized_rating  label\n",
       "0        1       17  944249077           0.304372      1\n",
       "1        1       25  944250228          -1.646377      0\n",
       "2        1       29  943230976          -0.996127      0\n",
       "3        1       30  944249077           0.954622      1\n",
       "4        1       32  943228858           0.954622      1\n",
       "..     ...      ...        ...                ...    ...\n",
       "95       1     1944  943231120          -0.996127      0\n",
       "96       1     1952  944253272           0.304372      1\n",
       "97       1     1960  943231236          -1.646377      0\n",
       "98       1     1961  944250182          -1.646377      0\n",
       "99       1     1965  943228697          -0.345878      0\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df:pd.DataFrame, min_rated=10, test_ratio=0.8, val_ratio=0.8):\n",
    "    print(\"Splitting data into train test and validation...\")\n",
    "    # Split data into training, testing and validation\n",
    "    df = df.sort_values(by='timestamp')\n",
    "    df2 = df[[\"userId\", \"movieId\"]].groupby(by=[\"userId\"]).agg(list).reset_index()\n",
    "\n",
    "    # Filter all user_ids who have rated more than 'min_rated' movies\n",
    "    df2 = df2[df2.movieId.apply(len) > min_rated]\n",
    "    df = df.merge(df2, on=[\"userId\"], how=\"inner\", suffixes=(\"\", \"_right\"))\n",
    "    df.drop(columns=['movieId_right'], inplace=True)\n",
    "\n",
    "    n = df.shape[0]\n",
    "    m = int(test_ratio*n)\n",
    "\n",
    "    df_train_val = df[:m]\n",
    "    df_test = df[m:]\n",
    "\n",
    "    k = int(val_ratio*m)\n",
    "    df_train = df_train_val[:k]\n",
    "    df_val = df_train_val[k:]\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train test and validation...\n"
     ]
    }
   ],
   "source": [
    "df_ratings_train, df_ratings_val, df_ratings_test = split_train_test(df_ratings, min_rated=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_train.sort_values(by=[\"userId\", \"timestamp\"], inplace=True)\n",
    "df_ratings_val.sort_values(by=[\"userId\", \"timestamp\"], inplace=True)\n",
    "df_ratings_test.sort_values(by=[\"userId\", \"timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x, vocab):\n",
    "    if isinstance(x, list):\n",
    "        out = []\n",
    "        for y in x:\n",
    "            out += [vocab[y]] if y in vocab else [0]\n",
    "        return out\n",
    "    else:\n",
    "        return vocab[x] if x in vocab else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_encoding(df:pd.DataFrame, col:str, max_vocab_size=1000):\n",
    "    all_vals = df[col].tolist()\n",
    "    unique_vals = {}\n",
    "\n",
    "    if len(all_vals) > 0 and isinstance(all_vals[0], list):\n",
    "        for v in all_vals:\n",
    "            for x in v:\n",
    "                if x not in unique_vals:\n",
    "                    unique_vals[x] = 0\n",
    "                unique_vals[x] += 1\n",
    "    else:\n",
    "        for x in all_vals:\n",
    "            if x not in unique_vals:\n",
    "                unique_vals[x] = 0\n",
    "            unique_vals[x] += 1\n",
    "    \n",
    "    unique_vals = sorted(unique_vals.items(), key=lambda item: item[1], reverse=True)\n",
    "    unique_vals = dict(unique_vals[:min(max_vocab_size, len(unique_vals))])\n",
    "    unique_vals = sorted(unique_vals.keys())\n",
    "    vocab = {unique_vals[i] : i+1 for i in range(len(unique_vals))}\n",
    "        \n",
    "    df[col] = df[col].apply(lambda x: transform(x, vocab))\n",
    "    return df[col], vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId\n",
      "movieId\n",
      "description\n",
      "genres\n",
      "movie_year\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "max_vocab_size = {'userId':1e100, 'movieId':1e100, 'description':1e5, 'genres':100, 'movie_year':1e100}\n",
    "\n",
    "for col in ['userId', 'movieId']:\n",
    "    print(col)\n",
    "    df_ratings_train[col], v = categorical_encoding(df_ratings_train, col, max_vocab_size[col])\n",
    "    vocabulary[col] = v\n",
    "\n",
    "for col in ['description', 'genres', 'movie_year']:\n",
    "    print(col)\n",
    "    df_movies[col], v = categorical_encoding(df_movies, col, max_vocab_size[col])\n",
    "    vocabulary[col] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId\n",
      "movieId\n"
     ]
    }
   ],
   "source": [
    "df_ratings_val = df_ratings_val.reset_index()\n",
    "for col in ['userId', 'movieId']:\n",
    "    print(col)\n",
    "    df_ratings_val[col] = df_ratings_val[col].apply(lambda x: transform(x, vocabulary[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId\n",
      "movieId\n"
     ]
    }
   ],
   "source": [
    "df_ratings_test = df_ratings_test.reset_index()\n",
    "for col in ['userId', 'movieId']:\n",
    "    print(col)\n",
    "    df_ratings_test[col] = df_ratings_test[col].apply(lambda x: transform(x, vocabulary[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_user_features(df:pd.DataFrame, max_hist=20):\n",
    "\tdf[\"seq_id\"] = list(range(df.shape[0]))\n",
    "\tdf2 = df[[\"seq_id\", \"userId\", \"movieId\", \"normalized_rating\", \"timestamp\"]].sort_values(by=[\"userId\", \"timestamp\"])\n",
    "\t\n",
    "\tdf2 = df2[[\"userId\", \"movieId\", \"normalized_rating\", \"seq_id\"]].groupby(by=[\"userId\"]).agg(list).reset_index()\n",
    "\tdf2.rename(columns={\"movieId\":\"prev_movie_ids\", \"normalized_rating\":\"prev_ratings\", \"seq_id\":\"prev_seq_ids\"}, inplace=True)\n",
    "\n",
    "\tuser_ids = []\n",
    "\tp_m_ids = []\n",
    "\tp_r_ids = []\n",
    "\tp_seq_ids = []\n",
    "\n",
    "\tfor i in range(df2.shape[0]):\n",
    "\t\tseq_id = df2.loc[i, \"prev_seq_ids\"]\n",
    "\t\tu_id   = df2.loc[i, \"userId\"]\n",
    "\t\tm_ids  = df2.loc[i, \"prev_movie_ids\"]\n",
    "\t\tr_ids  = df2.loc[i, \"prev_ratings\"]\n",
    "\n",
    "\t\tfor j in range(len(m_ids)):\n",
    "\t\t\tuser_ids += [u_id]\n",
    "\t\t\tp_seq_ids += [seq_id[j]]\n",
    "\t\t\tp_m_ids += [m_ids[:j][-max_hist:]] if j > 0 else [[]]\n",
    "\t\t\tp_r_ids += [r_ids[:j][-max_hist:]] if j > 0 else [[]]\n",
    "\t\n",
    "\tdf3 = pd.DataFrame({\"userId\":user_ids, \"prev_movie_ids\":p_m_ids, \"prev_ratings\":p_r_ids, \"seq_id\":p_seq_ids})\n",
    "\tdf = df.merge(df3, on=[\"userId\", \"seq_id\"], how=\"left\")\n",
    "\tdf.drop(columns=[\"seq_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ml_32m_py\n",
    "import numpy as np\n",
    "\n",
    "importlib.reload(ml_32m_py)\n",
    "\n",
    "def get_historical_user_features_cpp(df:pd.DataFrame, max_hist=20):\n",
    "        user_ids = df['userId'].to_numpy().astype(np.uint32)\n",
    "        movie_ids = df['movieId'].to_numpy().astype(np.uint32)\n",
    "        ratings = df['normalized_rating'].to_numpy().astype(np.float32)\n",
    "        timestamps = df['timestamp'].to_numpy().astype(np.uint64)\n",
    "\n",
    "        prev_movie_ids, prev_ratings  = ml_32m_py.py_get_historical_features(user_ids, movie_ids, timestamps, ratings, df.shape[0], max_hist)\n",
    "\n",
    "        df[\"prev_movie_ids\"] = prev_movie_ids\n",
    "        df[\"prev_ratings\"] = prev_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_historical_user_features_cpp(df_ratings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_historical_user_features_cpp(df_ratings_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_historical_user_features_cpp(df_ratings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(vocabulary, \"vocabulary.pkl\")\n",
    "joblib.dump(df_ratings_train, \"df_ratings_train.pkl\")\n",
    "joblib.dump(df_ratings_val, \"df_ratings_val.pkl\")\n",
    "joblib.dump(df_ratings_test, \"df_ratings_test.pkl\")\n",
    "joblib.dump(df_movies, \"df_movies.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_parts = 32\n",
    "df_ratings_train[\"partition\"] = [random.randint(1, num_parts) for _ in range(len(df_ratings_train))]\n",
    "df_ratings_val[\"partition\"]   = [random.randint(1, num_parts) for _ in range(len(df_ratings_val))]\n",
    "df_ratings_test[\"partition\"]  = [random.randint(1, num_parts) for _ in range(len(df_ratings_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "out_path = \"parquet_dataset_ml_32m/\"\n",
    "if os.path.exists(out_path):\n",
    "    try:\n",
    "        shutil.rmtree(out_path)\n",
    "    except:\n",
    "        pass\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "joblib.dump(vocabulary, f\"parquet_dataset_ml_32m/vocabulary.pkl\")\n",
    "df_ratings_train.to_parquet(out_path + \"train/\", partition_cols=[\"partition\"])\n",
    "df_ratings_val.to_parquet(out_path + \"validation/\", partition_cols=[\"partition\"])\n",
    "df_ratings_test.to_parquet(out_path + \"test/\", partition_cols=[\"partition\"])\n",
    "df_movies.to_parquet(out_path + \"movies.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = joblib.load(\"df_movies.pkl\")\n",
    "vocabulary = joblib.load(\"vocabulary.pkl\")\n",
    "df_ratings_train = joblib.load(\"df_ratings_train.pkl\")\n",
    "n = df_ratings_train.shape[0]\n",
    "m = df_ratings_train.shape[1]\n",
    "columns = df_ratings_train.columns\n",
    "# del df_ratings_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[1, 2997, 943226846, 0.3043722566393317, 1, list([]), list([])],\n",
       "        [1, 2966, 943226846, -1.6463772063672941, 0, list([2997]),\n",
       "         list([0.30437225103378296])],\n",
       "        [1, 2890, 943226916, 0.3043722566393317, 1, list([2997, 2966]),\n",
       "         list([0.30437225103378296, -1.6463772058486938])],\n",
       "        [1, 3078, 943226986, -0.9961273853650855, 0,\n",
       "         list([2997, 2966, 2890]),\n",
       "         list([0.30437225103378296, -1.6463772058486938, 0.30437225103378296])],\n",
       "        [1, 2882, 943227458, -1.6463772063672941, 0,\n",
       "         list([2997, 2966, 2890, 3078]),\n",
       "         list([0.30437225103378296, -1.6463772058486938, 0.30437225103378296, -0.9961273670196533])],\n",
       "        [1, 541, 943227521, 0.9546220776415403, 1,\n",
       "         list([2997, 2966, 2890, 3078, 2882]),\n",
       "         list([0.30437225103378296, -1.6463772058486938, 0.30437225103378296, -0.9961273670196533, -1.6463772058486938])],\n",
       "        [1, 838, 943227632, 0.9546220776415403, 1,\n",
       "         list([2997, 2966, 2890, 3078, 2882, 541]),\n",
       "         list([0.30437225103378296, -1.6463772058486938, 0.30437225103378296, -0.9961273670196533, -1.6463772058486938, 0.9546220898628235])],\n",
       "        [1, 1136, 943228327, -1.6463772063672941, 0,\n",
       "         list([2997, 2966, 2890, 3078, 2882, 541, 838]),\n",
       "         list([0.30437225103378296, -1.6463772058486938, 0.30437225103378296, -0.9961273670196533, -1.6463772058486938, 0.9546220898628235, 0.9546220898628235])],\n",
       "        [1, 2396, 943228400, 0.9546220776415403, 1,\n",
       "         list([2997, 2966, 2890, 3078, 2882, 541, 838, 1136]),\n",
       "         list([0.30437225103378296, -1.6463772058486938, 0.30437225103378296, -0.9961273670196533, -1.6463772058486938, 0.9546220898628235, 0.9546220898628235, -1.6463772058486938])],\n",
       "        [1, 2918, 943228400, 0.3043722566393317, 1,\n",
       "         list([2997, 2966, 2890, 3078, 2882, 541, 838, 1136, 2396]),\n",
       "         list([0.30437225103378296, -1.6463772058486938, 0.30437225103378296, -0.9961273670196533, -1.6463772058486938, 0.9546220898628235, 0.9546220898628235, -1.6463772058486938, 0.9546220898628235])]],\n",
       "       dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings_train_mmap = np.memmap(\"df_ratings_train.mmap\", dtype=np.object_, mode=\"r\", shape=(n,m))\n",
    "df_ratings_train_mmap[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ratings_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df_ratings_train.shape[0]\n",
    "m = df_ratings_train.shape[1]\n",
    "columns = df_ratings_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_train_mmap = np.memmap(\"df_ratings_train.mmap\", dtype=np.object_, mode=\"w+\", shape=df_ratings_train.shape)\n",
    "df_ratings_train_mmap[:,:] = df_ratings_train.to_numpy()\n",
    "df_ratings_train_mmap.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_val_mmap = np.memmap(\"df_ratings_val.mmap\", dtype=np.object_, mode=\"w+\", shape=df_ratings_val.shape)\n",
    "df_ratings_val_mmap[:,:] = df_ratings_val.to_numpy()\n",
    "df_ratings_val_mmap.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_test_mmap = np.memmap(\"df_ratings_test.mmap\", dtype=np.object_, mode=\"w+\", shape=df_ratings_test.shape)\n",
    "df_ratings_test_mmap[:,:] = df_ratings_test.to_numpy()\n",
    "df_ratings_test_mmap.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ratings_train\n",
    "del df_ratings_val\n",
    "del df_ratings_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp -R parquet_dataset_ml_32m gs://r6-ae-dev-adperf-adintelligence-data/amondal/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amondal/recsys/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "import joblib\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(\"Device:\", device)\n",
    "\n",
    "def checkpoint(model:nn.Module, optimizer:torch.optim.Optimizer, filename):\n",
    "    torch.save({'optimizer':optimizer.state_dict(), 'model':model.state_dict()}, filename)\n",
    "    \n",
    "def load_model(filename):\n",
    "    chkpt = torch.load(filename, weights_only=False)\n",
    "    return chkpt['model'], chkpt['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q:torch.Tensor, k:torch.Tensor, v:torch.Tensor, mask=None):\n",
    "    d_k = q.size()[-1] # q,k,v : (batch, head, seq_len, embed_size_per_head)\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1)) # (batch, head, seq_len, seq_len)\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v) # (batch, head, seq_len, embed_size_per_head)\n",
    "    return values, attention\n",
    "\n",
    "\n",
    "def init_weights(x:nn.Linear):\n",
    "    with torch.no_grad():\n",
    "        nn.init.xavier_uniform_(x.weight)\n",
    "        x.bias.data.fill_(0)\n",
    "\t\t\n",
    "        \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model) # (seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model)) # (seq_len, d_model)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model)) # (seq_len, d_model)\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)   \n",
    "        return self.dropout(x)\n",
    "\t\n",
    "    \n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, input_dim:int, d_model: int, h: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        self.w_q = nn.Linear(input_dim, d_model) # Wq\n",
    "        self.w_k = nn.Linear(input_dim, d_model) # Wk\n",
    "        self.w_v = nn.Linear(input_dim, d_model) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model) # Wo\n",
    "\n",
    "        init_weights(self.w_q)\n",
    "        init_weights(self.w_k)\n",
    "        init_weights(self.w_v)\n",
    "        init_weights(self.w_o)\n",
    "\n",
    "    def forward(self, q_x:torch.Tensor, k_x:torch.Tensor, v_x:torch.Tensor, mask=None):\n",
    "        q:torch.Tensor = self.w_q(q_x) # (batch, seq_len, d_model)\n",
    "        k:torch.Tensor = self.w_k(k_x) # (batch, seq_len, d_model)\n",
    "        v:torch.Tensor = self.w_v(v_x) # (batch, seq_len, d_model)\n",
    "\n",
    "        q_h = q.reshape(q.shape[0], q.shape[1], self.h, self.d_k).transpose(1, 2) # (batch, head, seq_len, d_k)\n",
    "        k_h = k.reshape(k.shape[0], k.shape[1], self.h, self.d_k).transpose(1, 2) # (batch, head, seq_len, d_k)\n",
    "        v_h = v.reshape(v.shape[0], v.shape[1], self.h, self.d_k).transpose(1, 2) # (batch, head, seq_len, d_k)\n",
    "\n",
    "        attn_out, _ = attention(q_h, k_h, v_h, mask) # (batch, head, seq_len, embed_size_per_head)\n",
    "        attn_out = attn_out.transpose(1, 2) # (batch, seq_len, head, embed_size_per_head)\n",
    "        attn_out = attn_out.reshape(attn_out.shape[0], attn_out.shape[1], attn_out.shape[2]*attn_out.shape[3]) # (batch, seq_len, d_model)\n",
    "\n",
    "        return self.w_o(attn_out) # (batch, seq_len, d_model)\n",
    "    \n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttentionBlock(input_dim, input_dim, num_heads)\n",
    "\n",
    "        self.ffn_1 = nn.Linear(input_dim, dim_feedforward)\n",
    "        self.ffn_2 = nn.Linear(dim_feedforward, input_dim)\n",
    "\n",
    "        init_weights(self.ffn_1)\n",
    "        init_weights(self.ffn_2)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            self.ffn_1,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            self.ffn_2,\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.self_attn(x, x, x, mask=mask) # (batch, seq_len, input_dim)\n",
    "        x = x + self.dropout(attn_out) # (batch, seq_len, input_dim)\n",
    "        x = self.norm1(x) # (batch, seq_len, input_dim)\n",
    "\n",
    "        ffn_out = self.ffn(x) # (batch, seq_len, input_dim)\n",
    "        x = x + self.dropout(ffn_out) # (batch, seq_len, input_dim)\n",
    "        x = self.norm2(x) # (batch, seq_len, input_dim)\n",
    "\n",
    "        return x\n",
    "\t\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dim_feedforward, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(d_model, num_heads, dim_feedforward, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "        return x\n",
    "\t\n",
    "    \n",
    "class CrossFeatureLayer(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers,dropout=0.0) -> None:\n",
    "        super(CrossFeatureLayer, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.cross_layer_params = []\n",
    "        self.cross_layer_norms = []\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            h = nn.Linear(input_dim, input_dim)\n",
    "            init_weights(h)\n",
    "            self.cross_layer_params += [h]\n",
    "\n",
    "            g = nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.LayerNorm(input_dim)\n",
    "            )\n",
    "\n",
    "            self.cross_layer_norms += [g]\n",
    "\n",
    "        self.cross_layer_params = nn.ModuleList(self.cross_layer_params)\n",
    "        self.cross_layer_norms = nn.ModuleList(self.cross_layer_norms)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_initial = torch.Tensor(x) # (batch, ..., input_dim)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = x_initial*self.cross_layer_params[i](x) + x # (batch, ..., input_dim)\n",
    "            x = self.cross_layer_norms[i](x) # (batch, ..., input_dim)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieId:\n",
    "    movie_id_emb = None\n",
    "\n",
    "    def __new__(cls, movie_id_size, emb_size=512):\n",
    "        if cls.movie_id_emb is None:\n",
    "            cls.movie_id_emb = nn.Embedding(movie_id_size, emb_size, padding_idx=0)\n",
    "        return cls.movie_id_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_averaging(inp:torch.Tensor, emb_layer:nn.Module, padding_idx:int=0):\n",
    "    # inp : (batch, num_tokens)\n",
    "    embeddings = emb_layer(inp) # (batch, num_tokens, emb_size)\n",
    "    mask = (inp != padding_idx).float().unsqueeze(-1) # (batch, num_tokens, 1)\n",
    "    masked_embeddings = embeddings * mask # (batch, num_tokens, emb_size)\n",
    "    sum_embeddings = 1.0 + torch.sum(masked_embeddings, dim=1) # (batch, emb_size)\n",
    "    sequence_lengths = 1.0 + torch.sum(mask, dim=1) # (batch, 1) # prevents division by zero by + 1\n",
    "    averaged_embeddings = sum_embeddings / sequence_lengths # (batch, emb_size)\n",
    "    return averaged_embeddings\n",
    "\n",
    "class MovieEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            movie_id_size, \n",
    "            movie_desc_size,\n",
    "            movie_genres_size,\n",
    "            movie_year_size, \n",
    "            embedding_size, \n",
    "            dropout=0.0\n",
    "        ) -> None:\n",
    "        \n",
    "        super(MovieEncoder, self).__init__()\n",
    "        \n",
    "        self.movie_id_emb = MovieId(movie_id_size, 512)\n",
    "        self.movie_desc_emb = nn.Embedding(movie_desc_size, 1024, padding_idx=0)\n",
    "        self.movie_genres_emb = nn.Embedding(movie_genres_size, 8, padding_idx=0)\n",
    "        self.movie_year_emb = nn.Embedding(movie_year_size, 16, padding_idx=0)\n",
    "\n",
    "        self.fc_concat = nn.Linear(1560, embedding_size)\n",
    "        init_weights(self.fc_concat)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            self.fc_concat,\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(embedding_size)\n",
    "        )\n",
    "\n",
    "        self.cross_features = CrossFeatureLayer(1560, 3, 0.0)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            ids:torch.Tensor, \n",
    "            descriptions:torch.Tensor, \n",
    "            genres:torch.Tensor, \n",
    "            years:torch.Tensor\n",
    "        ):\n",
    "        id_emb = self.movie_id_emb(ids) # (batch, 512)\n",
    "        desc_emb = emb_averaging(descriptions, self.movie_desc_emb) # (batch, 1024)\n",
    "        genres_emb = emb_averaging(genres, self.movie_genres_emb) # (batch, 8)\n",
    "        years_emb = self.movie_year_emb(years) # (batch, 16)\n",
    "\n",
    "        movie_embedding = torch.concat([id_emb, desc_emb, genres_emb, years_emb], dim=-1) # (batch, 1560)\n",
    "        movie_embedding = self.cross_features(movie_embedding) + movie_embedding # (batch, 1560)\n",
    "        movie_embedding = self.fc(movie_embedding) # (batch, emb_size)\n",
    "\n",
    "        return movie_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            user_id_size, \n",
    "            movie_id_size,\n",
    "            embedding_size, \n",
    "            prev_rated_seq_len, \n",
    "            num_encoder_layers, \n",
    "            num_heads=3, \n",
    "            dim_ff=512,\n",
    "            dropout=0.0\n",
    "        ) -> None:\n",
    "\n",
    "        super(UserEncoder, self).__init__()\n",
    "\n",
    "        self.user_id_emb = nn.Embedding(user_id_size, 512, padding_idx=0)\n",
    "        self.movie_id_emb = MovieId(movie_id_size, 512)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(512, prev_rated_seq_len, 0.0)\n",
    "        self.encoder_block = Encoder(num_encoder_layers, 512, num_heads, dim_ff, 0.0)\n",
    "\n",
    "        self.fc_concat = nn.Linear(1024, embedding_size)\n",
    "        init_weights(self.fc_concat)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            self.fc_concat,\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(embedding_size)\n",
    "        )\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            user_ids:torch.Tensor, \n",
    "            prev_rated_movie_ids:torch.Tensor, \n",
    "            prev_ratings:torch.Tensor\n",
    "        ):\n",
    "        user_id_emb:torch.Tensor = self.user_id_emb(user_ids) # (batch, 512)\n",
    "\n",
    "        mask = (prev_rated_movie_ids != 0).float().unsqueeze(-1) # (batch, prev_rated_seq_len, 1)\n",
    "        mask = torch.matmul(mask, mask.transpose(-2,-1)).unsqueeze(1).repeat(1,self.num_heads,1,1) # (batch, num_heads, prev_rated_seq_len, prev_rated_seq_len)\n",
    "        \n",
    "        rated_movie_emb = self.movie_id_emb(prev_rated_movie_ids)   # (batch, prev_rated_seq_len, 512)\n",
    "        rated_movie_emb = self.positional_encoding(rated_movie_emb) # (batch, prev_rated_seq_len, 512)\n",
    "        rated_movie_emb = self.encoder_block(rated_movie_emb, mask) # (batch, prev_rated_seq_len, 512)\n",
    "\n",
    "        rated_movie_ratings = prev_ratings.unsqueeze(1) # (batch, 1, prev_rated_seq_len)\n",
    "        # weighted sum of ratings\n",
    "        rated_movie_emb_weighted = torch.matmul(rated_movie_ratings, rated_movie_emb).squeeze(1) # (batch, 512)\n",
    "\n",
    "        user_embedding = torch.concat([user_id_emb, rated_movie_emb_weighted], dim=-1) # (batch, 1024)\n",
    "        user_embedding = self.fc(user_embedding) # (batch, emb_size)\n",
    "\n",
    "        return user_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderSystem(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            user_id_size, \n",
    "            user_embedding_size, \n",
    "            user_prev_rated_seq_len, \n",
    "            user_num_encoder_layers, \n",
    "            user_num_heads, \n",
    "            user_dim_ff,\n",
    "            user_dropout,\n",
    "            movie_id_size, \n",
    "            movie_desc_size,\n",
    "            movie_genres_size,\n",
    "            movie_year_size, \n",
    "            movie_embedding_size, \n",
    "            movie_dropout,\n",
    "            embedding_size,\n",
    "            dropout=0.0\n",
    "        ) -> None:\n",
    "\n",
    "        super(RecommenderSystem, self).__init__()\n",
    "\n",
    "        self.movie_encoder = \\\n",
    "            MovieEncoder\\\n",
    "            (\n",
    "                movie_id_size, \n",
    "                movie_desc_size,\n",
    "                movie_genres_size,\n",
    "                movie_year_size, \n",
    "                movie_embedding_size, \n",
    "                movie_dropout\n",
    "            )\n",
    "        \n",
    "        self.user_encoder = \\\n",
    "            UserEncoder\\\n",
    "            (\n",
    "                user_id_size, \n",
    "                movie_id_size,\n",
    "                user_embedding_size, \n",
    "                user_prev_rated_seq_len, \n",
    "                user_num_encoder_layers, \n",
    "                user_num_heads, \n",
    "                user_dim_ff,\n",
    "                user_dropout\n",
    "            )\n",
    "\n",
    "        self.fc_concat = nn.Linear(user_embedding_size + movie_embedding_size, embedding_size)\n",
    "        init_weights(self.fc_concat)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            self.fc_concat,\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(embedding_size)\n",
    "        )\n",
    "\n",
    "        self.cross_features = CrossFeatureLayer(embedding_size, 3, 0.0)\n",
    "\n",
    "        self.fc_out = nn.Linear(embedding_size, 1)\n",
    "        init_weights(self.fc_out)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            self.fc_out,\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            user_ids:torch.Tensor, # (batch,)\n",
    "            user_prev_rated_movie_ids:torch.Tensor, \n",
    "            user_prev_ratings:torch.Tensor,\n",
    "            movie_ids:torch.Tensor, \n",
    "            movie_descriptions:torch.Tensor, \n",
    "            movie_genres:torch.Tensor, \n",
    "            movie_years:torch.Tensor\n",
    "        ):\n",
    "        \n",
    "        movie_embeddings = \\\n",
    "            self.movie_encoder\\\n",
    "            (\n",
    "                movie_ids, \n",
    "                movie_descriptions, \n",
    "                movie_genres, \n",
    "                movie_years\n",
    "            ) # (batch, 1, embedding_size)\n",
    "        \n",
    "        user_embeddings = \\\n",
    "            self.user_encoder\\\n",
    "                (\n",
    "                    user_ids, \n",
    "                    user_prev_rated_movie_ids, \n",
    "                    user_prev_ratings,\n",
    "                )                     # (batch, 1, embedding_size), (batch, movie_seq_len, embedding_size)\n",
    "        \n",
    "        emb_concat = torch.concat([movie_embeddings, user_embeddings], dim=-1)\n",
    "        \n",
    "        emb  = self.fc_concat(emb_concat)\n",
    "        emb  = self.cross_features(emb)\n",
    "        out  = self.out(emb).squeeze(-1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(values, dtype, max_seq_len=None):\n",
    "    if max_seq_len is None:\n",
    "        max_seq_len = max([len(x) for x in values])\n",
    "    \n",
    "    arr = np.zeros((len(values), max_seq_len), dtype=dtype)\n",
    "\n",
    "    for i in range(len(values)):\n",
    "        k = max_seq_len-1\n",
    "        for j in range(len(values[i])-1, -1, -1):\n",
    "            arr[i,k] = values[i][j]\n",
    "            k -= 1\n",
    "    \n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches(n, m, columns, batch_size=128):\n",
    "    max_seq_len = 20\n",
    "    df_ratings_train_mmap = np.memmap(\"df_ratings_train.mmap\", dtype=np.object_, mode=\"r\", shape=(n,m))\n",
    "\n",
    "    for i in range(0, n, batch_size):\n",
    "        df_ratings_batch = df_ratings_train_mmap[i:min(n,i+batch_size)]\n",
    "\n",
    "        df_ratings_batch_df = pd.DataFrame(df_ratings_batch, columns=columns)\n",
    "        df_ratings_batch_df = df_ratings_batch_df.merge(df_movies, on=[\"movieId\"], how=\"left\")\n",
    "\n",
    "        df_ratings_batch_df['description'] = df_ratings_batch_df['description'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "        df_ratings_batch_df['genres'] = df_ratings_batch_df['genres'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "        df_ratings_batch_df['movie_year'] = df_ratings_batch_df['movie_year'].fillna(0)\n",
    "\n",
    "        user_ids = df_ratings_batch_df[\"userId\"].to_numpy(dtype=np.int32)\n",
    "        user_prev_rated_movie_ids = pad_batch(df_ratings_batch_df[\"prev_movie_ids\"].to_numpy(), dtype=np.int32, max_seq_len=max_seq_len)\n",
    "        user_prev_ratings = pad_batch(df_ratings_batch_df[\"prev_ratings\"].to_numpy(), dtype=np.float32, max_seq_len=max_seq_len)\n",
    "\n",
    "        movie_ids = df_ratings_batch_df[\"movieId\"].to_numpy(dtype=np.int32)\n",
    "        movie_descriptions = pad_batch(df_ratings_batch_df[\"description\"].to_numpy(), dtype=np.int32)\n",
    "        movie_genres = pad_batch(df_ratings_batch_df[\"genres\"].to_numpy(), dtype=np.int32)\n",
    "        movie_years = df_ratings_batch_df[\"movie_year\"].to_numpy(dtype=np.int32)\n",
    "\n",
    "        user_ids = torch.from_numpy(user_ids).to(device=device)\n",
    "        user_prev_rated_movie_ids = torch.from_numpy(user_prev_rated_movie_ids).to(device=device)\n",
    "        user_prev_ratings = torch.from_numpy(user_prev_ratings).to(device=device)\n",
    "\n",
    "        movie_ids = torch.from_numpy(movie_ids).to(device=device)\n",
    "        movie_descriptions = torch.from_numpy(movie_descriptions).to(device=device)\n",
    "        movie_genres = torch.from_numpy(movie_genres).to(device=device)\n",
    "        movie_years = torch.from_numpy(movie_years).to(device=device)\n",
    "\n",
    "        labels = torch.from_numpy(df_ratings_batch_df[\"label\"].to_numpy(dtype=np.float32)).to(device=device)\n",
    "\n",
    "        yield [user_ids, user_prev_rated_movie_ids, user_prev_ratings, movie_ids, movie_descriptions, movie_genres, movie_years], labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_size = len(vocabulary[\"userId\"])+1\n",
    "movie_id_size = len(vocabulary[\"movieId\"])+1\n",
    "user_embedding_size = 128\n",
    "user_prev_rated_seq_len = 20\n",
    "user_num_encoder_layers = 3\n",
    "user_num_heads = 4\n",
    "user_dim_ff = 512\n",
    "user_dropout = 0.0\n",
    "movie_desc_size = len(vocabulary[\"description\"])+1\n",
    "movie_genres_size = len(vocabulary[\"genres\"])+1\n",
    "movie_year_size = len(vocabulary[\"movie_year\"])+1\n",
    "movie_embedding_size = 128\n",
    "movie_dropout = 0.0\n",
    "embedding_size = 128\n",
    "\n",
    "rec = \\\n",
    "    RecommenderSystem(\n",
    "        user_id_size, \n",
    "        user_embedding_size, \n",
    "        user_prev_rated_seq_len, \n",
    "        user_num_encoder_layers, \n",
    "        user_num_heads, \n",
    "        user_dim_ff,\n",
    "        user_dropout,\n",
    "        movie_id_size, \n",
    "        movie_desc_size,\n",
    "        movie_genres_size,\n",
    "        movie_year_size, \n",
    "        movie_embedding_size, \n",
    "        movie_dropout,\n",
    "        embedding_size\n",
    "    ).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(rec.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1, Loss: 0.8581198453903198\n",
      "Epoch: 1, Batch: 2, Loss: 0.8544762134552002\n",
      "Epoch: 1, Batch: 3, Loss: 0.8364319205284119\n",
      "Epoch: 1, Batch: 4, Loss: 1.0355992317199707\n",
      "Epoch: 1, Batch: 5, Loss: 0.8777509927749634\n",
      "Epoch: 1, Batch: 6, Loss: 1.0402905941009521\n",
      "Epoch: 1, Batch: 7, Loss: 1.0100646018981934\n",
      "Epoch: 1, Batch: 8, Loss: 0.8982543349266052\n",
      "Epoch: 1, Batch: 9, Loss: 0.7890875339508057\n",
      "Epoch: 1, Batch: 10, Loss: 0.8997007012367249\n",
      "Epoch: 1, Batch: 11, Loss: 0.7742992043495178\n",
      "Epoch: 1, Batch: 12, Loss: 0.8106857538223267\n",
      "Epoch: 1, Batch: 13, Loss: 0.8882604241371155\n",
      "Epoch: 1, Batch: 14, Loss: 0.9207038879394531\n",
      "Epoch: 1, Batch: 15, Loss: 0.8766075372695923\n",
      "Epoch: 1, Batch: 16, Loss: 0.8358126282691956\n",
      "Epoch: 1, Batch: 17, Loss: 0.835970401763916\n",
      "Epoch: 1, Batch: 18, Loss: 0.7707377076148987\n",
      "Epoch: 1, Batch: 19, Loss: 0.8298932909965515\n",
      "Epoch: 1, Batch: 20, Loss: 0.7672211527824402\n",
      "Epoch: 1, Batch: 21, Loss: 0.8910873532295227\n",
      "Epoch: 1, Batch: 22, Loss: 0.7811427116394043\n",
      "Epoch: 1, Batch: 23, Loss: 0.8462780714035034\n",
      "Epoch: 1, Batch: 24, Loss: 0.7631685733795166\n",
      "Epoch: 1, Batch: 25, Loss: 0.8779076337814331\n",
      "Epoch: 1, Batch: 26, Loss: 1.0135295391082764\n",
      "Epoch: 1, Batch: 27, Loss: 0.9869141578674316\n",
      "Epoch: 1, Batch: 28, Loss: 0.877350389957428\n",
      "Epoch: 1, Batch: 29, Loss: 1.075430154800415\n",
      "Epoch: 1, Batch: 30, Loss: 0.9026620388031006\n",
      "Epoch: 1, Batch: 31, Loss: 0.9314839839935303\n",
      "Epoch: 1, Batch: 32, Loss: 0.8259533643722534\n",
      "Epoch: 1, Batch: 33, Loss: 0.7803653478622437\n",
      "Epoch: 1, Batch: 34, Loss: 0.8421405553817749\n",
      "Epoch: 1, Batch: 35, Loss: 0.8288421630859375\n",
      "Epoch: 1, Batch: 36, Loss: 0.7634083032608032\n",
      "Epoch: 1, Batch: 37, Loss: 0.8682280778884888\n",
      "Epoch: 1, Batch: 38, Loss: 0.890703558921814\n",
      "Epoch: 1, Batch: 39, Loss: 0.8277170658111572\n",
      "Epoch: 1, Batch: 40, Loss: 0.7418503761291504\n",
      "Epoch: 1, Batch: 41, Loss: 0.7764778137207031\n",
      "Epoch: 1, Batch: 42, Loss: 0.771308422088623\n",
      "Epoch: 1, Batch: 43, Loss: 0.7292547225952148\n",
      "Epoch: 1, Batch: 44, Loss: 0.8602983951568604\n",
      "Epoch: 1, Batch: 45, Loss: 1.0097934007644653\n",
      "Epoch: 1, Batch: 46, Loss: 0.8928722143173218\n",
      "Epoch: 1, Batch: 47, Loss: 0.7301543951034546\n",
      "Epoch: 1, Batch: 48, Loss: 0.8538594841957092\n",
      "Epoch: 1, Batch: 49, Loss: 0.8346240520477295\n",
      "Epoch: 1, Batch: 50, Loss: 0.7471015453338623\n",
      "Epoch: 1, Batch: 51, Loss: 0.7518138885498047\n",
      "Epoch: 1, Batch: 52, Loss: 0.8158066272735596\n",
      "Epoch: 1, Batch: 53, Loss: 0.7376545667648315\n",
      "Epoch: 1, Batch: 54, Loss: 0.8425787687301636\n",
      "Epoch: 1, Batch: 55, Loss: 0.7321525812149048\n",
      "Epoch: 1, Batch: 56, Loss: 0.7547701597213745\n",
      "Epoch: 1, Batch: 57, Loss: 0.8507708311080933\n",
      "Epoch: 1, Batch: 58, Loss: 0.8263044953346252\n",
      "Epoch: 1, Batch: 59, Loss: 0.7803370356559753\n",
      "Epoch: 1, Batch: 60, Loss: 0.7006710767745972\n",
      "Epoch: 1, Batch: 61, Loss: 0.7090138792991638\n",
      "Epoch: 1, Batch: 62, Loss: 0.7134402990341187\n",
      "Epoch: 1, Batch: 63, Loss: 0.7890828251838684\n",
      "Epoch: 1, Batch: 64, Loss: 0.7871476411819458\n",
      "Epoch: 1, Batch: 65, Loss: 0.6604288220405579\n",
      "Epoch: 1, Batch: 66, Loss: 0.6677248477935791\n",
      "Epoch: 1, Batch: 67, Loss: 0.5746798515319824\n",
      "Epoch: 1, Batch: 68, Loss: 0.8273199796676636\n",
      "Epoch: 1, Batch: 69, Loss: 0.8093750476837158\n",
      "Epoch: 1, Batch: 70, Loss: 0.7070440053939819\n",
      "Epoch: 1, Batch: 71, Loss: 0.7027867436408997\n",
      "Epoch: 1, Batch: 72, Loss: 0.8465944528579712\n",
      "Epoch: 1, Batch: 73, Loss: 0.7525941133499146\n",
      "Epoch: 1, Batch: 74, Loss: 0.6956053972244263\n",
      "Epoch: 1, Batch: 75, Loss: 0.7051862478256226\n",
      "Epoch: 1, Batch: 76, Loss: 0.7328746914863586\n",
      "Epoch: 1, Batch: 77, Loss: 0.6884737014770508\n",
      "Epoch: 1, Batch: 78, Loss: 0.7919762134552002\n",
      "Epoch: 1, Batch: 79, Loss: 0.7563430070877075\n",
      "Epoch: 1, Batch: 80, Loss: 0.6855921149253845\n",
      "Epoch: 1, Batch: 81, Loss: 0.7945509552955627\n",
      "Epoch: 1, Batch: 82, Loss: 0.7921729683876038\n",
      "Epoch: 1, Batch: 83, Loss: 0.7131500244140625\n",
      "Epoch: 1, Batch: 84, Loss: 0.6794456243515015\n",
      "Epoch: 1, Batch: 85, Loss: 0.7748576402664185\n",
      "Epoch: 1, Batch: 86, Loss: 0.7837927341461182\n",
      "Epoch: 1, Batch: 87, Loss: 0.7163335084915161\n",
      "Epoch: 1, Batch: 88, Loss: 0.7125096321105957\n",
      "Epoch: 1, Batch: 89, Loss: 0.8670032024383545\n",
      "Epoch: 1, Batch: 90, Loss: 0.7454613447189331\n",
      "Epoch: 1, Batch: 91, Loss: 0.6491692066192627\n",
      "Epoch: 1, Batch: 92, Loss: 0.827528178691864\n",
      "Epoch: 1, Batch: 93, Loss: 0.7430626749992371\n",
      "Epoch: 1, Batch: 94, Loss: 0.7273950576782227\n",
      "Epoch: 1, Batch: 95, Loss: 0.7128551006317139\n",
      "Epoch: 1, Batch: 96, Loss: 0.7596399784088135\n",
      "Epoch: 1, Batch: 97, Loss: 0.7127416133880615\n",
      "Epoch: 1, Batch: 98, Loss: 0.7088133692741394\n",
      "Epoch: 1, Batch: 99, Loss: 0.7184950709342957\n",
      "Epoch: 1, Batch: 100, Loss: 0.7416303157806396\n",
      "Epoch: 1, Batch: 101, Loss: 0.6406188011169434\n",
      "Epoch: 1, Batch: 102, Loss: 0.6318950057029724\n",
      "Epoch: 1, Batch: 103, Loss: 0.6823692321777344\n",
      "Epoch: 1, Batch: 104, Loss: 0.6786344051361084\n",
      "Epoch: 1, Batch: 105, Loss: 0.7457281351089478\n",
      "Epoch: 1, Batch: 106, Loss: 0.7553449273109436\n",
      "Epoch: 1, Batch: 107, Loss: 0.7096021771430969\n",
      "Epoch: 1, Batch: 108, Loss: 0.6991601586341858\n",
      "Epoch: 1, Batch: 109, Loss: 0.705588161945343\n",
      "Epoch: 1, Batch: 110, Loss: 0.7465012073516846\n",
      "Epoch: 1, Batch: 111, Loss: 0.7565888166427612\n",
      "Epoch: 1, Batch: 112, Loss: 0.6363213062286377\n",
      "Epoch: 1, Batch: 113, Loss: 0.7808715105056763\n",
      "Epoch: 1, Batch: 114, Loss: 0.7048733830451965\n",
      "Epoch: 1, Batch: 115, Loss: 0.7594053745269775\n",
      "Epoch: 1, Batch: 116, Loss: 0.711733341217041\n",
      "Epoch: 1, Batch: 117, Loss: 0.6572844982147217\n",
      "Epoch: 1, Batch: 118, Loss: 0.5572118759155273\n",
      "Epoch: 1, Batch: 119, Loss: 0.7274974584579468\n",
      "Epoch: 1, Batch: 120, Loss: 0.7369619011878967\n",
      "Epoch: 1, Batch: 121, Loss: 0.6642266511917114\n",
      "Epoch: 1, Batch: 122, Loss: 0.6844121217727661\n",
      "Epoch: 1, Batch: 123, Loss: 0.5637685656547546\n",
      "Epoch: 1, Batch: 124, Loss: 0.589961051940918\n",
      "Epoch: 1, Batch: 125, Loss: 0.715376615524292\n",
      "Epoch: 1, Batch: 126, Loss: 0.7238038182258606\n",
      "Epoch: 1, Batch: 127, Loss: 0.7949163913726807\n",
      "Epoch: 1, Batch: 128, Loss: 0.8260579109191895\n",
      "Epoch: 1, Batch: 129, Loss: 0.9569231271743774\n",
      "Epoch: 1, Batch: 130, Loss: 0.835180401802063\n",
      "Epoch: 1, Batch: 131, Loss: 0.8213235139846802\n",
      "Epoch: 1, Batch: 132, Loss: 0.7164326906204224\n",
      "Epoch: 1, Batch: 133, Loss: 0.7253265380859375\n",
      "Epoch: 1, Batch: 134, Loss: 0.729857325553894\n",
      "Epoch: 1, Batch: 135, Loss: 0.7200939655303955\n",
      "Epoch: 1, Batch: 136, Loss: 0.7302429676055908\n",
      "Epoch: 1, Batch: 137, Loss: 0.6112467050552368\n",
      "Epoch: 1, Batch: 138, Loss: 0.6638149619102478\n",
      "Epoch: 1, Batch: 139, Loss: 0.7151310443878174\n",
      "Epoch: 1, Batch: 140, Loss: 0.6880422234535217\n",
      "Epoch: 1, Batch: 141, Loss: 0.6569161415100098\n",
      "Epoch: 1, Batch: 142, Loss: 0.7256968021392822\n",
      "Epoch: 1, Batch: 143, Loss: 0.7402605414390564\n",
      "Epoch: 1, Batch: 144, Loss: 0.6634706258773804\n",
      "Epoch: 1, Batch: 145, Loss: 0.7597187757492065\n",
      "Epoch: 1, Batch: 146, Loss: 0.6609071493148804\n",
      "Epoch: 1, Batch: 147, Loss: 0.6434636116027832\n",
      "Epoch: 1, Batch: 148, Loss: 0.6958094835281372\n",
      "Epoch: 1, Batch: 149, Loss: 0.7014978528022766\n",
      "Epoch: 1, Batch: 150, Loss: 0.6956361532211304\n",
      "Epoch: 1, Batch: 151, Loss: 0.651738166809082\n",
      "Epoch: 1, Batch: 152, Loss: 0.5972376465797424\n",
      "Epoch: 1, Batch: 153, Loss: 0.6311273574829102\n",
      "Epoch: 1, Batch: 154, Loss: 0.6512989401817322\n",
      "Epoch: 1, Batch: 155, Loss: 0.7181805372238159\n",
      "Epoch: 1, Batch: 156, Loss: 0.7472559213638306\n",
      "Epoch: 1, Batch: 157, Loss: 0.665022611618042\n",
      "Epoch: 1, Batch: 158, Loss: 0.7357567548751831\n",
      "Epoch: 1, Batch: 159, Loss: 0.789833664894104\n",
      "Epoch: 1, Batch: 160, Loss: 0.7567950487136841\n",
      "Epoch: 1, Batch: 161, Loss: 0.718909740447998\n",
      "Epoch: 1, Batch: 162, Loss: 0.5863319039344788\n",
      "Epoch: 1, Batch: 163, Loss: 0.6831075549125671\n",
      "Epoch: 1, Batch: 164, Loss: 0.6509829163551331\n",
      "Epoch: 1, Batch: 165, Loss: 0.6784111857414246\n",
      "Epoch: 1, Batch: 166, Loss: 0.7846894860267639\n",
      "Epoch: 1, Batch: 167, Loss: 0.7081639766693115\n",
      "Epoch: 1, Batch: 168, Loss: 0.6415302753448486\n",
      "Epoch: 1, Batch: 169, Loss: 0.6729328632354736\n",
      "Epoch: 1, Batch: 170, Loss: 0.7428778409957886\n",
      "Epoch: 1, Batch: 171, Loss: 0.7177251577377319\n",
      "Epoch: 1, Batch: 172, Loss: 0.6898434162139893\n",
      "Epoch: 1, Batch: 173, Loss: 0.7365559935569763\n",
      "Epoch: 1, Batch: 174, Loss: 0.7124637365341187\n",
      "Epoch: 1, Batch: 175, Loss: 0.48501861095428467\n",
      "Epoch: 1, Batch: 176, Loss: 0.775833249092102\n",
      "Epoch: 1, Batch: 177, Loss: 0.6304643154144287\n",
      "Epoch: 1, Batch: 178, Loss: 0.7886898517608643\n",
      "Epoch: 1, Batch: 179, Loss: 0.7312355041503906\n",
      "Epoch: 1, Batch: 180, Loss: 0.6673463582992554\n",
      "Epoch: 1, Batch: 181, Loss: 0.6689289808273315\n",
      "Epoch: 1, Batch: 182, Loss: 0.7080252766609192\n",
      "Epoch: 1, Batch: 183, Loss: 0.6559615135192871\n",
      "Epoch: 1, Batch: 184, Loss: 0.6844513416290283\n",
      "Epoch: 1, Batch: 185, Loss: 0.6936789751052856\n",
      "Epoch: 1, Batch: 186, Loss: 0.7073936462402344\n",
      "Epoch: 1, Batch: 187, Loss: 0.6741621494293213\n",
      "Epoch: 1, Batch: 188, Loss: 0.7762776613235474\n",
      "Epoch: 1, Batch: 189, Loss: 0.7304254770278931\n",
      "Epoch: 1, Batch: 190, Loss: 0.6556448936462402\n",
      "Epoch: 1, Batch: 191, Loss: 0.5658221244812012\n",
      "Epoch: 1, Batch: 192, Loss: 0.7256585359573364\n",
      "Epoch: 1, Batch: 193, Loss: 0.6358513832092285\n",
      "Epoch: 1, Batch: 194, Loss: 0.6778804063796997\n",
      "Epoch: 1, Batch: 195, Loss: 0.7072298526763916\n",
      "Epoch: 1, Batch: 196, Loss: 0.7074269652366638\n",
      "Epoch: 1, Batch: 197, Loss: 0.7012759447097778\n",
      "Epoch: 1, Batch: 198, Loss: 0.6313928365707397\n",
      "Epoch: 1, Batch: 199, Loss: 0.7473417520523071\n",
      "Epoch: 1, Batch: 200, Loss: 0.737517237663269\n",
      "Epoch: 1, Batch: 201, Loss: 0.6839432716369629\n",
      "Epoch: 1, Batch: 202, Loss: 0.6979752779006958\n",
      "Epoch: 1, Batch: 203, Loss: 0.705603301525116\n",
      "Epoch: 1, Batch: 204, Loss: 0.7239944934844971\n",
      "Epoch: 1, Batch: 205, Loss: 0.6942416429519653\n",
      "Epoch: 1, Batch: 206, Loss: 0.7323521375656128\n",
      "Epoch: 1, Batch: 207, Loss: 0.671757161617279\n",
      "Epoch: 1, Batch: 208, Loss: 0.7018395066261292\n",
      "Epoch: 1, Batch: 209, Loss: 0.7022855877876282\n",
      "Epoch: 1, Batch: 210, Loss: 0.6323190927505493\n",
      "Epoch: 1, Batch: 211, Loss: 0.6792793273925781\n",
      "Epoch: 1, Batch: 212, Loss: 0.7155637741088867\n",
      "Epoch: 1, Batch: 213, Loss: 0.7354780435562134\n",
      "Epoch: 1, Batch: 214, Loss: 0.6794568300247192\n",
      "Epoch: 1, Batch: 215, Loss: 0.6823669075965881\n",
      "Epoch: 1, Batch: 216, Loss: 0.6808465123176575\n",
      "Epoch: 1, Batch: 217, Loss: 0.700579822063446\n",
      "Epoch: 1, Batch: 218, Loss: 0.7255949378013611\n",
      "Epoch: 1, Batch: 219, Loss: 0.6704529523849487\n",
      "Epoch: 1, Batch: 220, Loss: 0.6909854412078857\n",
      "Epoch: 1, Batch: 221, Loss: 0.7816881537437439\n",
      "Epoch: 1, Batch: 222, Loss: 0.66365647315979\n",
      "Epoch: 1, Batch: 223, Loss: 0.6657571792602539\n",
      "Epoch: 1, Batch: 224, Loss: 0.7747488021850586\n",
      "Epoch: 1, Batch: 225, Loss: 0.7327234745025635\n",
      "Epoch: 1, Batch: 226, Loss: 0.7171522378921509\n",
      "Epoch: 1, Batch: 227, Loss: 0.7375322580337524\n",
      "Epoch: 1, Batch: 228, Loss: 0.6777018308639526\n",
      "Epoch: 1, Batch: 229, Loss: 0.6993992328643799\n",
      "Epoch: 1, Batch: 230, Loss: 0.6746817231178284\n",
      "Epoch: 1, Batch: 231, Loss: 0.7387346625328064\n",
      "Epoch: 1, Batch: 232, Loss: 0.6238053441047668\n",
      "Epoch: 1, Batch: 233, Loss: 0.6067593097686768\n",
      "Epoch: 1, Batch: 234, Loss: 0.6325539350509644\n",
      "Epoch: 1, Batch: 235, Loss: 0.5774961113929749\n",
      "Epoch: 1, Batch: 236, Loss: 0.7355799674987793\n",
      "Epoch: 1, Batch: 237, Loss: 0.626865804195404\n",
      "Epoch: 1, Batch: 238, Loss: 0.6993900537490845\n",
      "Epoch: 1, Batch: 239, Loss: 0.7714922428131104\n",
      "Epoch: 1, Batch: 240, Loss: 0.7062063217163086\n",
      "Epoch: 1, Batch: 241, Loss: 0.7318904399871826\n",
      "Epoch: 1, Batch: 242, Loss: 0.7426025867462158\n",
      "Epoch: 1, Batch: 243, Loss: 0.6640630960464478\n",
      "Epoch: 1, Batch: 244, Loss: 0.6798727512359619\n",
      "Epoch: 1, Batch: 245, Loss: 0.7103172540664673\n",
      "Epoch: 1, Batch: 246, Loss: 0.7819826602935791\n",
      "Epoch: 1, Batch: 247, Loss: 0.7164524793624878\n",
      "Epoch: 1, Batch: 248, Loss: 0.6592803597450256\n",
      "Epoch: 1, Batch: 249, Loss: 0.6503959894180298\n",
      "Epoch: 1, Batch: 250, Loss: 0.6842541694641113\n",
      "Epoch: 1, Batch: 251, Loss: 0.7075446844100952\n",
      "Epoch: 1, Batch: 252, Loss: 0.6739434003829956\n",
      "Epoch: 1, Batch: 253, Loss: 0.6457395553588867\n",
      "Epoch: 1, Batch: 254, Loss: 0.699164092540741\n",
      "Epoch: 1, Batch: 255, Loss: 0.6222832202911377\n",
      "Epoch: 1, Batch: 256, Loss: 0.6626787781715393\n",
      "Epoch: 1, Batch: 257, Loss: 0.6759272217750549\n",
      "Epoch: 1, Batch: 258, Loss: 0.667466402053833\n",
      "Epoch: 1, Batch: 259, Loss: 0.6535662412643433\n",
      "Epoch: 1, Batch: 260, Loss: 0.6172058582305908\n",
      "Epoch: 1, Batch: 261, Loss: 0.6469818949699402\n",
      "Epoch: 1, Batch: 262, Loss: 0.6762639284133911\n",
      "Epoch: 1, Batch: 263, Loss: 0.667932391166687\n",
      "Epoch: 1, Batch: 264, Loss: 0.6157135963439941\n",
      "Epoch: 1, Batch: 265, Loss: 0.6571577787399292\n",
      "Epoch: 1, Batch: 266, Loss: 0.6877151727676392\n",
      "Epoch: 1, Batch: 267, Loss: 0.6694058179855347\n",
      "Epoch: 1, Batch: 268, Loss: 0.6656372547149658\n",
      "Epoch: 1, Batch: 269, Loss: 0.6133314371109009\n",
      "Epoch: 1, Batch: 270, Loss: 0.6706311702728271\n",
      "Epoch: 1, Batch: 271, Loss: 0.6213465929031372\n",
      "Epoch: 1, Batch: 272, Loss: 0.9043068289756775\n",
      "Epoch: 1, Batch: 273, Loss: 0.7349783778190613\n",
      "Epoch: 1, Batch: 274, Loss: 0.6580964922904968\n",
      "Epoch: 1, Batch: 275, Loss: 0.7267124652862549\n",
      "Epoch: 1, Batch: 276, Loss: 0.6702795624732971\n",
      "Epoch: 1, Batch: 277, Loss: 0.6634452939033508\n",
      "Epoch: 1, Batch: 278, Loss: 0.6328243613243103\n",
      "Epoch: 1, Batch: 279, Loss: 0.6162571907043457\n",
      "Epoch: 1, Batch: 280, Loss: 0.5904165506362915\n",
      "Epoch: 1, Batch: 281, Loss: 0.6396762132644653\n",
      "Epoch: 1, Batch: 282, Loss: 0.651269793510437\n",
      "Epoch: 1, Batch: 283, Loss: 0.7477614283561707\n",
      "Epoch: 1, Batch: 284, Loss: 0.6743525266647339\n",
      "Epoch: 1, Batch: 285, Loss: 0.6696357131004333\n",
      "Epoch: 1, Batch: 286, Loss: 0.7269842624664307\n",
      "Epoch: 1, Batch: 287, Loss: 0.7722838521003723\n",
      "Epoch: 1, Batch: 288, Loss: 0.7101398706436157\n",
      "Epoch: 1, Batch: 289, Loss: 0.6939439177513123\n",
      "Epoch: 1, Batch: 290, Loss: 0.683188259601593\n",
      "Epoch: 1, Batch: 291, Loss: 0.6799786686897278\n",
      "Epoch: 1, Batch: 292, Loss: 0.6900864839553833\n",
      "Epoch: 1, Batch: 293, Loss: 0.7249549031257629\n",
      "Epoch: 1, Batch: 294, Loss: 0.6514967679977417\n",
      "Epoch: 1, Batch: 295, Loss: 0.7268346548080444\n",
      "Epoch: 1, Batch: 296, Loss: 0.6993615627288818\n",
      "Epoch: 1, Batch: 297, Loss: 0.6588082313537598\n",
      "Epoch: 1, Batch: 298, Loss: 0.5990826487541199\n",
      "Epoch: 1, Batch: 299, Loss: 0.6945911645889282\n",
      "Epoch: 1, Batch: 300, Loss: 0.6891400218009949\n",
      "Epoch: 1, Batch: 301, Loss: 0.6888306140899658\n",
      "Epoch: 1, Batch: 302, Loss: 0.6892327070236206\n",
      "Epoch: 1, Batch: 303, Loss: 0.7422983646392822\n",
      "Epoch: 1, Batch: 304, Loss: 0.6359208822250366\n",
      "Epoch: 1, Batch: 305, Loss: 0.6258357763290405\n",
      "Epoch: 1, Batch: 306, Loss: 0.6205869913101196\n",
      "Epoch: 1, Batch: 307, Loss: 0.7351016998291016\n",
      "Epoch: 1, Batch: 308, Loss: 0.7073113322257996\n",
      "Epoch: 1, Batch: 309, Loss: 0.6107041239738464\n",
      "Epoch: 1, Batch: 310, Loss: 0.6628119945526123\n",
      "Epoch: 1, Batch: 311, Loss: 0.6734931468963623\n",
      "Epoch: 1, Batch: 312, Loss: 0.714769721031189\n",
      "Epoch: 1, Batch: 313, Loss: 0.7544046640396118\n",
      "Epoch: 1, Batch: 314, Loss: 0.682853102684021\n",
      "Epoch: 1, Batch: 315, Loss: 0.5547280311584473\n",
      "Epoch: 1, Batch: 316, Loss: 0.7163738012313843\n",
      "Epoch: 1, Batch: 317, Loss: 0.6957372426986694\n",
      "Epoch: 1, Batch: 318, Loss: 0.7041370272636414\n",
      "Epoch: 1, Batch: 319, Loss: 0.6512044668197632\n",
      "Epoch: 1, Batch: 320, Loss: 0.6979038715362549\n",
      "Epoch: 1, Batch: 321, Loss: 0.6471707224845886\n",
      "Epoch: 1, Batch: 322, Loss: 0.5985968112945557\n",
      "Epoch: 1, Batch: 323, Loss: 0.586896538734436\n",
      "Epoch: 1, Batch: 324, Loss: 0.6544586420059204\n",
      "Epoch: 1, Batch: 325, Loss: 0.64445960521698\n",
      "Epoch: 1, Batch: 326, Loss: 0.7324737310409546\n",
      "Epoch: 1, Batch: 327, Loss: 0.7065927982330322\n",
      "Epoch: 1, Batch: 328, Loss: 0.7294378280639648\n",
      "Epoch: 1, Batch: 329, Loss: 0.5907424688339233\n",
      "Epoch: 1, Batch: 330, Loss: 0.6546808481216431\n",
      "Epoch: 1, Batch: 331, Loss: 0.54256272315979\n",
      "Epoch: 1, Batch: 332, Loss: 0.6163715720176697\n",
      "Epoch: 1, Batch: 333, Loss: 0.6551538705825806\n",
      "Epoch: 1, Batch: 334, Loss: 0.7091562747955322\n",
      "Epoch: 1, Batch: 335, Loss: 0.6407248377799988\n",
      "Epoch: 1, Batch: 336, Loss: 0.6781021952629089\n",
      "Epoch: 1, Batch: 337, Loss: 0.6543176770210266\n",
      "Epoch: 1, Batch: 338, Loss: 0.6824108362197876\n",
      "Epoch: 1, Batch: 339, Loss: 0.6834851503372192\n",
      "Epoch: 1, Batch: 340, Loss: 0.6505771279335022\n",
      "Epoch: 1, Batch: 341, Loss: 0.6959875226020813\n",
      "Epoch: 1, Batch: 342, Loss: 0.6810442805290222\n",
      "Epoch: 1, Batch: 343, Loss: 0.6993704438209534\n",
      "Epoch: 1, Batch: 344, Loss: 0.6792161464691162\n",
      "Epoch: 1, Batch: 345, Loss: 0.6588732600212097\n",
      "Epoch: 1, Batch: 346, Loss: 0.6881157159805298\n",
      "Epoch: 1, Batch: 347, Loss: 0.683412492275238\n",
      "Epoch: 1, Batch: 348, Loss: 0.6700371503829956\n",
      "Epoch: 1, Batch: 349, Loss: 0.6954287886619568\n",
      "Epoch: 1, Batch: 350, Loss: 0.6483471393585205\n",
      "Epoch: 1, Batch: 351, Loss: 0.6663079261779785\n",
      "Epoch: 1, Batch: 352, Loss: 0.4981173276901245\n",
      "Epoch: 1, Batch: 353, Loss: 0.630765438079834\n",
      "Epoch: 1, Batch: 354, Loss: 0.6805802583694458\n",
      "Epoch: 1, Batch: 355, Loss: 0.6609660387039185\n",
      "Epoch: 1, Batch: 356, Loss: 0.6407250165939331\n",
      "Epoch: 1, Batch: 357, Loss: 0.6616542339324951\n",
      "Epoch: 1, Batch: 358, Loss: 0.6687142848968506\n",
      "Epoch: 1, Batch: 359, Loss: 0.6106152534484863\n",
      "Epoch: 1, Batch: 360, Loss: 0.7032840251922607\n",
      "Epoch: 1, Batch: 361, Loss: 0.6854104995727539\n",
      "Epoch: 1, Batch: 362, Loss: 0.6429535150527954\n",
      "Epoch: 1, Batch: 363, Loss: 0.6405342817306519\n",
      "Epoch: 1, Batch: 364, Loss: 0.6717913746833801\n",
      "Epoch: 1, Batch: 365, Loss: 0.6517754793167114\n",
      "Epoch: 1, Batch: 366, Loss: 0.7306275963783264\n",
      "Epoch: 1, Batch: 367, Loss: 0.6978973150253296\n",
      "Epoch: 1, Batch: 368, Loss: 0.7348626852035522\n",
      "Epoch: 1, Batch: 369, Loss: 0.6578188538551331\n",
      "Epoch: 1, Batch: 370, Loss: 0.5995441675186157\n",
      "Epoch: 1, Batch: 371, Loss: 0.6575884819030762\n",
      "Epoch: 1, Batch: 372, Loss: 0.6873533129692078\n",
      "Epoch: 1, Batch: 373, Loss: 0.6979324221611023\n",
      "Epoch: 1, Batch: 374, Loss: 0.6972503066062927\n",
      "Epoch: 1, Batch: 375, Loss: 0.6900798082351685\n",
      "Epoch: 1, Batch: 376, Loss: 0.6576067805290222\n",
      "Epoch: 1, Batch: 377, Loss: 0.6386927366256714\n",
      "Epoch: 1, Batch: 378, Loss: 0.651528537273407\n",
      "Epoch: 1, Batch: 379, Loss: 0.6141069531440735\n",
      "Epoch: 1, Batch: 380, Loss: 0.6436072587966919\n",
      "Epoch: 1, Batch: 381, Loss: 0.6738535165786743\n",
      "Epoch: 1, Batch: 382, Loss: 0.6403890252113342\n",
      "Epoch: 1, Batch: 383, Loss: 0.608709454536438\n",
      "Epoch: 1, Batch: 384, Loss: 0.6870431900024414\n",
      "Epoch: 1, Batch: 385, Loss: 0.5975497961044312\n",
      "Epoch: 1, Batch: 386, Loss: 0.6611651182174683\n",
      "Epoch: 1, Batch: 387, Loss: 0.7473805546760559\n",
      "Epoch: 1, Batch: 388, Loss: 0.7486728429794312\n",
      "Epoch: 1, Batch: 389, Loss: 0.676277756690979\n",
      "Epoch: 1, Batch: 390, Loss: 0.7310272455215454\n",
      "Epoch: 1, Batch: 391, Loss: 0.6767731308937073\n",
      "Epoch: 1, Batch: 392, Loss: 0.6433128118515015\n",
      "Epoch: 1, Batch: 393, Loss: 0.6464685797691345\n",
      "Epoch: 1, Batch: 394, Loss: 0.720431387424469\n",
      "Epoch: 1, Batch: 395, Loss: 0.6593396663665771\n",
      "Epoch: 1, Batch: 396, Loss: 0.6606470346450806\n",
      "Epoch: 1, Batch: 397, Loss: 0.5548015832901001\n",
      "Epoch: 1, Batch: 398, Loss: 0.7599953413009644\n",
      "Epoch: 1, Batch: 399, Loss: 0.6431381106376648\n",
      "Epoch: 1, Batch: 400, Loss: 0.6623557806015015\n",
      "Epoch: 1, Batch: 401, Loss: 0.7204432487487793\n",
      "Epoch: 1, Batch: 402, Loss: 0.620569109916687\n",
      "Epoch: 1, Batch: 403, Loss: 0.6097351908683777\n",
      "Epoch: 1, Batch: 404, Loss: 0.7094742655754089\n",
      "Epoch: 1, Batch: 405, Loss: 0.7231615781784058\n",
      "Epoch: 1, Batch: 406, Loss: 0.737462043762207\n",
      "Epoch: 1, Batch: 407, Loss: 0.617700457572937\n",
      "Epoch: 1, Batch: 408, Loss: 0.6446901559829712\n",
      "Epoch: 1, Batch: 409, Loss: 0.625960111618042\n",
      "Epoch: 1, Batch: 410, Loss: 0.639721155166626\n",
      "Epoch: 1, Batch: 411, Loss: 0.707708477973938\n",
      "Epoch: 1, Batch: 412, Loss: 0.6356326937675476\n",
      "Epoch: 1, Batch: 413, Loss: 0.6467597484588623\n",
      "Epoch: 1, Batch: 414, Loss: 0.6399073600769043\n",
      "Epoch: 1, Batch: 415, Loss: 0.7565839886665344\n",
      "Epoch: 1, Batch: 416, Loss: 0.6950081586837769\n",
      "Epoch: 1, Batch: 417, Loss: 0.7291178703308105\n",
      "Epoch: 1, Batch: 418, Loss: 0.6720556020736694\n",
      "Epoch: 1, Batch: 419, Loss: 0.6416822075843811\n",
      "Epoch: 1, Batch: 420, Loss: 0.5350936651229858\n",
      "Epoch: 1, Batch: 421, Loss: 0.6053836345672607\n",
      "Epoch: 1, Batch: 422, Loss: 0.7066727876663208\n",
      "Epoch: 1, Batch: 423, Loss: 0.7086431980133057\n",
      "Epoch: 1, Batch: 424, Loss: 0.7075526714324951\n",
      "Epoch: 1, Batch: 425, Loss: 0.694441556930542\n",
      "Epoch: 1, Batch: 426, Loss: 0.6970894932746887\n",
      "Epoch: 1, Batch: 427, Loss: 0.6897239685058594\n",
      "Epoch: 1, Batch: 428, Loss: 0.6846761703491211\n",
      "Epoch: 1, Batch: 429, Loss: 0.7376750707626343\n",
      "Epoch: 1, Batch: 430, Loss: 0.6798791885375977\n",
      "Epoch: 1, Batch: 431, Loss: 0.7062942385673523\n",
      "Epoch: 1, Batch: 432, Loss: 0.6532953977584839\n",
      "Epoch: 1, Batch: 433, Loss: 0.7199543118476868\n",
      "Epoch: 1, Batch: 434, Loss: 0.6615992784500122\n",
      "Epoch: 1, Batch: 435, Loss: 0.7124176025390625\n",
      "Epoch: 1, Batch: 436, Loss: 0.7085033655166626\n",
      "Epoch: 1, Batch: 437, Loss: 0.6654037833213806\n",
      "Epoch: 1, Batch: 438, Loss: 0.7041120529174805\n",
      "Epoch: 1, Batch: 439, Loss: 0.7144302129745483\n",
      "Epoch: 1, Batch: 440, Loss: 0.6801097393035889\n",
      "Epoch: 1, Batch: 441, Loss: 0.669653594493866\n",
      "Epoch: 1, Batch: 442, Loss: 0.697751522064209\n",
      "Epoch: 1, Batch: 443, Loss: 0.6899500489234924\n",
      "Epoch: 1, Batch: 444, Loss: 0.6903573274612427\n",
      "Epoch: 1, Batch: 445, Loss: 0.749133288860321\n",
      "Epoch: 1, Batch: 446, Loss: 0.709333598613739\n",
      "Epoch: 1, Batch: 447, Loss: 0.726391077041626\n",
      "Epoch: 1, Batch: 448, Loss: 0.6269556283950806\n",
      "Epoch: 1, Batch: 449, Loss: 0.7054427862167358\n",
      "Epoch: 1, Batch: 450, Loss: 0.7066823840141296\n",
      "Epoch: 1, Batch: 451, Loss: 0.7104095220565796\n",
      "Epoch: 1, Batch: 452, Loss: 0.649964451789856\n",
      "Epoch: 1, Batch: 453, Loss: 0.7699189186096191\n",
      "Epoch: 1, Batch: 454, Loss: 0.7594215869903564\n",
      "Epoch: 1, Batch: 455, Loss: 0.7174243927001953\n",
      "Epoch: 1, Batch: 456, Loss: 0.685958206653595\n",
      "Epoch: 1, Batch: 457, Loss: 0.7043550610542297\n",
      "Epoch: 1, Batch: 458, Loss: 0.7058366537094116\n",
      "Epoch: 1, Batch: 459, Loss: 0.6954540014266968\n",
      "Epoch: 1, Batch: 460, Loss: 0.6853442192077637\n",
      "Epoch: 1, Batch: 461, Loss: 0.6119236946105957\n",
      "Epoch: 1, Batch: 462, Loss: 0.6804933547973633\n",
      "Epoch: 1, Batch: 463, Loss: 0.6462511420249939\n",
      "Epoch: 1, Batch: 464, Loss: 0.6720926761627197\n",
      "Epoch: 1, Batch: 465, Loss: 0.6952038407325745\n",
      "Epoch: 1, Batch: 466, Loss: 0.6758949160575867\n",
      "Epoch: 1, Batch: 467, Loss: 0.6685870289802551\n",
      "Epoch: 1, Batch: 468, Loss: 0.6691538691520691\n",
      "Epoch: 1, Batch: 469, Loss: 0.7347047328948975\n",
      "Epoch: 1, Batch: 470, Loss: 0.7128661870956421\n",
      "Epoch: 1, Batch: 471, Loss: 0.7098315954208374\n",
      "Epoch: 1, Batch: 472, Loss: 0.6119175553321838\n",
      "Epoch: 1, Batch: 473, Loss: 0.751875638961792\n",
      "Epoch: 1, Batch: 474, Loss: 0.6963528990745544\n",
      "Epoch: 1, Batch: 475, Loss: 0.6950588226318359\n",
      "Epoch: 1, Batch: 476, Loss: 0.6588085293769836\n",
      "Epoch: 1, Batch: 477, Loss: 0.5847140550613403\n",
      "Epoch: 1, Batch: 478, Loss: 0.6548863649368286\n",
      "Epoch: 1, Batch: 479, Loss: 0.7417502999305725\n",
      "Epoch: 1, Batch: 480, Loss: 0.6292676329612732\n",
      "Epoch: 1, Batch: 481, Loss: 0.6946690082550049\n",
      "Epoch: 1, Batch: 482, Loss: 0.6934921741485596\n",
      "Epoch: 1, Batch: 483, Loss: 0.7715429067611694\n",
      "Epoch: 1, Batch: 484, Loss: 0.7447198033332825\n",
      "Epoch: 1, Batch: 485, Loss: 0.6936068534851074\n",
      "Epoch: 1, Batch: 486, Loss: 0.6104257106781006\n",
      "Epoch: 1, Batch: 487, Loss: 0.5845537185668945\n",
      "Epoch: 1, Batch: 488, Loss: 0.648748517036438\n",
      "Epoch: 1, Batch: 489, Loss: 0.6393129825592041\n",
      "Epoch: 1, Batch: 490, Loss: 0.6900954246520996\n",
      "Epoch: 1, Batch: 491, Loss: 0.686043918132782\n",
      "Epoch: 1, Batch: 492, Loss: 0.6677805781364441\n",
      "Epoch: 1, Batch: 493, Loss: 0.7119753360748291\n",
      "Epoch: 1, Batch: 494, Loss: 0.6612515449523926\n",
      "Epoch: 1, Batch: 495, Loss: 0.6766790747642517\n",
      "Epoch: 1, Batch: 496, Loss: 0.6627025604248047\n",
      "Epoch: 1, Batch: 497, Loss: 0.6583383679389954\n",
      "Epoch: 1, Batch: 498, Loss: 0.6707310676574707\n",
      "Epoch: 1, Batch: 499, Loss: 0.7157309651374817\n",
      "Epoch: 1, Batch: 500, Loss: 0.61773282289505\n",
      "Epoch: 1, Batch: 501, Loss: 0.7155694961547852\n",
      "Epoch: 1, Batch: 502, Loss: 0.7047113180160522\n",
      "Epoch: 1, Batch: 503, Loss: 0.6642859578132629\n",
      "Epoch: 1, Batch: 504, Loss: 0.7026557922363281\n",
      "Epoch: 1, Batch: 505, Loss: 0.6226507425308228\n",
      "Epoch: 1, Batch: 506, Loss: 0.642334520816803\n",
      "Epoch: 1, Batch: 507, Loss: 0.6958802938461304\n",
      "Epoch: 1, Batch: 508, Loss: 0.7240634560585022\n",
      "Epoch: 1, Batch: 509, Loss: 0.6029106378555298\n",
      "Epoch: 1, Batch: 510, Loss: 0.653144121170044\n",
      "Epoch: 1, Batch: 511, Loss: 0.6408209800720215\n",
      "Epoch: 1, Batch: 512, Loss: 0.6571831107139587\n",
      "Epoch: 1, Batch: 513, Loss: 0.6743686199188232\n",
      "Epoch: 1, Batch: 514, Loss: 0.7234739661216736\n",
      "Epoch: 1, Batch: 515, Loss: 0.697588324546814\n",
      "Epoch: 1, Batch: 516, Loss: 0.6039313077926636\n",
      "Epoch: 1, Batch: 517, Loss: 0.6490139365196228\n",
      "Epoch: 1, Batch: 518, Loss: 0.7221943140029907\n",
      "Epoch: 1, Batch: 519, Loss: 0.7285752296447754\n",
      "Epoch: 1, Batch: 520, Loss: 0.7236026525497437\n",
      "Epoch: 1, Batch: 521, Loss: 0.7092872858047485\n",
      "Epoch: 1, Batch: 522, Loss: 0.6918368339538574\n",
      "Epoch: 1, Batch: 523, Loss: 0.6431474089622498\n",
      "Epoch: 1, Batch: 524, Loss: 0.7035670280456543\n",
      "Epoch: 1, Batch: 525, Loss: 0.6120022535324097\n",
      "Epoch: 1, Batch: 526, Loss: 0.7269212007522583\n",
      "Epoch: 1, Batch: 527, Loss: 0.6910206079483032\n",
      "Epoch: 1, Batch: 528, Loss: 0.7319390177726746\n",
      "Epoch: 1, Batch: 529, Loss: 0.68498694896698\n",
      "Epoch: 1, Batch: 530, Loss: 0.6811866760253906\n",
      "Epoch: 1, Batch: 531, Loss: 0.6288286447525024\n",
      "Epoch: 1, Batch: 532, Loss: 0.608532726764679\n",
      "Epoch: 1, Batch: 533, Loss: 0.689359724521637\n",
      "Epoch: 1, Batch: 534, Loss: 0.7088379859924316\n",
      "Epoch: 1, Batch: 535, Loss: 0.7051903605461121\n",
      "Epoch: 1, Batch: 536, Loss: 0.6922177076339722\n",
      "Epoch: 1, Batch: 537, Loss: 0.6953645348548889\n",
      "Epoch: 1, Batch: 538, Loss: 0.6918667554855347\n",
      "Epoch: 1, Batch: 539, Loss: 0.6645830869674683\n",
      "Epoch: 1, Batch: 540, Loss: 0.6635870933532715\n",
      "Epoch: 1, Batch: 541, Loss: 0.6216587424278259\n",
      "Epoch: 1, Batch: 542, Loss: 0.6726471185684204\n",
      "Epoch: 1, Batch: 543, Loss: 0.6502851843833923\n",
      "Epoch: 1, Batch: 544, Loss: 0.5628746151924133\n",
      "Epoch: 1, Batch: 545, Loss: 0.721178412437439\n",
      "Epoch: 1, Batch: 546, Loss: 0.6599911451339722\n",
      "Epoch: 1, Batch: 547, Loss: 0.6705428957939148\n",
      "Epoch: 1, Batch: 548, Loss: 0.6622248888015747\n",
      "Epoch: 1, Batch: 549, Loss: 0.6691622734069824\n",
      "Epoch: 1, Batch: 550, Loss: 0.6903991103172302\n",
      "Epoch: 1, Batch: 551, Loss: 0.6779099702835083\n",
      "Epoch: 1, Batch: 552, Loss: 0.6970878839492798\n",
      "Epoch: 1, Batch: 553, Loss: 0.6792892217636108\n",
      "Epoch: 1, Batch: 554, Loss: 0.6326229572296143\n",
      "Epoch: 1, Batch: 555, Loss: 0.6525277495384216\n",
      "Epoch: 1, Batch: 556, Loss: 0.4833977520465851\n",
      "Epoch: 1, Batch: 557, Loss: 0.5395488739013672\n",
      "Epoch: 1, Batch: 558, Loss: 0.5973176956176758\n",
      "Epoch: 1, Batch: 559, Loss: 0.6490787267684937\n",
      "Epoch: 1, Batch: 560, Loss: 0.5825197696685791\n",
      "Epoch: 1, Batch: 561, Loss: 0.6361426711082458\n",
      "Epoch: 1, Batch: 562, Loss: 0.7179924845695496\n",
      "Epoch: 1, Batch: 563, Loss: 0.6760391592979431\n",
      "Epoch: 1, Batch: 564, Loss: 0.6148015856742859\n",
      "Epoch: 1, Batch: 565, Loss: 0.6463971138000488\n",
      "Epoch: 1, Batch: 566, Loss: 0.7113523483276367\n",
      "Epoch: 1, Batch: 567, Loss: 0.5734217166900635\n",
      "Epoch: 1, Batch: 568, Loss: 0.7764079570770264\n",
      "Epoch: 1, Batch: 569, Loss: 0.5882176160812378\n",
      "Epoch: 1, Batch: 570, Loss: 0.7360923290252686\n",
      "Epoch: 1, Batch: 571, Loss: 0.7068527936935425\n",
      "Epoch: 1, Batch: 572, Loss: 0.6292383670806885\n",
      "Epoch: 1, Batch: 573, Loss: 0.5866414308547974\n",
      "Epoch: 1, Batch: 574, Loss: 0.629385232925415\n",
      "Epoch: 1, Batch: 575, Loss: 0.6353528499603271\n",
      "Epoch: 1, Batch: 576, Loss: 0.581842303276062\n",
      "Epoch: 1, Batch: 577, Loss: 0.5690006017684937\n",
      "Epoch: 1, Batch: 578, Loss: 0.5438268184661865\n",
      "Epoch: 1, Batch: 579, Loss: 0.6344109177589417\n",
      "Epoch: 1, Batch: 580, Loss: 0.6711106300354004\n",
      "Epoch: 1, Batch: 581, Loss: 0.7527710795402527\n",
      "Epoch: 1, Batch: 582, Loss: 0.658846378326416\n",
      "Epoch: 1, Batch: 583, Loss: 0.6661256551742554\n",
      "Epoch: 1, Batch: 584, Loss: 0.697233259677887\n",
      "Epoch: 1, Batch: 585, Loss: 0.7478750944137573\n",
      "Epoch: 1, Batch: 586, Loss: 0.6710975170135498\n",
      "Epoch: 1, Batch: 587, Loss: 0.7181426286697388\n",
      "Epoch: 1, Batch: 588, Loss: 0.6826547384262085\n",
      "Epoch: 1, Batch: 589, Loss: 0.6405278444290161\n",
      "Epoch: 1, Batch: 590, Loss: 0.639448881149292\n",
      "Epoch: 1, Batch: 591, Loss: 0.6807743906974792\n",
      "Epoch: 1, Batch: 592, Loss: 0.6598861217498779\n",
      "Epoch: 1, Batch: 593, Loss: 0.6358715295791626\n",
      "Epoch: 1, Batch: 594, Loss: 0.6155372262001038\n",
      "Epoch: 1, Batch: 595, Loss: 0.5973342657089233\n",
      "Epoch: 1, Batch: 596, Loss: 0.6829668283462524\n",
      "Epoch: 1, Batch: 597, Loss: 0.6397727727890015\n",
      "Epoch: 1, Batch: 598, Loss: 0.5949465036392212\n",
      "Epoch: 1, Batch: 599, Loss: 0.652647852897644\n",
      "Epoch: 1, Batch: 600, Loss: 0.7239250540733337\n",
      "Epoch: 1, Batch: 601, Loss: 0.6602160930633545\n",
      "Epoch: 1, Batch: 602, Loss: 0.6472921967506409\n",
      "Epoch: 1, Batch: 603, Loss: 0.6195579171180725\n",
      "Epoch: 1, Batch: 604, Loss: 0.6427979469299316\n",
      "Epoch: 1, Batch: 605, Loss: 0.6381325721740723\n",
      "Epoch: 1, Batch: 606, Loss: 0.6819934844970703\n",
      "Epoch: 1, Batch: 607, Loss: 0.7475091814994812\n",
      "Epoch: 1, Batch: 608, Loss: 0.5606040358543396\n",
      "Epoch: 1, Batch: 609, Loss: 0.7417514324188232\n",
      "Epoch: 1, Batch: 610, Loss: 0.699393630027771\n",
      "Epoch: 1, Batch: 611, Loss: 0.6340022087097168\n",
      "Epoch: 1, Batch: 612, Loss: 0.6234073638916016\n",
      "Epoch: 1, Batch: 613, Loss: 0.5942865014076233\n",
      "Epoch: 1, Batch: 614, Loss: 0.7008974552154541\n",
      "Epoch: 1, Batch: 615, Loss: 0.6410881280899048\n",
      "Epoch: 1, Batch: 616, Loss: 0.630978524684906\n",
      "Epoch: 1, Batch: 617, Loss: 0.659674346446991\n",
      "Epoch: 1, Batch: 618, Loss: 0.6500494480133057\n",
      "Epoch: 1, Batch: 619, Loss: 0.6281740069389343\n",
      "Epoch: 1, Batch: 620, Loss: 0.7312448024749756\n",
      "Epoch: 1, Batch: 621, Loss: 0.7278587818145752\n",
      "Epoch: 1, Batch: 622, Loss: 0.6998015642166138\n",
      "Epoch: 1, Batch: 623, Loss: 0.6862351894378662\n",
      "Epoch: 1, Batch: 624, Loss: 0.6442383527755737\n",
      "Epoch: 1, Batch: 625, Loss: 0.7447143197059631\n",
      "Epoch: 1, Batch: 626, Loss: 0.669217586517334\n",
      "Epoch: 1, Batch: 627, Loss: 0.6428465843200684\n",
      "Epoch: 1, Batch: 628, Loss: 0.6458394527435303\n",
      "Epoch: 1, Batch: 629, Loss: 0.6393448710441589\n",
      "Epoch: 1, Batch: 630, Loss: 0.7083145380020142\n",
      "Epoch: 1, Batch: 631, Loss: 0.6325908303260803\n",
      "Epoch: 1, Batch: 632, Loss: 0.7259001731872559\n",
      "Epoch: 1, Batch: 633, Loss: 0.659287691116333\n",
      "Epoch: 1, Batch: 634, Loss: 0.6022446155548096\n",
      "Epoch: 1, Batch: 635, Loss: 0.6044667959213257\n",
      "Epoch: 1, Batch: 636, Loss: 0.7176651954650879\n",
      "Epoch: 1, Batch: 637, Loss: 0.6723066568374634\n",
      "Epoch: 1, Batch: 638, Loss: 0.6918755769729614\n",
      "Epoch: 1, Batch: 639, Loss: 0.6002542972564697\n",
      "Epoch: 1, Batch: 640, Loss: 0.6459479331970215\n",
      "Epoch: 1, Batch: 641, Loss: 0.6630101203918457\n",
      "Epoch: 1, Batch: 642, Loss: 0.6153870820999146\n",
      "Epoch: 1, Batch: 643, Loss: 0.6049869060516357\n",
      "Epoch: 1, Batch: 644, Loss: 0.5658512115478516\n",
      "Epoch: 1, Batch: 645, Loss: 0.589924693107605\n",
      "Epoch: 1, Batch: 646, Loss: 0.6439259648323059\n",
      "Epoch: 1, Batch: 647, Loss: 0.6719109416007996\n",
      "Epoch: 1, Batch: 648, Loss: 0.7139503955841064\n",
      "Epoch: 1, Batch: 649, Loss: 0.7604383230209351\n",
      "Epoch: 1, Batch: 650, Loss: 0.6898306608200073\n",
      "Epoch: 1, Batch: 651, Loss: 0.6899228096008301\n",
      "Epoch: 1, Batch: 652, Loss: 0.6604180335998535\n",
      "Epoch: 1, Batch: 653, Loss: 0.6594551205635071\n",
      "Epoch: 1, Batch: 654, Loss: 0.6743616461753845\n",
      "Epoch: 1, Batch: 655, Loss: 0.7115444540977478\n",
      "Epoch: 1, Batch: 656, Loss: 0.6413428783416748\n",
      "Epoch: 1, Batch: 657, Loss: 0.7288014888763428\n",
      "Epoch: 1, Batch: 658, Loss: 0.6793237924575806\n",
      "Epoch: 1, Batch: 659, Loss: 0.6502932906150818\n",
      "Epoch: 1, Batch: 660, Loss: 0.6244007349014282\n",
      "Epoch: 1, Batch: 661, Loss: 0.6952409744262695\n",
      "Epoch: 1, Batch: 662, Loss: 0.6635212898254395\n",
      "Epoch: 1, Batch: 663, Loss: 0.658562421798706\n",
      "Epoch: 1, Batch: 664, Loss: 0.6433350443840027\n",
      "Epoch: 1, Batch: 665, Loss: 0.6801669597625732\n",
      "Epoch: 1, Batch: 666, Loss: 0.6874773502349854\n",
      "Epoch: 1, Batch: 667, Loss: 0.5902086496353149\n",
      "Epoch: 1, Batch: 668, Loss: 0.6529347896575928\n",
      "Epoch: 1, Batch: 669, Loss: 0.6260092258453369\n",
      "Epoch: 1, Batch: 670, Loss: 0.693053126335144\n",
      "Epoch: 1, Batch: 671, Loss: 0.6589840650558472\n",
      "Epoch: 1, Batch: 672, Loss: 0.7932997941970825\n",
      "Epoch: 1, Batch: 673, Loss: 0.7703644037246704\n",
      "Epoch: 1, Batch: 674, Loss: 0.6598485708236694\n",
      "Epoch: 1, Batch: 675, Loss: 0.5956268906593323\n",
      "Epoch: 1, Batch: 676, Loss: 0.6438776254653931\n",
      "Epoch: 1, Batch: 677, Loss: 0.7181426286697388\n",
      "Epoch: 1, Batch: 678, Loss: 0.6038508415222168\n",
      "Epoch: 1, Batch: 679, Loss: 0.583865761756897\n",
      "Epoch: 1, Batch: 680, Loss: 0.6212010383605957\n",
      "Epoch: 1, Batch: 681, Loss: 0.6321895718574524\n",
      "Epoch: 1, Batch: 682, Loss: 0.6637355089187622\n",
      "Epoch: 1, Batch: 683, Loss: 0.6219103336334229\n",
      "Epoch: 1, Batch: 684, Loss: 0.7508543133735657\n",
      "Epoch: 1, Batch: 685, Loss: 0.7424540519714355\n",
      "Epoch: 1, Batch: 686, Loss: 0.6230164170265198\n",
      "Epoch: 1, Batch: 687, Loss: 0.6642447710037231\n",
      "Epoch: 1, Batch: 688, Loss: 0.674007773399353\n",
      "Epoch: 1, Batch: 689, Loss: 0.6220215559005737\n",
      "Epoch: 1, Batch: 690, Loss: 0.6655222177505493\n",
      "Epoch: 1, Batch: 691, Loss: 0.6656303405761719\n",
      "Epoch: 1, Batch: 692, Loss: 0.7394891977310181\n",
      "Epoch: 1, Batch: 693, Loss: 0.6913284659385681\n",
      "Epoch: 1, Batch: 694, Loss: 0.6478949785232544\n",
      "Epoch: 1, Batch: 695, Loss: 0.7395995259284973\n",
      "Epoch: 1, Batch: 696, Loss: 0.6609318256378174\n",
      "Epoch: 1, Batch: 697, Loss: 0.7313984632492065\n",
      "Epoch: 1, Batch: 698, Loss: 0.6524723768234253\n",
      "Epoch: 1, Batch: 699, Loss: 0.5621534585952759\n",
      "Epoch: 1, Batch: 700, Loss: 0.6127629280090332\n",
      "Epoch: 1, Batch: 701, Loss: 0.6873502731323242\n",
      "Epoch: 1, Batch: 702, Loss: 0.7056310176849365\n",
      "Epoch: 1, Batch: 703, Loss: 0.6827847361564636\n",
      "Epoch: 1, Batch: 704, Loss: 0.6461985111236572\n",
      "Epoch: 1, Batch: 705, Loss: 0.6587986946105957\n",
      "Epoch: 1, Batch: 706, Loss: 0.5870494842529297\n",
      "Epoch: 1, Batch: 707, Loss: 0.5597662925720215\n",
      "Epoch: 1, Batch: 708, Loss: 0.6209402084350586\n",
      "Epoch: 1, Batch: 709, Loss: 0.6852648258209229\n",
      "Epoch: 1, Batch: 710, Loss: 0.775425374507904\n",
      "Epoch: 1, Batch: 711, Loss: 0.7058099508285522\n",
      "Epoch: 1, Batch: 712, Loss: 0.6306599378585815\n",
      "Epoch: 1, Batch: 713, Loss: 0.6114407777786255\n",
      "Epoch: 1, Batch: 714, Loss: 0.5611660480499268\n",
      "Epoch: 1, Batch: 715, Loss: 0.6132280826568604\n",
      "Epoch: 1, Batch: 716, Loss: 0.7155666947364807\n",
      "Epoch: 1, Batch: 717, Loss: 0.5628967881202698\n",
      "Epoch: 1, Batch: 718, Loss: 0.7037063241004944\n",
      "Epoch: 1, Batch: 719, Loss: 0.6454029083251953\n",
      "Epoch: 1, Batch: 720, Loss: 0.6062894463539124\n",
      "Epoch: 1, Batch: 721, Loss: 0.6483932137489319\n",
      "Epoch: 1, Batch: 722, Loss: 0.6131064891815186\n",
      "Epoch: 1, Batch: 723, Loss: 0.6477212905883789\n",
      "Epoch: 1, Batch: 724, Loss: 0.6886191368103027\n",
      "Epoch: 1, Batch: 725, Loss: 0.6516417264938354\n",
      "Epoch: 1, Batch: 726, Loss: 0.6800897717475891\n",
      "Epoch: 1, Batch: 727, Loss: 0.6836572885513306\n",
      "Epoch: 1, Batch: 728, Loss: 0.5832241773605347\n",
      "Epoch: 1, Batch: 729, Loss: 0.6292853951454163\n",
      "Epoch: 1, Batch: 730, Loss: 0.6639741659164429\n",
      "Epoch: 1, Batch: 731, Loss: 0.707512617111206\n",
      "Epoch: 1, Batch: 732, Loss: 0.6123425364494324\n",
      "Epoch: 1, Batch: 733, Loss: 0.6840296387672424\n",
      "Epoch: 1, Batch: 734, Loss: 0.7018906474113464\n",
      "Epoch: 1, Batch: 735, Loss: 0.6186975240707397\n",
      "Epoch: 1, Batch: 736, Loss: 0.6336649656295776\n",
      "Epoch: 1, Batch: 737, Loss: 0.6650051474571228\n",
      "Epoch: 1, Batch: 738, Loss: 0.7069296836853027\n",
      "Epoch: 1, Batch: 739, Loss: 0.6534067988395691\n",
      "Epoch: 1, Batch: 740, Loss: 0.6905932426452637\n",
      "Epoch: 1, Batch: 741, Loss: 0.5831136703491211\n",
      "Epoch: 1, Batch: 742, Loss: 0.6523453593254089\n",
      "Epoch: 1, Batch: 743, Loss: 0.6660267114639282\n",
      "Epoch: 1, Batch: 744, Loss: 0.637450098991394\n",
      "Epoch: 1, Batch: 745, Loss: 0.6572021245956421\n",
      "Epoch: 1, Batch: 746, Loss: 0.738562822341919\n",
      "Epoch: 1, Batch: 747, Loss: 0.6652755737304688\n",
      "Epoch: 1, Batch: 748, Loss: 0.6316696405410767\n",
      "Epoch: 1, Batch: 749, Loss: 0.7189323902130127\n",
      "Epoch: 1, Batch: 750, Loss: 0.6462793946266174\n",
      "Epoch: 1, Batch: 751, Loss: 0.7036572098731995\n",
      "Epoch: 1, Batch: 752, Loss: 0.6532242298126221\n",
      "Epoch: 1, Batch: 753, Loss: 0.5654324293136597\n",
      "Epoch: 1, Batch: 754, Loss: 0.6224336624145508\n",
      "Epoch: 1, Batch: 755, Loss: 0.653678834438324\n",
      "Epoch: 1, Batch: 756, Loss: 0.6075443029403687\n",
      "Epoch: 1, Batch: 757, Loss: 0.661702036857605\n",
      "Epoch: 1, Batch: 758, Loss: 0.6072322726249695\n",
      "Epoch: 1, Batch: 759, Loss: 0.6116662621498108\n",
      "Epoch: 1, Batch: 760, Loss: 0.6694417595863342\n",
      "Epoch: 1, Batch: 761, Loss: 0.6519849300384521\n",
      "Epoch: 1, Batch: 762, Loss: 0.6193509101867676\n",
      "Epoch: 1, Batch: 763, Loss: 0.6676623821258545\n",
      "Epoch: 1, Batch: 764, Loss: 0.615585446357727\n",
      "Epoch: 1, Batch: 765, Loss: 0.5954238176345825\n",
      "Epoch: 1, Batch: 766, Loss: 0.7016198039054871\n",
      "Epoch: 1, Batch: 767, Loss: 0.5594760179519653\n",
      "Epoch: 1, Batch: 768, Loss: 0.6964701414108276\n",
      "Epoch: 1, Batch: 769, Loss: 0.6475518345832825\n",
      "Epoch: 1, Batch: 770, Loss: 0.6404274702072144\n",
      "Epoch: 1, Batch: 771, Loss: 0.6350764036178589\n",
      "Epoch: 1, Batch: 772, Loss: 0.6347953081130981\n",
      "Epoch: 1, Batch: 773, Loss: 0.592735767364502\n",
      "Epoch: 1, Batch: 774, Loss: 0.6823121309280396\n",
      "Epoch: 1, Batch: 775, Loss: 0.690345048904419\n",
      "Epoch: 1, Batch: 776, Loss: 0.5959995985031128\n",
      "Epoch: 1, Batch: 777, Loss: 0.6545015573501587\n",
      "Epoch: 1, Batch: 778, Loss: 0.6823385953903198\n",
      "Epoch: 1, Batch: 779, Loss: 0.7195574641227722\n",
      "Epoch: 1, Batch: 780, Loss: 0.6163792610168457\n",
      "Epoch: 1, Batch: 781, Loss: 0.8212010264396667\n",
      "Epoch: 1, Batch: 782, Loss: 0.7387995719909668\n",
      "Epoch: 1, Batch: 783, Loss: 0.7910786271095276\n",
      "Epoch: 1, Batch: 784, Loss: 0.6953292489051819\n",
      "Epoch: 1, Batch: 785, Loss: 0.7309499382972717\n",
      "Epoch: 1, Batch: 786, Loss: 0.7200540900230408\n",
      "Epoch: 1, Batch: 787, Loss: 0.6355044841766357\n",
      "Epoch: 1, Batch: 788, Loss: 0.5797139406204224\n",
      "Epoch: 1, Batch: 789, Loss: 0.6268173456192017\n",
      "Epoch: 1, Batch: 790, Loss: 0.7338930368423462\n",
      "Epoch: 1, Batch: 791, Loss: 0.7193847894668579\n",
      "Epoch: 1, Batch: 792, Loss: 0.6005897521972656\n",
      "Epoch: 1, Batch: 793, Loss: 0.5872159600257874\n",
      "Epoch: 1, Batch: 794, Loss: 0.707565426826477\n",
      "Epoch: 1, Batch: 795, Loss: 0.65726238489151\n",
      "Epoch: 1, Batch: 796, Loss: 0.6288962364196777\n",
      "Epoch: 1, Batch: 797, Loss: 0.6181724667549133\n",
      "Epoch: 1, Batch: 798, Loss: 0.6656323671340942\n",
      "Epoch: 1, Batch: 799, Loss: 0.6645758152008057\n",
      "Epoch: 1, Batch: 800, Loss: 0.6698800325393677\n",
      "Epoch: 1, Batch: 801, Loss: 0.652318000793457\n",
      "Epoch: 1, Batch: 802, Loss: 0.7302371263504028\n",
      "Epoch: 1, Batch: 803, Loss: 0.6931437253952026\n",
      "Epoch: 1, Batch: 804, Loss: 0.6850975751876831\n",
      "Epoch: 1, Batch: 805, Loss: 0.6532493829727173\n",
      "Epoch: 1, Batch: 806, Loss: 0.7314963340759277\n",
      "Epoch: 1, Batch: 807, Loss: 0.7027039527893066\n",
      "Epoch: 1, Batch: 808, Loss: 0.7225796580314636\n",
      "Epoch: 1, Batch: 809, Loss: 0.6249254941940308\n",
      "Epoch: 1, Batch: 810, Loss: 0.5786004066467285\n",
      "Epoch: 1, Batch: 811, Loss: 0.6477353572845459\n",
      "Epoch: 1, Batch: 812, Loss: 0.6620221138000488\n",
      "Epoch: 1, Batch: 813, Loss: 0.7169230580329895\n",
      "Epoch: 1, Batch: 814, Loss: 0.7002501487731934\n",
      "Epoch: 1, Batch: 815, Loss: 0.7203270196914673\n",
      "Epoch: 1, Batch: 816, Loss: 0.7069083452224731\n",
      "Epoch: 1, Batch: 817, Loss: 0.6612246632575989\n",
      "Epoch: 1, Batch: 818, Loss: 0.6756567358970642\n",
      "Epoch: 1, Batch: 819, Loss: 0.6885089874267578\n",
      "Epoch: 1, Batch: 820, Loss: 0.5311247110366821\n",
      "Epoch: 1, Batch: 821, Loss: 0.6388201713562012\n",
      "Epoch: 1, Batch: 822, Loss: 0.6531263589859009\n",
      "Epoch: 1, Batch: 823, Loss: 0.6870989799499512\n",
      "Epoch: 1, Batch: 824, Loss: 0.6808912754058838\n",
      "Epoch: 1, Batch: 825, Loss: 0.6669917106628418\n",
      "Epoch: 1, Batch: 826, Loss: 0.6363730430603027\n",
      "Epoch: 1, Batch: 827, Loss: 0.5705811381340027\n",
      "Epoch: 1, Batch: 828, Loss: 0.689667820930481\n",
      "Epoch: 1, Batch: 829, Loss: 0.7062323689460754\n",
      "Epoch: 1, Batch: 830, Loss: 0.6678783893585205\n",
      "Epoch: 1, Batch: 831, Loss: 0.6138355731964111\n",
      "Epoch: 1, Batch: 832, Loss: 0.49831056594848633\n",
      "Epoch: 1, Batch: 833, Loss: 0.6862971782684326\n",
      "Epoch: 1, Batch: 834, Loss: 0.6535292267799377\n",
      "Epoch: 1, Batch: 835, Loss: 0.6947168111801147\n",
      "Epoch: 1, Batch: 836, Loss: 0.620501697063446\n",
      "Epoch: 1, Batch: 837, Loss: 0.6918653249740601\n",
      "Epoch: 1, Batch: 838, Loss: 0.7041264176368713\n",
      "Epoch: 1, Batch: 839, Loss: 0.6465889811515808\n",
      "Epoch: 1, Batch: 840, Loss: 0.6459569931030273\n",
      "Epoch: 1, Batch: 841, Loss: 0.6803628206253052\n",
      "Epoch: 1, Batch: 842, Loss: 0.6703598499298096\n",
      "Epoch: 1, Batch: 843, Loss: 0.5540180206298828\n",
      "Epoch: 1, Batch: 844, Loss: 0.7884705066680908\n",
      "Epoch: 1, Batch: 845, Loss: 0.7284433841705322\n",
      "Epoch: 1, Batch: 846, Loss: 0.6087807416915894\n",
      "Epoch: 1, Batch: 847, Loss: 0.6326705813407898\n",
      "Epoch: 1, Batch: 848, Loss: 0.5728405117988586\n",
      "Epoch: 1, Batch: 849, Loss: 0.6880294680595398\n",
      "Epoch: 1, Batch: 850, Loss: 0.6725649237632751\n",
      "Epoch: 1, Batch: 851, Loss: 0.6549011468887329\n",
      "Epoch: 1, Batch: 852, Loss: 0.666412353515625\n",
      "Epoch: 1, Batch: 853, Loss: 0.681842565536499\n",
      "Epoch: 1, Batch: 854, Loss: 0.6111368536949158\n",
      "Epoch: 1, Batch: 855, Loss: 0.5506772398948669\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m loss:torch.Tensor = criterion(output.contiguous(), labels.contiguous())\n\u001b[32m     34\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m lr_scheduler.step()\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recsys/.venv/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:133\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m opt = opt_ref()\n\u001b[32m    132\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recsys/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:516\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    513\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recsys/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recsys/.venv/lib/python3.13/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recsys/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:149\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recsys/.venv/lib/python3.13/site-packages/torch/optim/adam.py:949\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    947\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recsys/.venv/lib/python3.13/site-packages/torch/optim/adam.py:535\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    533\u001b[39m         denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch.is_complex(params[i]):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 10    # number of epochs to run\n",
    "batch_size = 128  # size of each batch\n",
    "batches_per_epoch = n // batch_size\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "lr_scheduler = CosineWarmupScheduler(optimizer, warmup=50, max_iters=batches_per_epoch*n_epochs)\n",
    "\n",
    "best_vloss = float(\"Inf\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    rec.train()\n",
    "    batch_iter = prepare_batches(n, m, columns, batch_size=batch_size)\n",
    "    i = 0\n",
    "    while True:\n",
    "        optimizer.zero_grad()\n",
    "        batch = next(batch_iter)\n",
    "\n",
    "        if batch:\n",
    "            data, labels = batch\n",
    "            user_ids, user_prev_rated_movie_ids, user_prev_ratings, movie_ids, movie_descriptions, movie_genres, movie_years = data\n",
    "\n",
    "            output:torch.Tensor = \\\n",
    "                rec(\n",
    "                    user_ids,\n",
    "                    user_prev_rated_movie_ids, \n",
    "                    user_prev_ratings,\n",
    "                    movie_ids, \n",
    "                    movie_descriptions, \n",
    "                    movie_genres, \n",
    "                    movie_years\n",
    "                )\n",
    "            \n",
    "            loss:torch.Tensor = criterion(output.contiguous(), labels.contiguous())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            print(f\"Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item()}\")\n",
    "        else:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87585"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary[\"movieId\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87585"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([v for k, v in vocabulary[\"movieId\"].items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
