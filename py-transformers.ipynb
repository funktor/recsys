{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --quiet \"pytorch-lightning >=2.0,<2.6\" \"matplotlib\" \"torch >=1.8.1,<2.8\" \"seaborn\" \"torchmetrics >=1.0,<1.8\" \"numpy <3.0\" \"torchvision\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import math\n",
    "import os\n",
    "import urllib.request\n",
    "from functools import partial\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "plt.set_cmap(\"cividis\")\n",
    "%matplotlib inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_orig()\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q:torch.Tensor, k:torch.Tensor, v:torch.Tensor, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attn_logits, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " tensor([[ 0.3367,  0.1288],\n",
      "        [ 0.2345,  0.2303],\n",
      "        [-1.1229, -0.1863]])\n",
      "K\n",
      " tensor([[ 2.2082, -0.6380],\n",
      "        [ 0.4617,  0.2674],\n",
      "        [ 0.5349,  0.8094]])\n",
      "V\n",
      " tensor([[ 1.1103, -1.6898],\n",
      "        [-0.9890,  0.9580],\n",
      "        [ 1.3221,  0.8172]])\n",
      "Values\n",
      " tensor([[ 0.5698, -0.1520],\n",
      "        [ 0.5379, -0.0265],\n",
      "        [ 0.2246,  0.5556]])\n",
      "Attention\n",
      " tensor([[0.4028, 0.2886, 0.3086],\n",
      "        [0.3538, 0.3069, 0.3393],\n",
      "        [0.1303, 0.4630, 0.4067]])\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "pl.seed_everything(42)\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3 * embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x:torch.Tensor, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv:torch.Tensor = self.qkv_proj(x)\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)  # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3)  # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"EncoderBlock.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Dimensionality of the input\n",
    "            num_heads: Number of heads to use in the attention block\n",
    "            dim_feedforward: Dimensionality of the hidden layer in the MLP\n",
    "            dropout: Dropout probability to use in the dropout layers\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim),\n",
    "        )\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # MLP part\n",
    "        linear_out = self.linear_net(x)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for layer in self.layers:\n",
    "            _, attn_map = layer.self_attn(x, mask=mask, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = layer(x)\n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"Positional Encoding.\n",
    "\n",
    "        Args:\n",
    "            d_model: Hidden dimensionality of the input.\n",
    "            max_len: Maximum length of a sequence to expect.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer(\"pe\", pe, persistent=False)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "d_model = 32\n",
    "pe = torch.zeros(max_len, d_model)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "pe = pe.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe[:, :20].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPredictor(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        model_dim,\n",
    "        num_classes,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        lr,\n",
    "        warmup,\n",
    "        max_iters,\n",
    "        dropout=0.0,\n",
    "        input_dropout=0.0,\n",
    "    ):\n",
    "        \"\"\"TransformerPredictor.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Hidden dimensionality of the input\n",
    "            model_dim: Hidden dimensionality to use inside the Transformer\n",
    "            num_classes: Number of classes to predict per sequence element\n",
    "            num_heads: Number of heads to use in the Multi-Head Attention blocks\n",
    "            num_layers: Number of encoder blocks to use.\n",
    "            lr: Learning rate in the optimizer\n",
    "            warmup: Number of warmup steps. Usually between 50 and 500\n",
    "            max_iters: Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
    "            dropout: Dropout to apply inside the model\n",
    "            input_dropout: Dropout to apply on the input features\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self._create_model()\n",
    "\n",
    "    def _create_model(self):\n",
    "        # Input dim -> Model dim\n",
    "        self.input_net = nn.Sequential(\n",
    "            nn.Dropout(self.hparams.input_dropout), nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n",
    "        )\n",
    "        # Positional encoding for sequences\n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(\n",
    "            num_layers=self.hparams.num_layers,\n",
    "            input_dim=self.hparams.model_dim,\n",
    "            dim_feedforward=2 * self.hparams.model_dim,\n",
    "            num_heads=self.hparams.num_heads,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "        # Output classifier per sequence element\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
    "            nn.LayerNorm(self.hparams.model_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.hparams.dropout),\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input features of shape [Batch, SeqLen, input_dim]\n",
    "            mask: Mask to apply on the attention outputs (optional)\n",
    "            add_positional_encoding: If True, we add the positional encoding to the input.\n",
    "                                      Might not be desired for some tasks.\n",
    "        \"\"\"\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        x = self.output_net(x)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
    "        \"\"\"Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
    "\n",
    "        Input arguments same as the forward pass.\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
    "        return attention_maps\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "        # We don't return the lr scheduler because we need to apply it per iteration, not per epoch\n",
    "        self.lr_scheduler = CosineWarmupScheduler(\n",
    "            optimizer, warmup=self.hparams.warmup, max_iters=self.hparams.max_iters\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def optimizer_step(self, *args, **kwargs):\n",
    "        super().optimizer_step(*args, **kwargs)\n",
    "        self.lr_scheduler.step()  # Step per iteration\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDataset(data.Dataset):\n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "\n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        labels = torch.flip(inp_data, dims=(0,))\n",
    "        return inp_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = partial(ReverseDataset, 10, 16)\n",
    "train_loader = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader = data.DataLoader(dataset(1000), batch_size=128)\n",
    "test_loader = data.DataLoader(dataset(10000), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([9, 6, 2, 0, 6, 2, 7, 9, 7, 3, 3, 4, 3, 7, 0, 9])\n",
      "Labels:     tensor([9, 0, 7, 3, 4, 3, 3, 7, 9, 7, 2, 6, 0, 2, 6, 9])\n"
     ]
    }
   ],
   "source": [
    "inp_data, labels = train_loader.dataset[0]\n",
    "print(\"Input data:\", inp_data)\n",
    "print(\"Labels:    \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversePredictor(TransformerPredictor):\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        # Fetch data and transform categories to one-hot vectors\n",
    "        inp_data, labels = batch\n",
    "        inp_data = F.one_hot(inp_data, num_classes=self.hparams.num_classes).float()\n",
    "\n",
    "        # Perform prediction and calculate loss and accuracy\n",
    "        preds = self.forward(inp_data, add_positional_encoding=True)\n",
    "        loss = F.cross_entropy(preds.view(-1, preds.size(-1)), labels.view(-1))\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        # Logging\n",
    "        self.log(f\"{mode}_loss\", loss)\n",
    "        self.log(f\"{mode}_acc\", acc)\n",
    "        return loss, acc\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reverse(**kwargs):\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"mps\",\n",
    "        devices=1,\n",
    "        max_epochs=10,\n",
    "        gradient_clip_val=5,\n",
    "    )\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "    \n",
    "    model = ReversePredictor(max_iters=trainer.max_epochs * len(train_loader), **kwargs)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    model = model.to(device)\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                | Type               | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | input_net           | Sequential         | 352    | train\n",
      "1 | positional_encoding | PositionalEncoding | 0      | train\n",
      "2 | transformer         | TransformerEncoder | 8.5 K  | train\n",
      "3 | output_net          | Sequential         | 1.4 K  | train\n",
      "-------------------------------------------------------------------\n",
      "10.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.3 K    Total params\n",
      "0.041     Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 390/390 [00:03<00:00, 128.20it/s, v_num=9]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 390/390 [00:03<00:00, 127.71it/s, v_num=9]\n",
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 281.36it/s]\n",
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 385.64it/s]\n"
     ]
    }
   ],
   "source": [
    "reverse_model, reverse_result = train_reverse(\n",
    "    input_dim=train_loader.dataset.num_categories,\n",
    "    model_dim=32,\n",
    "    num_heads=1,\n",
    "    num_classes=train_loader.dataset.num_categories,\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    lr=5e-4,\n",
    "    warmup=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy:  73.88%\n",
      "Test accuracy: 73.92%\n"
     ]
    }
   ],
   "source": [
    "print(\"Val accuracy:  %4.2f%%\" % (100.0 * reverse_result[\"val_acc\"]))\n",
    "print(\"Test accuracy: %4.2f%%\" % (100.0 * reverse_result[\"test_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"datasets/ml-32m\"\n",
    "\n",
    "ratings_path = os.path.join(folder, 'ratings.csv')\n",
    "movies_path = os.path.join(folder, 'movies.csv')\n",
    "tags_path = os.path.join(folder, 'tags.csv')\n",
    "\n",
    "rating_column_names = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "movies_column_names = ['movieId', 'title', 'genres']\n",
    "tags_column_names = ['userId', 'movieId', 'tag', 'timestamp']\n",
    "\n",
    "df_ratings = pd.read_csv(ratings_path, sep=',', names=rating_column_names, dtype={'userId':'int32', 'movieId':'int32', 'rating':float, 'timestamp':'int64'}, header=0)\n",
    "df_movies = pd.read_csv(movies_path, sep=',', names=movies_column_names, dtype={'movieId':'int32', 'title':'object', 'genres':'object'}, header=0)\n",
    "df_tags = pd.read_csv(tags_path, sep=',', names=tags_column_names, dtype={'userId':'int32', 'movieId':'int32', 'tag':'object', 'timestamp':'int64'}, header=0)\n",
    "\n",
    "df_ratings.dropna(inplace=True, subset=['userId', 'movieId', 'rating'])\n",
    "df_movies.dropna(inplace=True, subset=['movieId', 'title', 'genres'])\n",
    "df_tags.dropna(inplace=True, subset=['userId', 'movieId', 'tag'])\n",
    "df_tags.drop(columns=[\"userId\",\"timestamp\"], inplace=True)\n",
    "\n",
    "# Extract movie genres\n",
    "df_movies['genres'] = df_movies['genres'].apply(lambda x: x.lower().split('|'))\n",
    "\n",
    "# Extract movie year from title\n",
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "def remove_stop(x):\n",
    "    out = []\n",
    "    for y in x:\n",
    "        if len(y) > 0 and y not in stopwords:\n",
    "            out += [y]\n",
    "    return out\n",
    "\n",
    "def flatten_lists(x):\n",
    "    out = set()\n",
    "    for y in x:\n",
    "        out.update(y.split(\" \"))\n",
    "    out = list(out)\n",
    "    return out\n",
    "\n",
    "df_movies['movie_year'] = df_movies['title'].str.extract(r'\\((\\d{4})\\)').fillna(\"2025\").astype('int')\n",
    "\n",
    "df_movies['title'] = df_movies['title'].str.replace(r'\\((\\d{4})\\)', '', regex=True)\n",
    "df_movies['title'] = df_movies['title'].str.replace(r'[^a-zA-Z0-9\\s]+', '', regex=True)\n",
    "df_movies['title'] = df_movies['title'].apply(lambda x: x.strip().lower().split(\" \"))\n",
    "df_movies['title'] = df_movies['title'].apply(lambda x: remove_stop(x))\n",
    "\n",
    "df_tags['tag'] = df_tags['tag'].str.replace(r'[^a-zA-Z0-9\\s]+', '', regex=True)\n",
    "df_tags['tag'] = df_tags['tag'].apply(lambda x: x.strip().lower())\n",
    "df_tags = df_tags.groupby(\"movieId\").agg(set).reset_index()\n",
    "df_tags['tag'] = df_tags['tag'].apply(list)\n",
    "df_tags['tag'] = df_tags['tag'].apply(lambda x: flatten_lists(x))\n",
    "df_tags['tag'] = df_tags['tag'].apply(lambda x: remove_stop(x))\n",
    "df_tags['tag'] = df_tags['tag'].astype(\"object\")\n",
    "\n",
    "df_movies = df_movies.merge(df_tags, on=['movieId'], how='left')\n",
    "df_movies[\"tag\"] = df_movies[\"tag\"].fillna({i: [\"\"] for i in df_movies.index})\n",
    "df_movies[\"description\"] = df_movies[\"title\"] + df_movies[\"tag\"]\n",
    "df_movies.drop(columns=[\"tag\"], inplace=True)\n",
    "df_movies.drop(columns=[\"title\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>944249077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>944250228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>943230976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>5.0</td>\n",
       "      <td>944249077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>5.0</td>\n",
       "      <td>943228858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>1944</td>\n",
       "      <td>2.0</td>\n",
       "      <td>943231120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1952</td>\n",
       "      <td>4.0</td>\n",
       "      <td>944253272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>1960</td>\n",
       "      <td>1.0</td>\n",
       "      <td>943231236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>944250182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>1965</td>\n",
       "      <td>3.0</td>\n",
       "      <td>943228697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  rating  timestamp\n",
       "0        1       17     4.0  944249077\n",
       "1        1       25     1.0  944250228\n",
       "2        1       29     2.0  943230976\n",
       "3        1       30     5.0  944249077\n",
       "4        1       32     5.0  943228858\n",
       "..     ...      ...     ...        ...\n",
       "95       1     1944     2.0  943231120\n",
       "96       1     1952     4.0  944253272\n",
       "97       1     1960     1.0  943231236\n",
       "98       1     1961     1.0  944250182\n",
       "99       1     1965     3.0  943228697\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>genres</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[adventure, animation, children, comedy, fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[toy, story, robot, antenna, slow, karate, cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[adventure, children, fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[jumanji, coming, effect, small, poison, blund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[comedy, romance]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[grumpier, old, men, duringcreditsstinger, mid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[comedy, drama, romance]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[waiting, exhale, slurs, divorce, flick, inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[comedy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[father, bride, part, ii, aging, parent, famil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>97</td>\n",
       "      <td>[crime, drama]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[hate, haine, la, biting, jockey, cow, disc, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>98</td>\n",
       "      <td>[action, thriller]</td>\n",
       "      <td>1994</td>\n",
       "      <td>[shopping, want, law, directorial, debut, jude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>99</td>\n",
       "      <td>[documentary]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[heidi, fleiss, hollywood, madam, hollywood, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>100</td>\n",
       "      <td>[drama, thriller]</td>\n",
       "      <td>1996</td>\n",
       "      <td>[city, hall, election, politics, investigation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>101</td>\n",
       "      <td>[adventure, comedy, crime, romance]</td>\n",
       "      <td>1996</td>\n",
       "      <td>[bottle, rocket, gas, talky, quirky, slow, sho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    movieId                                             genres  movie_year  \\\n",
       "0         1  [adventure, animation, children, comedy, fantasy]        1995   \n",
       "1         2                     [adventure, children, fantasy]        1995   \n",
       "2         3                                  [comedy, romance]        1995   \n",
       "3         4                           [comedy, drama, romance]        1995   \n",
       "4         5                                           [comedy]        1995   \n",
       "..      ...                                                ...         ...   \n",
       "95       97                                     [crime, drama]        1995   \n",
       "96       98                                 [action, thriller]        1994   \n",
       "97       99                                      [documentary]        1995   \n",
       "98      100                                  [drama, thriller]        1996   \n",
       "99      101                [adventure, comedy, crime, romance]        1996   \n",
       "\n",
       "                                          description  \n",
       "0   [toy, story, robot, antenna, slow, karate, cha...  \n",
       "1   [jumanji, coming, effect, small, poison, blund...  \n",
       "2   [grumpier, old, men, duringcreditsstinger, mid...  \n",
       "3   [waiting, exhale, slurs, divorce, flick, inter...  \n",
       "4   [father, bride, part, ii, aging, parent, famil...  \n",
       "..                                                ...  \n",
       "95  [hate, haine, la, biting, jockey, cow, disc, t...  \n",
       "96    [shopping, want, law, directorial, debut, jude]  \n",
       "97  [heidi, fleiss, hollywood, madam, hollywood, p...  \n",
       "98  [city, hall, election, politics, investigation...  \n",
       "99  [bottle, rocket, gas, talky, quirky, slow, sho...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ratings(df:pd.DataFrame):\n",
    "    df2 = df[[\"userId\", \"rating\"]].groupby(by=[\"userId\"]).agg(mean_user_rating=('rating', 'mean'), std_user_rating=('rating', 'std'))\n",
    "    df = df.merge(df2, on=[\"userId\"], how=\"inner\")\n",
    "    df[\"normalized_rating\"] = (df[\"rating\"] - df[\"mean_user_rating\"])/df[\"std_user_rating\"]\n",
    "    df[\"normalized_rating\"] = df[\"normalized_rating\"].fillna(df[\"rating\"])\n",
    "    df.drop(columns=[\"mean_user_rating\", \"std_user_rating\", \"rating\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = normalize_ratings(df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings['label'] = [0 for _ in range(len(df_ratings))]\n",
    "df_ratings['label'] = np.where(df_ratings[\"normalized_rating\"] > 0, 1, df_ratings['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>normalized_rating</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>944249077</td>\n",
       "      <td>0.304372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>944250228</td>\n",
       "      <td>-1.646377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>943230976</td>\n",
       "      <td>-0.996127</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>944249077</td>\n",
       "      <td>0.954622</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>943228858</td>\n",
       "      <td>0.954622</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>1944</td>\n",
       "      <td>943231120</td>\n",
       "      <td>-0.996127</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1952</td>\n",
       "      <td>944253272</td>\n",
       "      <td>0.304372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>1960</td>\n",
       "      <td>943231236</td>\n",
       "      <td>-1.646377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1961</td>\n",
       "      <td>944250182</td>\n",
       "      <td>-1.646377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>1965</td>\n",
       "      <td>943228697</td>\n",
       "      <td>-0.345878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  timestamp  normalized_rating  label\n",
       "0        1       17  944249077           0.304372      1\n",
       "1        1       25  944250228          -1.646377      0\n",
       "2        1       29  943230976          -0.996127      0\n",
       "3        1       30  944249077           0.954622      1\n",
       "4        1       32  943228858           0.954622      1\n",
       "..     ...      ...        ...                ...    ...\n",
       "95       1     1944  943231120          -0.996127      0\n",
       "96       1     1952  944253272           0.304372      1\n",
       "97       1     1960  943231236          -1.646377      0\n",
       "98       1     1961  944250182          -1.646377      0\n",
       "99       1     1965  943228697          -0.345878      0\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df:pd.DataFrame, min_rated=10, test_ratio=0.8, val_ratio=0.8):\n",
    "    print(\"Splitting data into train test and validation...\")\n",
    "    # Split data into training, testing and validation\n",
    "    df = df.sort_values(by='timestamp')\n",
    "    df2 = df[[\"userId\", \"movieId\"]].groupby(by=[\"userId\"]).agg(list).reset_index()\n",
    "\n",
    "    # Filter all user_ids who have rated more than 'min_rated' movies\n",
    "    df2 = df2[df2.movieId.apply(len) > min_rated]\n",
    "    df = df.merge(df2, on=[\"userId\"], how=\"inner\", suffixes=(\"\", \"_right\"))\n",
    "    df.drop(columns=['movieId_right'], inplace=True)\n",
    "\n",
    "    n = df.shape[0]\n",
    "    m = int(test_ratio*n)\n",
    "\n",
    "    df_train_val = df[:m]\n",
    "    df_test = df[m:]\n",
    "\n",
    "    k = int(val_ratio*m)\n",
    "    df_train = df_train_val[:k]\n",
    "    df_val = df_train_val[k:]\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train test and validation...\n"
     ]
    }
   ],
   "source": [
    "df_ratings_train, df_ratings_val, df_ratings_test = split_train_test(df_ratings, min_rated=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_train.sort_values(by=[\"userId\", \"timestamp\"], inplace=True)\n",
    "df_ratings_val.sort_values(by=[\"userId\", \"timestamp\"], inplace=True)\n",
    "df_ratings_test.sort_values(by=[\"userId\", \"timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x, vocab):\n",
    "    if isinstance(x, list):\n",
    "        out = []\n",
    "        for y in x:\n",
    "            out += [vocab[y]] if y in vocab else [0]\n",
    "        return out\n",
    "    else:\n",
    "        return vocab[x] if x in vocab else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_encoding(df:pd.DataFrame, col:str, max_vocab_size=1000):\n",
    "    all_vals = df[col].tolist()\n",
    "    unique_vals = {}\n",
    "\n",
    "    if len(all_vals) > 0 and isinstance(all_vals[0], list):\n",
    "        for v in all_vals:\n",
    "            for x in v:\n",
    "                if x not in unique_vals:\n",
    "                    unique_vals[x] = 0\n",
    "                unique_vals[x] += 1\n",
    "    else:\n",
    "        for x in all_vals:\n",
    "            if x not in unique_vals:\n",
    "                unique_vals[x] = 0\n",
    "            unique_vals[x] += 1\n",
    "    \n",
    "    unique_vals = sorted(unique_vals.items(), key=lambda item: item[1], reverse=True)\n",
    "    unique_vals = dict(unique_vals[:min(max_vocab_size, len(unique_vals))])\n",
    "    unique_vals = sorted(unique_vals.keys())\n",
    "    vocab = {unique_vals[i] : i+1 for i in range(len(unique_vals))}\n",
    "        \n",
    "    df[col] = df[col].apply(lambda x: transform(x, vocab))\n",
    "    return df[col], vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId\n",
      "movieId\n",
      "description\n",
      "genres\n",
      "movie_year\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "max_vocab_size = {'userId':1e100, 'movieId':1e100, 'description':1e5, 'genres':100, 'movie_year':1e100}\n",
    "\n",
    "for col in ['userId']:\n",
    "    print(col)\n",
    "    df_ratings_train[col], v = categorical_encoding(df_ratings_train, col, max_vocab_size[col])\n",
    "    vocabulary[col] = v\n",
    "\n",
    "for col in ['movieId', 'description', 'genres', 'movie_year']:\n",
    "    print(col)\n",
    "    df_movies[col], v = categorical_encoding(df_movies, col, max_vocab_size[col])\n",
    "    vocabulary[col] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId\n"
     ]
    }
   ],
   "source": [
    "df_ratings_val = df_ratings_val.reset_index()\n",
    "for col in ['userId']:\n",
    "    print(col)\n",
    "    df_ratings_val[col] = df_ratings_val[col].apply(lambda x: transform(x, vocabulary[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId\n"
     ]
    }
   ],
   "source": [
    "df_ratings_test = df_ratings_test.reset_index()\n",
    "for col in ['userId']:\n",
    "    print(col)\n",
    "    df_ratings_test[col] = df_ratings_test[col].apply(lambda x: transform(x, vocabulary[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_user_features(df:pd.DataFrame, max_hist=20):\n",
    "\tdf[\"seq_id\"] = list(range(df.shape[0]))\n",
    "\tdf2 = df[[\"seq_id\", \"userId\", \"movieId\", \"normalized_rating\", \"timestamp\"]].sort_values(by=[\"userId\", \"timestamp\"])\n",
    "\t\n",
    "\tdf2 = df2[[\"userId\", \"movieId\", \"normalized_rating\", \"seq_id\"]].groupby(by=[\"userId\"]).agg(list).reset_index()\n",
    "\tdf2.rename(columns={\"movieId\":\"prev_movie_ids\", \"normalized_rating\":\"prev_ratings\", \"seq_id\":\"prev_seq_ids\"}, inplace=True)\n",
    "\n",
    "\tuser_ids = []\n",
    "\tp_m_ids = []\n",
    "\tp_r_ids = []\n",
    "\tp_seq_ids = []\n",
    "\n",
    "\tfor i in range(df2.shape[0]):\n",
    "\t\tseq_id = df2.loc[i, \"prev_seq_ids\"]\n",
    "\t\tu_id   = df2.loc[i, \"userId\"]\n",
    "\t\tm_ids  = df2.loc[i, \"prev_movie_ids\"]\n",
    "\t\tr_ids  = df2.loc[i, \"prev_ratings\"]\n",
    "\n",
    "\t\tfor j in range(len(m_ids)):\n",
    "\t\t\tuser_ids += [u_id]\n",
    "\t\t\tp_seq_ids += [seq_id[j]]\n",
    "\t\t\tp_m_ids += [m_ids[:j][-max_hist:]] if j > 0 else [[]]\n",
    "\t\t\tp_r_ids += [r_ids[:j][-max_hist:]] if j > 0 else [[]]\n",
    "\t\n",
    "\tdf3 = pd.DataFrame({\"userId\":user_ids, \"prev_movie_ids\":p_m_ids, \"prev_ratings\":p_r_ids, \"seq_id\":p_seq_ids})\n",
    "\tdf = df.merge(df3, on=[\"userId\", \"seq_id\"], how=\"left\")\n",
    "\tdf.drop(columns=[\"seq_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ml_32m_py\n",
    "import numpy as np\n",
    "\n",
    "importlib.reload(ml_32m_py)\n",
    "\n",
    "def get_historical_user_features_cpp(df:pd.DataFrame, max_hist=20):\n",
    "        user_ids = df['userId'].to_numpy().astype(np.uint32)\n",
    "        movie_ids = df['movieId'].to_numpy().astype(np.uint32)\n",
    "        ratings = df['normalized_rating'].to_numpy().astype(np.float32)\n",
    "        timestamps = df['timestamp'].to_numpy().astype(np.uint64)\n",
    "\n",
    "        prev_movie_ids, prev_ratings  = ml_32m_py.py_get_historical_features(user_ids, movie_ids, timestamps, ratings, df.shape[0], max_hist)\n",
    "\n",
    "        df[\"prev_movie_ids\"] = prev_movie_ids\n",
    "        df[\"prev_ratings\"] = prev_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_historical_user_features_cpp(df_ratings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_historical_user_features_cpp(df_ratings_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_historical_user_features_cpp(df_ratings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df_movies.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(vocabulary, \"vocabulary.pkl\")\n",
    "joblib.dump(df_ratings_train, \"df_ratings_train.pkl\")\n",
    "joblib.dump(df_ratings_val, \"df_ratings_val.pkl\")\n",
    "joblib.dump(df_ratings_test, \"df_ratings_test.pkl\")\n",
    "joblib.dump(df_movies, \"df_movies.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>normalized_rating</th>\n",
       "      <th>label</th>\n",
       "      <th>prev_movie_ids</th>\n",
       "      <th>prev_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3237871</th>\n",
       "      <td>1</td>\n",
       "      <td>2997</td>\n",
       "      <td>943226846</td>\n",
       "      <td>0.304372</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237872</th>\n",
       "      <td>1</td>\n",
       "      <td>2966</td>\n",
       "      <td>943226846</td>\n",
       "      <td>-1.646377</td>\n",
       "      <td>0</td>\n",
       "      <td>[2997]</td>\n",
       "      <td>[0.30437225103378296]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237887</th>\n",
       "      <td>1</td>\n",
       "      <td>2890</td>\n",
       "      <td>943226916</td>\n",
       "      <td>0.304372</td>\n",
       "      <td>1</td>\n",
       "      <td>[2997, 2966]</td>\n",
       "      <td>[0.30437225103378296, -1.6463772058486938]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237908</th>\n",
       "      <td>1</td>\n",
       "      <td>3078</td>\n",
       "      <td>943226986</td>\n",
       "      <td>-0.996127</td>\n",
       "      <td>0</td>\n",
       "      <td>[2997, 2966, 2890]</td>\n",
       "      <td>[0.30437225103378296, -1.6463772058486938, 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237980</th>\n",
       "      <td>1</td>\n",
       "      <td>2882</td>\n",
       "      <td>943227458</td>\n",
       "      <td>-1.646377</td>\n",
       "      <td>0</td>\n",
       "      <td>[2997, 2966, 2890, 3078]</td>\n",
       "      <td>[0.30437225103378296, -1.6463772058486938, 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3305726</th>\n",
       "      <td>1</td>\n",
       "      <td>835</td>\n",
       "      <td>944248888</td>\n",
       "      <td>-0.345878</td>\n",
       "      <td>0</td>\n",
       "      <td>[1885, 1080, 176, 2973, 2243, 2502, 1060, 933,...</td>\n",
       "      <td>[0.30437225103378296, -1.6463772058486938, 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3305727</th>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "      <td>944248943</td>\n",
       "      <td>-0.996127</td>\n",
       "      <td>0</td>\n",
       "      <td>[1080, 176, 2973, 2243, 2502, 1060, 933, 1270,...</td>\n",
       "      <td>[-1.6463772058486938, 0.30437225103378296, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3305728</th>\n",
       "      <td>1</td>\n",
       "      <td>2268</td>\n",
       "      <td>944248943</td>\n",
       "      <td>-1.646377</td>\n",
       "      <td>0</td>\n",
       "      <td>[176, 2973, 2243, 2502, 1060, 933, 1270, 1259,...</td>\n",
       "      <td>[0.30437225103378296, 0.9546220898628235, -1.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3305729</th>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>944248943</td>\n",
       "      <td>0.954622</td>\n",
       "      <td>1</td>\n",
       "      <td>[2973, 2243, 2502, 1060, 933, 1270, 1259, 915,...</td>\n",
       "      <td>[0.9546220898628235, -1.6463772058486938, -0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3305730</th>\n",
       "      <td>1</td>\n",
       "      <td>1172</td>\n",
       "      <td>944248943</td>\n",
       "      <td>-0.345878</td>\n",
       "      <td>0</td>\n",
       "      <td>[2243, 2502, 1060, 933, 1270, 1259, 915, 223, ...</td>\n",
       "      <td>[-1.6463772058486938, -0.3458775579929352, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userId  movieId  timestamp  normalized_rating  label  \\\n",
       "3237871       1     2997  943226846           0.304372      1   \n",
       "3237872       1     2966  943226846          -1.646377      0   \n",
       "3237887       1     2890  943226916           0.304372      1   \n",
       "3237908       1     3078  943226986          -0.996127      0   \n",
       "3237980       1     2882  943227458          -1.646377      0   \n",
       "...         ...      ...        ...                ...    ...   \n",
       "3305726       1      835  944248888          -0.345878      0   \n",
       "3305727       1      608  944248943          -0.996127      0   \n",
       "3305728       1     2268  944248943          -1.646377      0   \n",
       "3305729       1       80  944248943           0.954622      1   \n",
       "3305730       1     1172  944248943          -0.345878      0   \n",
       "\n",
       "                                            prev_movie_ids  \\\n",
       "3237871                                                 []   \n",
       "3237872                                             [2997]   \n",
       "3237887                                       [2997, 2966]   \n",
       "3237908                                 [2997, 2966, 2890]   \n",
       "3237980                           [2997, 2966, 2890, 3078]   \n",
       "...                                                    ...   \n",
       "3305726  [1885, 1080, 176, 2973, 2243, 2502, 1060, 933,...   \n",
       "3305727  [1080, 176, 2973, 2243, 2502, 1060, 933, 1270,...   \n",
       "3305728  [176, 2973, 2243, 2502, 1060, 933, 1270, 1259,...   \n",
       "3305729  [2973, 2243, 2502, 1060, 933, 1270, 1259, 915,...   \n",
       "3305730  [2243, 2502, 1060, 933, 1270, 1259, 915, 223, ...   \n",
       "\n",
       "                                              prev_ratings  \n",
       "3237871                                                 []  \n",
       "3237872                              [0.30437225103378296]  \n",
       "3237887         [0.30437225103378296, -1.6463772058486938]  \n",
       "3237908  [0.30437225103378296, -1.6463772058486938, 0.3...  \n",
       "3237980  [0.30437225103378296, -1.6463772058486938, 0.3...  \n",
       "...                                                    ...  \n",
       "3305726  [0.30437225103378296, -1.6463772058486938, 0.3...  \n",
       "3305727  [-1.6463772058486938, 0.30437225103378296, 0.9...  \n",
       "3305728  [0.30437225103378296, 0.9546220898628235, -1.6...  \n",
       "3305729  [0.9546220898628235, -1.6463772058486938, -0.3...  \n",
       "3305730  [-1.6463772058486938, -0.3458775579929352, -0....  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_parts = 32\n",
    "df_ratings_train[\"partition\"] = [random.randint(1, num_parts) for _ in range(len(df_ratings_train))]\n",
    "df_ratings_val[\"partition\"]   = [random.randint(1, num_parts) for _ in range(len(df_ratings_val))]\n",
    "df_ratings_test[\"partition\"]  = [random.randint(1, num_parts) for _ in range(len(df_ratings_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "out_path = \"parquet_dataset_ml_32m/\"\n",
    "if os.path.exists(out_path):\n",
    "    try:\n",
    "        shutil.rmtree(out_path)\n",
    "    except:\n",
    "        pass\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "joblib.dump(vocabulary, f\"parquet_dataset_ml_32m/vocabulary.pkl\")\n",
    "df_ratings_train.to_parquet(out_path + \"train/\", partition_cols=[\"partition\"])\n",
    "df_ratings_val.to_parquet(out_path + \"validation/\", partition_cols=[\"partition\"])\n",
    "df_ratings_test.to_parquet(out_path + \"test/\", partition_cols=[\"partition\"])\n",
    "df_movies.to_parquet(out_path + \"movies.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>genres</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[3, 4, 5, 6, 10]</td>\n",
       "      <td>114</td>\n",
       "      <td>[67865, 63820, 56112, 3754, 61552, 35107, 1212...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[3, 5, 10]</td>\n",
       "      <td>114</td>\n",
       "      <td>[34536, 14196, 20591, 61609, 51356, 8627, 5045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[6, 16]</td>\n",
       "      <td>114</td>\n",
       "      <td>[28008, 47432, 42191, 20134, 42734, 1766, 4148...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[6, 9, 16]</td>\n",
       "      <td>114</td>\n",
       "      <td>[71951, 22280, 61583, 18900, 24140, 32889, 126...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[6]</td>\n",
       "      <td>114</td>\n",
       "      <td>[23069, 9709, 49188, 31779, 2197, 49084, 22827...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87580</th>\n",
       "      <td>87581</td>\n",
       "      <td>[9]</td>\n",
       "      <td>141</td>\n",
       "      <td>[43805, 2056, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87581</th>\n",
       "      <td>87582</td>\n",
       "      <td>[6, 9]</td>\n",
       "      <td>142</td>\n",
       "      <td>[60008, 62100, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87582</th>\n",
       "      <td>87583</td>\n",
       "      <td>[9]</td>\n",
       "      <td>142</td>\n",
       "      <td>[47819, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87583</th>\n",
       "      <td>87584</td>\n",
       "      <td>[9]</td>\n",
       "      <td>87</td>\n",
       "      <td>[3529, 9621, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87584</th>\n",
       "      <td>87585</td>\n",
       "      <td>[2, 3, 8]</td>\n",
       "      <td>142</td>\n",
       "      <td>[53734, 64532, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87585 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       movieId            genres  movie_year  \\\n",
       "0            1  [3, 4, 5, 6, 10]         114   \n",
       "1            2        [3, 5, 10]         114   \n",
       "2            3           [6, 16]         114   \n",
       "3            4        [6, 9, 16]         114   \n",
       "4            5               [6]         114   \n",
       "...        ...               ...         ...   \n",
       "87580    87581               [9]         141   \n",
       "87581    87582            [6, 9]         142   \n",
       "87582    87583               [9]         142   \n",
       "87583    87584               [9]          87   \n",
       "87584    87585         [2, 3, 8]         142   \n",
       "\n",
       "                                             description  \n",
       "0      [67865, 63820, 56112, 3754, 61552, 35107, 1212...  \n",
       "1      [34536, 14196, 20591, 61609, 51356, 8627, 5045...  \n",
       "2      [28008, 47432, 42191, 20134, 42734, 1766, 4148...  \n",
       "3      [71951, 22280, 61583, 18900, 24140, 32889, 126...  \n",
       "4      [23069, 9709, 49188, 31779, 2197, 49084, 22827...  \n",
       "...                                                  ...  \n",
       "87580                                   [43805, 2056, 1]  \n",
       "87581                                  [60008, 62100, 1]  \n",
       "87582                                         [47819, 1]  \n",
       "87583                                    [3529, 9621, 1]  \n",
       "87584                                  [53734, 64532, 1]  \n",
       "\n",
       "[87585 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>normalized_rating</th>\n",
       "      <th>label</th>\n",
       "      <th>prev_movie_ids</th>\n",
       "      <th>prev_ratings</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3237871</th>\n",
       "      <td>1</td>\n",
       "      <td>2997</td>\n",
       "      <td>943226846</td>\n",
       "      <td>0.304372</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237872</th>\n",
       "      <td>1</td>\n",
       "      <td>2966</td>\n",
       "      <td>943226846</td>\n",
       "      <td>-1.646377</td>\n",
       "      <td>0</td>\n",
       "      <td>[2997]</td>\n",
       "      <td>[0.30437225103378296]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237887</th>\n",
       "      <td>1</td>\n",
       "      <td>2890</td>\n",
       "      <td>943226916</td>\n",
       "      <td>0.304372</td>\n",
       "      <td>1</td>\n",
       "      <td>[2997, 2966]</td>\n",
       "      <td>[0.30437225103378296, -1.6463772058486938]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237908</th>\n",
       "      <td>1</td>\n",
       "      <td>3078</td>\n",
       "      <td>943226986</td>\n",
       "      <td>-0.996127</td>\n",
       "      <td>0</td>\n",
       "      <td>[2997, 2966, 2890]</td>\n",
       "      <td>[0.30437225103378296, -1.6463772058486938, 0.3...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237980</th>\n",
       "      <td>1</td>\n",
       "      <td>2882</td>\n",
       "      <td>943227458</td>\n",
       "      <td>-1.646377</td>\n",
       "      <td>0</td>\n",
       "      <td>[2997, 2966, 2890, 3078]</td>\n",
       "      <td>[0.30437225103378296, -1.6463772058486938, 0.3...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3305726</th>\n",
       "      <td>1</td>\n",
       "      <td>835</td>\n",
       "      <td>944248888</td>\n",
       "      <td>-0.345878</td>\n",
       "      <td>0</td>\n",
       "      <td>[1885, 1080, 176, 2973, 2243, 2502, 1060, 933,...</td>\n",
       "      <td>[0.30437225103378296, -1.6463772058486938, 0.3...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3305727</th>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "      <td>944248943</td>\n",
       "      <td>-0.996127</td>\n",
       "      <td>0</td>\n",
       "      <td>[1080, 176, 2973, 2243, 2502, 1060, 933, 1270,...</td>\n",
       "      <td>[-1.6463772058486938, 0.30437225103378296, 0.9...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3305728</th>\n",
       "      <td>1</td>\n",
       "      <td>2268</td>\n",
       "      <td>944248943</td>\n",
       "      <td>-1.646377</td>\n",
       "      <td>0</td>\n",
       "      <td>[176, 2973, 2243, 2502, 1060, 933, 1270, 1259,...</td>\n",
       "      <td>[0.30437225103378296, 0.9546220898628235, -1.6...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3305729</th>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>944248943</td>\n",
       "      <td>0.954622</td>\n",
       "      <td>1</td>\n",
       "      <td>[2973, 2243, 2502, 1060, 933, 1270, 1259, 915,...</td>\n",
       "      <td>[0.9546220898628235, -1.6463772058486938, -0.3...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3305730</th>\n",
       "      <td>1</td>\n",
       "      <td>1172</td>\n",
       "      <td>944248943</td>\n",
       "      <td>-0.345878</td>\n",
       "      <td>0</td>\n",
       "      <td>[2243, 2502, 1060, 933, 1270, 1259, 915, 223, ...</td>\n",
       "      <td>[-1.6463772058486938, -0.3458775579929352, -0....</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userId  movieId  timestamp  normalized_rating  label  \\\n",
       "3237871       1     2997  943226846           0.304372      1   \n",
       "3237872       1     2966  943226846          -1.646377      0   \n",
       "3237887       1     2890  943226916           0.304372      1   \n",
       "3237908       1     3078  943226986          -0.996127      0   \n",
       "3237980       1     2882  943227458          -1.646377      0   \n",
       "...         ...      ...        ...                ...    ...   \n",
       "3305726       1      835  944248888          -0.345878      0   \n",
       "3305727       1      608  944248943          -0.996127      0   \n",
       "3305728       1     2268  944248943          -1.646377      0   \n",
       "3305729       1       80  944248943           0.954622      1   \n",
       "3305730       1     1172  944248943          -0.345878      0   \n",
       "\n",
       "                                            prev_movie_ids  \\\n",
       "3237871                                                 []   \n",
       "3237872                                             [2997]   \n",
       "3237887                                       [2997, 2966]   \n",
       "3237908                                 [2997, 2966, 2890]   \n",
       "3237980                           [2997, 2966, 2890, 3078]   \n",
       "...                                                    ...   \n",
       "3305726  [1885, 1080, 176, 2973, 2243, 2502, 1060, 933,...   \n",
       "3305727  [1080, 176, 2973, 2243, 2502, 1060, 933, 1270,...   \n",
       "3305728  [176, 2973, 2243, 2502, 1060, 933, 1270, 1259,...   \n",
       "3305729  [2973, 2243, 2502, 1060, 933, 1270, 1259, 915,...   \n",
       "3305730  [2243, 2502, 1060, 933, 1270, 1259, 915, 223, ...   \n",
       "\n",
       "                                              prev_ratings  partition  \n",
       "3237871                                                 []         13  \n",
       "3237872                              [0.30437225103378296]         11  \n",
       "3237887         [0.30437225103378296, -1.6463772058486938]          9  \n",
       "3237908  [0.30437225103378296, -1.6463772058486938, 0.3...          7  \n",
       "3237980  [0.30437225103378296, -1.6463772058486938, 0.3...         18  \n",
       "...                                                    ...        ...  \n",
       "3305726  [0.30437225103378296, -1.6463772058486938, 0.3...         29  \n",
       "3305727  [-1.6463772058486938, 0.30437225103378296, 0.9...         31  \n",
       "3305728  [0.30437225103378296, 0.9546220898628235, -1.6...         29  \n",
       "3305729  [0.9546220898628235, -1.6463772058486938, -0.3...         18  \n",
       "3305730  [-1.6463772058486938, -0.3458775579929352, -0....         21  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_train_mmap = np.memmap(\"df_ratings_train.mmap\", dtype=np.object_, mode=\"w+\", shape=df_ratings_train.shape)\n",
    "df_ratings_train_mmap[:,:] = df_ratings_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_val_mmap = np.memmap(\"df_ratings_val.mmap\", dtype=np.object_, mode=\"w+\", shape=df_ratings_val.shape)\n",
    "df_ratings_val_mmap[:,:] = df_ratings_val.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_test_mmap = np.memmap(\"df_ratings_test.mmap\", dtype=np.object_, mode=\"w+\", shape=df_ratings_test.shape)\n",
    "df_ratings_test_mmap[:,:] = df_ratings_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: gsutil: command not found\n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -R parquet_dataset_ml_32m gs://r6-ae-dev-adperf-adintelligence-data/amondal/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amondal/recsys/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "import joblib\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(\"Device:\", device)\n",
    "\n",
    "def checkpoint(model:nn.Module, optimizer:torch.optim.Optimizer, filename):\n",
    "    torch.save({'optimizer':optimizer.state_dict(), 'model':model.state_dict()}, filename)\n",
    "    \n",
    "def load_model(filename):\n",
    "    chkpt = torch.load(filename, weights_only=False)\n",
    "    return chkpt['model'], chkpt['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q:torch.Tensor, k:torch.Tensor, v:torch.Tensor, mask=None):\n",
    "    d_k = q.size()[-1] # q,k,v : (batch, head, seq_len, embed_size_per_head)\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1)) # (batch, head, seq_len, seq_len)\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v) # (batch, head, seq_len, embed_size_per_head)\n",
    "    return values, attention\n",
    "\n",
    "\n",
    "def init_weights(x:nn.Linear):\n",
    "    with torch.no_grad():\n",
    "        nn.init.xavier_uniform_(x.weight)\n",
    "        x.bias.data.fill_(0)\n",
    "\t\t\n",
    "        \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model) # (seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model)) # (seq_len, d_model)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model)) # (seq_len, d_model)\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)   \n",
    "        return self.dropout(x)\n",
    "\t\n",
    "    \n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, input_dim:int, d_model: int, h: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        self.w_q = nn.Linear(input_dim, d_model) # Wq\n",
    "        self.w_k = nn.Linear(input_dim, d_model) # Wk\n",
    "        self.w_v = nn.Linear(input_dim, d_model) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model) # Wo\n",
    "\n",
    "        init_weights(self.w_q)\n",
    "        init_weights(self.w_k)\n",
    "        init_weights(self.w_v)\n",
    "        init_weights(self.w_o)\n",
    "\n",
    "    def forward(self, q_x:torch.Tensor, k_x:torch.Tensor, v_x:torch.Tensor, mask=None):\n",
    "        q:torch.Tensor = self.w_q(q_x) # (batch, seq_len, d_model)\n",
    "        k:torch.Tensor = self.w_k(k_x) # (batch, seq_len, d_model)\n",
    "        v:torch.Tensor = self.w_v(v_x) # (batch, seq_len, d_model)\n",
    "\n",
    "        q_h = q.reshape(q.shape[0], q.shape[1], self.h, self.d_k).transpose(1, 2) # (batch, head, seq_len, d_k)\n",
    "        k_h = k.reshape(k.shape[0], k.shape[1], self.h, self.d_k).transpose(1, 2) # (batch, head, seq_len, d_k)\n",
    "        v_h = v.reshape(v.shape[0], v.shape[1], self.h, self.d_k).transpose(1, 2) # (batch, head, seq_len, d_k)\n",
    "\n",
    "        attn_out, _ = attention(q_h, k_h, v_h, mask) # (batch, head, seq_len, embed_size_per_head)\n",
    "        attn_out = attn_out.transpose(1, 2) # (batch, seq_len, head, embed_size_per_head)\n",
    "        attn_out = attn_out.reshape(attn_out.shape[0], attn_out.shape[1], attn_out.shape[2]*attn_out.shape[3]) # (batch, seq_len, d_model)\n",
    "\n",
    "        return self.w_o(attn_out) # (batch, seq_len, d_model)\n",
    "    \n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttentionBlock(input_dim, input_dim, num_heads)\n",
    "\n",
    "        self.ffn_1 = nn.Linear(input_dim, dim_feedforward)\n",
    "        self.ffn_2 = nn.Linear(dim_feedforward, input_dim)\n",
    "\n",
    "        init_weights(self.ffn_1)\n",
    "        init_weights(self.ffn_2)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            self.ffn_1,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            self.ffn_2,\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.self_attn(x, x, x, mask=mask) # (batch, seq_len, input_dim)\n",
    "        x = x + self.dropout(attn_out) # (batch, seq_len, input_dim)\n",
    "        x = self.norm1(x) # (batch, seq_len, input_dim)\n",
    "\n",
    "        ffn_out = self.ffn(x) # (batch, seq_len, input_dim)\n",
    "        x = x + self.dropout(ffn_out) # (batch, seq_len, input_dim)\n",
    "        x = self.norm2(x) # (batch, seq_len, input_dim)\n",
    "\n",
    "        return x\n",
    "\t\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dim_feedforward, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(d_model, num_heads, dim_feedforward, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "        return x\n",
    "\t\n",
    "    \n",
    "class CrossFeatureLayer(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers,dropout=0.0) -> None:\n",
    "        super(CrossFeatureLayer, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.cross_layer_params = []\n",
    "        self.cross_layer_norms = []\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            h = nn.Linear(input_dim, input_dim)\n",
    "            init_weights(h)\n",
    "            self.cross_layer_params += [h]\n",
    "\n",
    "            g = nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.LayerNorm(input_dim)\n",
    "            )\n",
    "\n",
    "            self.cross_layer_norms += [g]\n",
    "\n",
    "        self.cross_layer_params = nn.ModuleList(self.cross_layer_params)\n",
    "        self.cross_layer_norms = nn.ModuleList(self.cross_layer_norms)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_initial = torch.Tensor(x) # (batch, ..., input_dim)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = x_initial*self.cross_layer_params[i](x) + x # (batch, ..., input_dim)\n",
    "            x = self.cross_layer_norms[i](x) # (batch, ..., input_dim)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieId:\n",
    "    movie_id_emb = None\n",
    "\n",
    "    def __new__(cls, movie_id_size, emb_size=512):\n",
    "        if cls.movie_id_emb is None:\n",
    "            cls.movie_id_emb = nn.Embedding(movie_id_size, emb_size, padding_idx=0)\n",
    "        return cls.movie_id_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_averaging(inp:torch.Tensor, emb_layer:nn.Module, padding_idx:int=0):\n",
    "    # inp : (batch, num_tokens)\n",
    "    embeddings = emb_layer(inp) # (batch, num_tokens, emb_size)\n",
    "    mask = (inp != padding_idx).float().unsqueeze(-1) # (batch, num_tokens, 1)\n",
    "    masked_embeddings = embeddings * mask # (batch, num_tokens, emb_size)\n",
    "    sum_embeddings = 1.0 + torch.sum(masked_embeddings, dim=1) # (batch, emb_size)\n",
    "    sequence_lengths = 1.0 + torch.sum(mask, dim=1) # (batch, 1) # prevents division by zero by + 1\n",
    "    averaged_embeddings = sum_embeddings / sequence_lengths # (batch, emb_size)\n",
    "    return averaged_embeddings\n",
    "\n",
    "class MovieEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            movie_id_size, \n",
    "            movie_desc_size,\n",
    "            movie_genres_size,\n",
    "            movie_year_size, \n",
    "            embedding_size, \n",
    "            dropout=0.0\n",
    "        ) -> None:\n",
    "        \n",
    "        super(MovieEncoder, self).__init__()\n",
    "        \n",
    "        self.movie_id_emb = MovieId(movie_id_size, 512)\n",
    "        self.movie_desc_emb = nn.Embedding(movie_desc_size, 1024, padding_idx=0)\n",
    "        self.movie_genres_emb = nn.Embedding(movie_genres_size, 8, padding_idx=0)\n",
    "        self.movie_year_emb = nn.Embedding(movie_year_size, 16, padding_idx=0)\n",
    "\n",
    "        self.fc_concat = nn.Linear(1560, embedding_size)\n",
    "        init_weights(self.fc_concat)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            self.fc_concat,\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(embedding_size)\n",
    "        )\n",
    "\n",
    "        self.cross_features = CrossFeatureLayer(1560, 3, 0.0)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            ids:torch.Tensor, \n",
    "            descriptions:torch.Tensor, \n",
    "            genres:torch.Tensor, \n",
    "            years:torch.Tensor\n",
    "        ):\n",
    "        id_emb = self.movie_id_emb(ids) # (batch, 512)\n",
    "        desc_emb = emb_averaging(descriptions, self.movie_desc_emb) # (batch, 1024)\n",
    "        genres_emb = emb_averaging(genres, self.movie_genres_emb) # (batch, 8)\n",
    "        years_emb = self.movie_year_emb(years) # (batch, 16)\n",
    "\n",
    "        movie_embedding = torch.concat([id_emb, desc_emb, genres_emb, years_emb], dim=-1) # (batch, 1560)\n",
    "        movie_embedding = self.cross_features(movie_embedding) + movie_embedding # (batch, 1560)\n",
    "        movie_embedding = self.fc(movie_embedding) # (batch, emb_size)\n",
    "\n",
    "        return movie_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            user_id_size, \n",
    "            user_prev_rated_movie_ids_size,\n",
    "            embedding_size, \n",
    "            prev_rated_seq_len, \n",
    "            num_encoder_layers, \n",
    "            num_heads=3, \n",
    "            dim_ff=512,\n",
    "            dropout=0.0\n",
    "        ) -> None:\n",
    "\n",
    "        super(UserEncoder, self).__init__()\n",
    "\n",
    "        self.user_id_emb = nn.Embedding(user_id_size, 512, padding_idx=0)\n",
    "        self.movie_id_emb = MovieId(user_prev_rated_movie_ids_size, 512)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(512, prev_rated_seq_len, 0.0)\n",
    "        self.encoder_block = Encoder(num_encoder_layers, 512, num_heads, dim_ff, 0.0)\n",
    "\n",
    "        self.fc_concat = nn.Linear(1024, embedding_size)\n",
    "        init_weights(self.fc_concat)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            self.fc_concat,\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(embedding_size)\n",
    "        )\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            user_ids:torch.Tensor, \n",
    "            prev_rated_movie_ids:torch.Tensor, \n",
    "            prev_ratings:torch.Tensor\n",
    "        ):\n",
    "        user_id_emb:torch.Tensor = self.user_id_emb(user_ids) # (batch, 512)\n",
    "\n",
    "        mask = (prev_rated_movie_ids != 0).float().unsqueeze(-1) # (batch, prev_rated_seq_len, 1)\n",
    "        mask = torch.matmul(mask, mask.transpose(-2,-1)).unsqueeze(1).repeat(1,self.num_heads,1,1) # (batch, num_heads, prev_rated_seq_len, prev_rated_seq_len)\n",
    "        \n",
    "        rated_movie_emb = self.movie_id_emb(prev_rated_movie_ids)   # (batch, prev_rated_seq_len, 512)\n",
    "        rated_movie_emb = self.positional_encoding(rated_movie_emb) # (batch, prev_rated_seq_len, 512)\n",
    "        rated_movie_emb = self.encoder_block(rated_movie_emb, mask) # (batch, prev_rated_seq_len, 512)\n",
    "\n",
    "        rated_movie_ratings = prev_ratings.unsqueeze(1) # (batch, 1, prev_rated_seq_len)\n",
    "        # weighted sum of ratings\n",
    "        rated_movie_emb_weighted = torch.matmul(rated_movie_ratings, rated_movie_emb).squeeze(1) # (batch, 512)\n",
    "\n",
    "        user_embedding = torch.concat([user_id_emb, rated_movie_emb_weighted], dim=-1) # (batch, 1024)\n",
    "        user_embedding = self.fc(user_embedding) # (batch, emb_size)\n",
    "\n",
    "        return user_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderSystem(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            user_id_size, \n",
    "            user_prev_rated_movie_ids_size,\n",
    "            user_embedding_size, \n",
    "            user_prev_rated_seq_len, \n",
    "            user_num_encoder_layers, \n",
    "            user_num_heads, \n",
    "            user_dim_ff,\n",
    "            user_dropout,\n",
    "            movie_id_size, \n",
    "            movie_desc_size,\n",
    "            movie_genres_size,\n",
    "            movie_year_size, \n",
    "            movie_embedding_size, \n",
    "            movie_dropout,\n",
    "            embedding_size,\n",
    "            dropout=0.0\n",
    "        ) -> None:\n",
    "\n",
    "        super(RecommenderSystem, self).__init__()\n",
    "\n",
    "        self.movie_encoder = \\\n",
    "            MovieEncoder\\\n",
    "            (\n",
    "                movie_id_size, \n",
    "                movie_desc_size,\n",
    "                movie_genres_size,\n",
    "                movie_year_size, \n",
    "                movie_embedding_size, \n",
    "                movie_dropout\n",
    "            )\n",
    "        \n",
    "        self.user_encoder = \\\n",
    "            UserEncoder\\\n",
    "            (\n",
    "                user_id_size, \n",
    "                user_prev_rated_movie_ids_size,\n",
    "                user_embedding_size, \n",
    "                user_prev_rated_seq_len, \n",
    "                user_num_encoder_layers, \n",
    "                user_num_heads, \n",
    "                user_dim_ff,\n",
    "                user_dropout\n",
    "            )\n",
    "\n",
    "        self.fc_concat = nn.Linear(user_embedding_size + movie_embedding_size, embedding_size)\n",
    "        init_weights(self.fc_concat)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            self.fc_concat,\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(embedding_size)\n",
    "        )\n",
    "\n",
    "        self.cross_features = CrossFeatureLayer(embedding_size, 3, 0.0)\n",
    "\n",
    "        self.fc_out = nn.Linear(embedding_size, 1)\n",
    "        init_weights(self.fc_out)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            self.fc_out,\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            user_ids:torch.Tensor, # (batch,)\n",
    "            user_prev_rated_movie_ids:torch.Tensor, \n",
    "            user_prev_ratings:torch.Tensor,\n",
    "            movie_ids:torch.Tensor, \n",
    "            movie_descriptions:torch.Tensor, \n",
    "            movie_genres:torch.Tensor, \n",
    "            movie_years:torch.Tensor\n",
    "        ):\n",
    "        \n",
    "        movie_embeddings = \\\n",
    "            self.movie_encoder\\\n",
    "            (\n",
    "                movie_ids, \n",
    "                movie_descriptions, \n",
    "                movie_genres, \n",
    "                movie_years\n",
    "            ) # (batch, 1, embedding_size)\n",
    "        \n",
    "        user_embeddings = \\\n",
    "            self.user_encoder\\\n",
    "                (\n",
    "                    user_ids, \n",
    "                    user_prev_rated_movie_ids, \n",
    "                    user_prev_ratings,\n",
    "                )                     # (batch, 1, embedding_size), (batch, movie_seq_len, embedding_size)\n",
    "        \n",
    "        emb_concat = torch.concat([movie_embeddings, user_embeddings], dim=-1)\n",
    "        \n",
    "        emb  = self.fc_concat(emb_concat)\n",
    "        emb  = self.cross_features(emb)\n",
    "        out  = self.out(emb)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ids = df_movies[\"movieId\"].to_numpy(dtype=np.uint32)\n",
    "movie_descriptions = df_movies[\"description\"].to_numpy(dtype=np.uint32)\n",
    "movie_genres = df_movies[\"genres\"].to_numpy(dtype=np.uint8)\n",
    "movie_years = df_movies[\"movie_year\"].to_numpy(dtype=np.uint16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df_ratings_train[\"prev_movie_ids\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(values, dtype, max_seq_len=None):\n",
    "    if max_seq_len is None:\n",
    "        max_seq_len = max([len(x) for x in values])\n",
    "    \n",
    "    arr = np.zeros((len(values), max_seq_len), dtype=dtype)\n",
    "\n",
    "    for i in range(len(values)):\n",
    "        k = max_seq_len-1\n",
    "        for j in range(len(values[i])-1, -1, -1):\n",
    "            arr[i,k] = values[i][j]\n",
    "            k -= 1\n",
    "    \n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches(n, batch_size=128):\n",
    "    max_seq_len = 20\n",
    "    columns = df_ratings_train.columns\n",
    "    df_ratings_train_mmap = np.memmap(\"df_ratings_train.mmap\", dtype=np.object_, mode=\"r\", shape=df_ratings_train.shape)\n",
    "\n",
    "    while True:\n",
    "        for i in range(0, n, batch_size):\n",
    "            df_ratings_batch = df_ratings_train_mmap[i:min(n,i+batch_size)]\n",
    "\n",
    "            df_ratings_batch_df = pd.DataFrame(df_ratings_batch, columns=columns)\n",
    "            df_ratings_batch_df = df_ratings_batch_df.merge(df_movies, on=[\"movieId\"], how=\"left\")\n",
    "\n",
    "            user_ids = df_ratings_batch_df[\"userId\"].to_numpy(dtype=np.uint32)\n",
    "            user_prev_rated_movie_ids = pad_batch(df_ratings_batch_df[\"prev_movie_ids\"].to_numpy(), dtype=np.uint32, max_seq_len=max_seq_len)\n",
    "            user_prev_ratings = pad_batch(df_ratings_batch_df[\"prev_ratings\"].to_numpy(), dtype=np.float32, max_seq_len=max_seq_len)\n",
    "\n",
    "            movie_ids = df_ratings_batch_df[\"movieId\"].to_numpy(dtype=np.uint32)\n",
    "            movie_descriptions = pad_batch(df_ratings_batch_df[\"description\"].to_numpy(), dtype=np.uint32)\n",
    "            movie_genres = pad_batch(df_ratings_batch_df[\"genres\"].to_numpy(), dtype=np.uint8)\n",
    "            movie_years = df_ratings_batch_df[\"movie_year\"].to_numpy(dtype=np.uint16)\n",
    "\n",
    "            user_ids = torch.from_numpy(user_ids)\n",
    "            user_prev_rated_movie_ids = torch.from_numpy(user_prev_rated_movie_ids)\n",
    "            user_prev_ratings = torch.from_numpy(user_prev_ratings)\n",
    "\n",
    "            movie_ids = torch.from_numpy(movie_ids)\n",
    "            movie_descriptions = torch.from_numpy(movie_descriptions)\n",
    "            movie_genres = torch.from_numpy(movie_genres)\n",
    "            movie_years = torch.from_numpy(movie_years)\n",
    "\n",
    "            labels = torch.from_numpy(df_ratings_batch_df[\"label\"].to_numpy(dtype=np.uint32))\n",
    "\n",
    "            yield [user_ids, user_prev_rated_movie_ids, user_prev_ratings, movie_ids, movie_descriptions, movie_genres, movie_years], labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     0,     0,  ..., 52087, 52378, 16288],\n",
      "        [    0,     0,     0,  ..., 64905,  4044, 58330],\n",
      "        [    0,     0,     0,  ..., 17668, 27702, 44365],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ..., 37732, 13745, 34444],\n",
      "        [    0,     0,     0,  ..., 39769,  4037, 70090],\n",
      "        [    0,     0,     0,  ..., 54393, 51458,  6928]], dtype=torch.uint32)\n",
      "torch.Size([128, 634])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprepare_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_ratings_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mprepare_batches\u001b[39m\u001b[34m(n, batch_size)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(movie_descriptions)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(movie_descriptions.shape)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[32m1\u001b[39m == \u001b[32m0\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "prepare_batches(df_ratings_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
