{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amondal/recsys/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import math\n",
    "import os\n",
    "import urllib.request\n",
    "from functools import partial\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "plt.set_cmap(\"cividis\")\n",
    "%matplotlib inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_orig()\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model) # (seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model)) # (seq_len, d_model)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model)) # (seq_len, d_model)\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)   \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q:torch.Tensor, k:torch.Tensor, v:torch.Tensor, mask=None):\n",
    "    d_k = q.size()[-1] # q,k,v : (batch, head, seq_len, embed_size_per_head)\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1)) # (batch, head, seq_len, seq_len)\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v) # (batch, head, seq_len, embed_size_per_head)\n",
    "    return values, attention\n",
    "\n",
    "def init_weights(x:nn.Linear):\n",
    "    with torch.no_grad():\n",
    "        nn.init.xavier_uniform_(x.weight)\n",
    "        x.bias.data.fill_(0)\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, input_dim:int, d_model: int, h: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        self.w_q = nn.Linear(input_dim, d_model) # Wq\n",
    "        self.w_k = nn.Linear(input_dim, d_model) # Wk\n",
    "        self.w_v = nn.Linear(input_dim, d_model) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model) # Wo\n",
    "\n",
    "        init_weights(self.w_q)\n",
    "        init_weights(self.w_k)\n",
    "        init_weights(self.w_v)\n",
    "        init_weights(self.w_o)\n",
    "\n",
    "    def forward(self, q_x:torch.Tensor, k_x:torch.Tensor, v_x:torch.Tensor, mask=None):\n",
    "        q:torch.Tensor = self.w_q(q_x) # (batch, seq_len, d_model)\n",
    "        k:torch.Tensor = self.w_k(k_x) # (batch, seq_len, d_model)\n",
    "        v:torch.Tensor = self.w_v(v_x) # (batch, seq_len, d_model)\n",
    "\n",
    "        q_h = q.reshape(q.shape[0], q.shape[1], self.h, self.d_k).transpose(1, 2) # (batch, head, seq_len, d_k)\n",
    "        k_h = k.reshape(k.shape[0], k.shape[1], self.h, self.d_k).transpose(1, 2) # (batch, head, seq_len, d_k)\n",
    "        v_h = v.reshape(v.shape[0], v.shape[1], self.h, self.d_k).transpose(1, 2) # (batch, head, seq_len, d_k)\n",
    "\n",
    "        attn_out, _ = attention(q_h, k_h, v_h, mask) # (batch, head, seq_len, embed_size_per_head)\n",
    "        attn_out = attn_out.transpose(1, 2) # (batch, seq_len, head, embed_size_per_head)\n",
    "        attn_out = attn_out.reshape(attn_out.shape[0], attn_out.shape[1], attn_out.shape[2]*attn_out.shape[3]) # (batch, seq_len, d_model)\n",
    "\n",
    "        return self.w_o(attn_out) # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttentionBlock(input_dim, input_dim, num_heads)\n",
    "\n",
    "        self.ffn_1 = nn.Linear(input_dim, dim_feedforward)\n",
    "        self.ffn_2 = nn.Linear(dim_feedforward, input_dim)\n",
    "\n",
    "        init_weights(self.ffn_1)\n",
    "        init_weights(self.ffn_2)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            self.ffn_1,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            self.ffn_2,\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.self_attn(x, x, x, mask=mask) # (batch, seq_len, input_dim)\n",
    "        x = x + self.dropout(attn_out) # (batch, seq_len, input_dim)\n",
    "        x = self.norm1(x) # (batch, seq_len, input_dim)\n",
    "\n",
    "        ffn_out = self.ffn(x) # (batch, seq_len, input_dim)\n",
    "        x = x + self.dropout(ffn_out) # (batch, seq_len, input_dim)\n",
    "        x = self.norm2(x) # (batch, seq_len, input_dim)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dim_feedforward, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(d_model, num_heads, dim_feedforward, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0)->None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttentionBlock(input_dim, input_dim, num_heads)\n",
    "        self.crss_attn = MultiHeadAttentionBlock(input_dim, input_dim, num_heads)\n",
    "\n",
    "        self.ffn_1 = nn.Linear(input_dim, dim_feedforward)\n",
    "        self.ffn_2 = nn.Linear(dim_feedforward, input_dim)\n",
    "\n",
    "        init_weights(self.ffn_1)\n",
    "        init_weights(self.ffn_2)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            self.ffn_1,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            self.ffn_2,\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.norm3 = nn.LayerNorm(input_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, pred_mask, pad_mask):\n",
    "        self_attn_out = self.self_attn(x, x, x, mask=pred_mask) # (batch, seq_len, input_dim)\n",
    "        x = x + self.dropout(self_attn_out) # (batch, seq_len, input_dim)\n",
    "        x = self.norm1(x) # (batch, seq_len, input_dim)\n",
    "\n",
    "        crss_attn_out = self.crss_attn(x, encoder_output, encoder_output, mask=pad_mask) # (batch, seq_len, input_dim)\n",
    "        x = x + self.dropout(crss_attn_out) # (batch, seq_len, input_dim)\n",
    "        x = self.norm2(x) # (batch, seq_len, input_dim)\n",
    "\n",
    "        ffn_out = self.ffn(x) # (batch, seq_len, input_dim)\n",
    "        x = x + self.dropout(ffn_out) # (batch, seq_len, input_dim)\n",
    "        x = self.norm3(x) # (batch, seq_len, input_dim)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderBlock(d_model, num_heads, dim_feedforward, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, encoder_output, pred_mask=None, pad_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, pred_mask, pad_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            movie_vocab_size, \n",
    "            genres_vocab_size, \n",
    "            years_vocab_size, \n",
    "            embedding_size, \n",
    "            dropout=0.0\n",
    "        ) -> None:\n",
    "        \n",
    "        super(MovieEncoder, self).__init__()\n",
    "        \n",
    "        self.movie_embedding_layer = nn.Embedding(movie_vocab_size, embedding_size)\n",
    "        self.years_embedding_layer = nn.Embedding(years_vocab_size, 32)\n",
    "\n",
    "        self.genres_encoder_layer = nn.Linear(genres_vocab_size, 4)\n",
    "        init_weights(self.genres_encoder_layer)\n",
    "\n",
    "        self.fc_concat = nn.Linear(embedding_size + 36, embedding_size)\n",
    "        init_weights(self.fc_concat)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            self.fc_concat,\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(embedding_size)\n",
    "\n",
    "    def forward(self, movies, genres, years):\n",
    "        movie_embedding = self.movie_embedding_layer(movies) # (batch, ..., embedding_size)\n",
    "        genres_embedding = self.genres_encoder_layer(genres) # (batch, ..., 4)\n",
    "        years_embedding = self.years_embedding_layer(years) # (batch, ..., 32)\n",
    "\n",
    "        movie_embedding = torch.concat([movie_embedding, genres_embedding, years_embedding], dim=-1) # (batch, ..., embedding_size + 36)\n",
    "        movie_embedding = self.fc(movie_embedding) # (batch, ..., embedding_size)\n",
    "        movie_embedding = self.dropout(movie_embedding) # (batch, ..., embedding_size)\n",
    "        movie_embedding = self.norm(movie_embedding) # (batch, ..., embedding_size)\n",
    "\n",
    "        return movie_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            user_vocab_size, \n",
    "            movie_vocab_size, \n",
    "            genres_vocab_size, \n",
    "            years_vocab_size, \n",
    "            embedding_size, \n",
    "            movie_seq_len, \n",
    "            num_encoder_layers, \n",
    "            num_heads, \n",
    "            dim_ff,\n",
    "            dropout=0.0\n",
    "        ) -> None:\n",
    "\n",
    "        super(UserEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.movie_seq_len = movie_seq_len\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_ff = dim_ff\n",
    "\n",
    "        self.user_embedding_layer = nn.Embedding(user_vocab_size, embedding_size)\n",
    "        self.movie_encoder = MovieEncoder(movie_vocab_size, genres_vocab_size, years_vocab_size, embedding_size, dropout)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(embedding_size, movie_seq_len, dropout)\n",
    "        self.encoder_block = Encoder(num_encoder_layers, embedding_size, num_heads, dim_ff, dropout)\n",
    "\n",
    "        self.fc_concat = nn.Linear(2*embedding_size, embedding_size)\n",
    "        init_weights(self.fc_concat)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            self.fc_concat,\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(embedding_size)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            user_ids, \n",
    "            rated_movie_ids, \n",
    "            rated_movie_genres, \n",
    "            rated_movie_years, \n",
    "            rated_movie_ratings):\n",
    "        \n",
    "        user_embedding = self.user_embedding_layer(user_ids) # (batch, 1, embedding_size)\n",
    "        \n",
    "        movie_embeddings = self.movie_encoder(rated_movie_ids, rated_movie_genres, rated_movie_years) # (batch, movie_seq_len, embedding_size)\n",
    "        movie_embeddings = self.positional_encoding(movie_embeddings) # (batch, movie_seq_len, embedding_size)\n",
    "        movie_embeddings = self.encoder_block(movie_embeddings, None) # (batch, movie_seq_len, embedding_size)\n",
    "\n",
    "        rated_movie_ratings = F.softmax(rated_movie_ratings, dim=-1) # (batch, movie_seq_len)\n",
    "        rated_movie_ratings = rated_movie_ratings.unsqueeze(1) # (batch, 1, movie_seq_len)\n",
    "        movie_embeddings_weighted = torch.matmul(rated_movie_ratings, movie_embeddings) # (batch, 1, embedding_size)\n",
    "\n",
    "        user_embedding = torch.concat([user_embedding, movie_embeddings_weighted], dim=-1) # (batch, 1, 2*embedding_size)\n",
    "        user_embedding = self.fc(user_embedding) # (batch, 1, embedding_size)\n",
    "        user_embedding = self.dropout(user_embedding) # (batch, 1, embedding_size)\n",
    "        user_embedding = self.norm(user_embedding) # (batch, 1, embedding_size)\n",
    "\n",
    "        return user_embedding, movie_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderSystem(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            user_vocab_size, \n",
    "            movie_vocab_size, \n",
    "            genres_vocab_size, \n",
    "            years_vocab_size, \n",
    "            embedding_size, \n",
    "            movie_seq_len, \n",
    "            num_encoder_layers, \n",
    "            num_heads, \n",
    "            dim_ff,\n",
    "            dropout=0.0\n",
    "        ) -> None:\n",
    "\n",
    "        super(RecommenderSystem, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.movie_seq_len = movie_seq_len\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_ff = dim_ff\n",
    "\n",
    "        self.movie_encoder = \\\n",
    "            MovieEncoder\\\n",
    "            (\n",
    "                movie_vocab_size, \n",
    "                genres_vocab_size, \n",
    "                years_vocab_size, \n",
    "                embedding_size, \n",
    "                dropout\n",
    "            )\n",
    "        \n",
    "        self.user_encoder = \\\n",
    "            UserEncoder\\\n",
    "            (\n",
    "                user_vocab_size, \n",
    "                movie_vocab_size, \n",
    "                genres_vocab_size, \n",
    "                years_vocab_size, \n",
    "                embedding_size, \n",
    "                movie_seq_len, \n",
    "                num_encoder_layers, \n",
    "                num_heads, \n",
    "                dim_ff,\n",
    "                dropout\n",
    "            )\n",
    "        \n",
    "        self.cross_attn = MultiHeadAttentionBlock(embedding_size, embedding_size, num_heads)\n",
    "\n",
    "        self.fc_concat1 = nn.Linear(2*embedding_size, embedding_size)\n",
    "        init_weights(self.fc_concat1)\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            self.fc_concat1,\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.fc_concat2 = nn.Linear(2*embedding_size, embedding_size)\n",
    "        init_weights(self.fc_concat2)\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            self.fc_concat2,\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.fc_ratings_linear = nn.Linear(embedding_size, 1)\n",
    "        init_weights(self.fc_ratings_linear)\n",
    "\n",
    "        self.fc_ratings = nn.Sequential(\n",
    "            self.fc_ratings_linear,\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embedding_size)\n",
    "        self.norm2 = nn.LayerNorm(embedding_size)\n",
    "        self.norm3 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            user_ids, \n",
    "            movie_ids, \n",
    "            rated_movie_ids, \n",
    "            rated_movie_genres, \n",
    "            rated_movie_years, \n",
    "            rated_movie_ratings, \n",
    "            movie_genres, \n",
    "            movie_years):\n",
    "        \n",
    "        movie_embeddings = \\\n",
    "            self.movie_encoder(movie_ids, movie_genres, movie_years) # (batch, 1, embedding_size)\n",
    "        \n",
    "        user_embeddings, rated_movie_embeddings = \\\n",
    "            self.user_encoder\\\n",
    "                (\n",
    "                    user_ids, \n",
    "                    rated_movie_ids, \n",
    "                    rated_movie_genres, \n",
    "                    rated_movie_years, \n",
    "                    rated_movie_ratings\n",
    "                )                     # (batch, 1, embedding_size), (batch, movie_seq_len, embedding_size)\n",
    "        \n",
    "        crss_attn_out = self.cross_attn(movie_embeddings, rated_movie_embeddings, rated_movie_embeddings, mask=None) # (batch, 1, embedding_size)\n",
    "        rated_movie_embeddings = movie_embeddings + self.dropout(crss_attn_out) # (batch, 1, embedding_size)\n",
    "        rated_movie_embeddings = self.norm1(rated_movie_embeddings) # (batch, 1, embedding_size)\n",
    "\n",
    "        movie_embeddings = torch.concat([movie_embeddings, rated_movie_embeddings], dim=-1) # (batch, 1, 2*embedding_size)\n",
    "        movie_embeddings = self.fc1(movie_embeddings) # (batch, 1, embedding_size)\n",
    "        movie_embeddings = self.dropout(movie_embeddings) # (batch, 1, embedding_size)\n",
    "        movie_embeddings = self.norm2(movie_embeddings) # (batch, 1, embedding_size)\n",
    "        \n",
    "        encoded = torch.concat([user_embeddings, movie_embeddings], dim=-1) # (batch, 1, 2*embedding_size)\n",
    "        encoded = self.fc2(encoded) # (batch, 1, embedding_size)\n",
    "        encoded = self.dropout(encoded) # (batch, 1, embedding_size)\n",
    "        encoded = self.norm3(encoded) # (batch, 1, embedding_size)\n",
    "\n",
    "        output = self.fc_ratings(encoded) # (batch, 1, 1)\n",
    "        output = torch.clamp(output, min=0.0, max=5.0).squeeze(1) # (batch, 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred:torch.Tensor, y_true:torch.Tensor):\n",
    "        y_pred = F.softmax(y_pred, dim=-1)\n",
    "        y_pred_mask = (y_pred > 0)\n",
    "        y_pred = y_pred.where(y_pred_mask, 1.0e-15)\n",
    "\n",
    "        y_true = F.one_hot(y_true, num_classes=y_pred.shape[2])\n",
    "\n",
    "        # ratings = F.softmax(ratings, dim=-1)\n",
    "        # return ((-y_true*torch.log(y_pred)).sum(dim=-1)*ratings).sum()/y_pred.shape[0]\n",
    "        return (-y_true*torch.log(y_pred)).sum()/(y_pred.shape[0]*y_pred.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred:torch.Tensor, y_true:torch.Tensor, ratings:torch.Tensor):\n",
    "        y_pred = F.softmax(y_pred, dim=-1)\n",
    "        y_pred_mask = (y_pred > 0)\n",
    "        y_pred = y_pred.where(y_pred_mask, 1.0e-15)\n",
    "\n",
    "        y_true = F.one_hot(y_true, num_classes=y_pred.shape[2])\n",
    "\n",
    "        ratings = F.softmax(ratings, dim=-1)\n",
    "        return ((-y_true*torch.log(y_pred)).sum(dim=-1)*ratings).sum()/y_pred.shape[0]\n",
    "        # return (-y_true*torch.log(y_pred)).sum()/(y_pred.shape[0]*y_pred.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings_path = '/Users/amondal/recsys/datasets/ml-32m/ratings.csv'\n",
    "genres_path = '/Users/amondal/recsys/datasets/ml-32m/movies.csv'\n",
    "\n",
    "rating_column_names = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "genres_column_names = ['movieId', 'title', 'genres']\n",
    "\n",
    "df_rating = pd.read_csv(ratings_path, sep=',', names=rating_column_names, dtype={'userId':'int32', 'movieId':'int32', 'rating':float, 'timestamp':'int64'}, header=0)\n",
    "df_genres = pd.read_csv(genres_path, sep=',', names=genres_column_names, dtype={'movieId':'int32', 'title':'object', 'genres':'object'}, header=0)\n",
    "\n",
    "df_rating.dropna(inplace=True, subset=['userId', 'movieId', 'rating'])\n",
    "df_genres.dropna(inplace=True, subset=['movieId', 'title', 'genres'])\n",
    "\n",
    "df_genres['genres'] = df_genres['genres'].apply(lambda x: x.split('|'))\n",
    "df_genres['movie_year'] = df_genres['title'].str.extract(r'\\((\\d{4})\\)').fillna(\"2025\").astype('int')\n",
    "df_genres.drop(columns=['title'], inplace=True)\n",
    "\n",
    "df = df_rating.merge(df_genres, on=['movieId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = df['genres'].tolist()\n",
    "\n",
    "genres_set = set()\n",
    "for x in all_genres:\n",
    "    genres_set.update(set(x))\n",
    "\n",
    "genres_set = list(genres_set)\n",
    "inv_idx = {genres_set[i]:i for i in range(len(genres_set))}\n",
    "\n",
    "genres_mh = []\n",
    "for x in all_genres:\n",
    "    h = [0]*len(genres_set)\n",
    "    for y in x:\n",
    "        h[inv_idx[y]] = 1\n",
    "    genres_mh += [h]\n",
    "\n",
    "df['genres_mh'] = genres_mh\n",
    "df.drop(columns=['genres'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='timestamp')\n",
    "df2 = df[[\"userId\", \"movieId\"]].groupby(by=[\"userId\"]).agg(list).reset_index()\n",
    "df2 = df2[df2.movieId.apply(len) > 10]\n",
    "df = df.merge(df2, on=[\"userId\"], how=\"inner\", suffixes=(\"\", \"_right\"))\n",
    "df.drop(columns=['movieId_right'], inplace=True)\n",
    "\n",
    "n = df.shape[0]\n",
    "m = int(0.8*n)\n",
    "\n",
    "df_train = df[:m]\n",
    "df_test = df[m:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "prev_seq_len = 10\n",
    "\n",
    "def get_movies_data(df:pd.DataFrame):\n",
    "    df2 = df.groupby(by=[\"userId\"]).agg(list).reset_index()\n",
    "\n",
    "    user_ids, movie_ids, genres, years, ratings = [], [], [], [], []\n",
    "    prev_movie_ids = []\n",
    "    prev_movie_genres = []\n",
    "    prev_movie_years = []\n",
    "    prev_movie_ratings = []\n",
    "\n",
    "    for i in range(df2.shape[0]):\n",
    "        movie_ids_seq = df2.loc[i, 'movieId']\n",
    "        user_id = df2.loc[i, 'userId']\n",
    "        genres_seq = df2.loc[i, 'genres_mh']\n",
    "        ratings_seq = df2.loc[i, 'rating']\n",
    "        years_seq = df2.loc[i, 'movie_year']\n",
    "\n",
    "        m = len(movie_ids_seq)-prev_seq_len\n",
    "        if m > 0:\n",
    "            indices = random.sample(range(prev_seq_len, len(movie_ids_seq)), k=min(m, 20))\n",
    "\n",
    "            for j in indices:\n",
    "                rated_movie_ids = movie_ids_seq[max(0, j-prev_seq_len):j]\n",
    "                rated_movie_genres = genres_seq[max(0, j-prev_seq_len):j]\n",
    "                rated_movie_years = years_seq[max(0, j-prev_seq_len):j]\n",
    "                rated_movie_ratings = ratings_seq[max(0, j-prev_seq_len):j]\n",
    "\n",
    "                user_ids += [user_id]\n",
    "                movie_ids += [movie_ids_seq[j]]\n",
    "                genres += [genres_seq[j]]\n",
    "                years += [years_seq[j]]\n",
    "                ratings += [ratings_seq[j]]\n",
    "\n",
    "                prev_movie_ids += [rated_movie_ids]\n",
    "                prev_movie_genres += [rated_movie_genres]\n",
    "                prev_movie_years += [rated_movie_years]\n",
    "                prev_movie_ratings += [rated_movie_ratings]\n",
    "    \n",
    "    user_ids = torch.tensor(user_ids, dtype=torch.int32)\n",
    "    movie_ids = torch.tensor(movie_ids, dtype=torch.int32)\n",
    "    genres = torch.tensor(genres, dtype=torch.int8)\n",
    "    years = torch.tensor(years, dtype=torch.int32)\n",
    "    ratings = torch.tensor(ratings, dtype=torch.float32)\n",
    "\n",
    "    prev_movie_ids = torch.tensor(prev_movie_ids, dtype=torch.int32)\n",
    "    prev_movie_genres = torch.tensor(prev_movie_genres, dtype=torch.int8)\n",
    "    prev_movie_years = torch.tensor(prev_movie_years, dtype=torch.int32)\n",
    "    prev_movie_ratings = torch.tensor(prev_movie_ratings, dtype=torch.float32)\n",
    "\n",
    "    return user_ids, movie_ids, genres, years, ratings, prev_movie_ids, prev_movie_genres, prev_movie_years, prev_movie_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_vocab_size = int(df[\"userId\"].max()+1)\n",
    "movie_id_vocab_size = int(df[\"movieId\"].max()+1)\n",
    "genres_vocab_size = len(genres_set)\n",
    "years_vocab_size = int(df[\"movie_year\"].max()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_train, movie_ids_train, genres_train, years_train, ratings_train, prev_movie_ids_train, prev_movie_genres_train, prev_movie_years_train, prev_movie_ratings_train = get_movies_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_test, movie_ids_test, genres_test, years_test, ratings_test, prev_movie_ids_test, prev_movie_genres_test, prev_movie_years_test, prev_movie_ratings_test = get_movies_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "movie_seq_len = 10\n",
    "num_encoder_layers = 4\n",
    "num_heads = 4\n",
    "dropout = 0.0\n",
    "dff = 32\n",
    "\n",
    "rec = RecommenderSystem(user_id_vocab_size, movie_id_vocab_size, genres_vocab_size, years_vocab_size, embedding_size, movie_seq_len, num_encoder_layers, num_heads, dff, dropout).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1, Loss: 13.138029098510742\n",
      "Epoch: 1, Batch: 2, Loss: 14.040908813476562\n",
      "Epoch: 1, Batch: 3, Loss: 13.302042961120605\n",
      "Epoch: 1, Batch: 4, Loss: 11.476131439208984\n",
      "Epoch: 1, Batch: 5, Loss: 8.419654846191406\n",
      "Epoch: 1, Batch: 6, Loss: 3.9147191047668457\n",
      "Epoch: 1, Batch: 7, Loss: 1.9049830436706543\n",
      "Epoch: 1, Batch: 8, Loss: 2.2799997329711914\n",
      "Epoch: 1, Batch: 9, Loss: 2.5360569953918457\n",
      "Epoch: 1, Batch: 10, Loss: 2.957446575164795\n",
      "Epoch: 1, Batch: 11, Loss: 2.890961170196533\n",
      "Epoch: 1, Batch: 12, Loss: 2.661355972290039\n",
      "Epoch: 1, Batch: 13, Loss: 2.7205376625061035\n",
      "Epoch: 1, Batch: 14, Loss: 3.091193199157715\n",
      "Epoch: 1, Batch: 15, Loss: 2.9084126949310303\n",
      "Epoch: 1, Batch: 16, Loss: 2.440443992614746\n",
      "Epoch: 1, Batch: 17, Loss: 2.2274422645568848\n",
      "Epoch: 1, Batch: 18, Loss: 2.2450220584869385\n",
      "Epoch: 1, Batch: 19, Loss: 2.8363850116729736\n",
      "Epoch: 1, Batch: 20, Loss: 2.0096678733825684\n",
      "Epoch: 1, Batch: 21, Loss: 2.154967784881592\n",
      "Epoch: 1, Batch: 22, Loss: 2.5201210975646973\n",
      "Epoch: 1, Batch: 23, Loss: 2.690408945083618\n",
      "Epoch: 1, Batch: 24, Loss: 2.5871191024780273\n",
      "Epoch: 1, Batch: 25, Loss: 2.6997992992401123\n",
      "Epoch: 1, Batch: 26, Loss: 2.410305976867676\n",
      "Epoch: 1, Batch: 27, Loss: 1.71559739112854\n",
      "Epoch: 1, Batch: 28, Loss: 1.8859277963638306\n",
      "Epoch: 1, Batch: 29, Loss: 2.056586742401123\n",
      "Epoch: 1, Batch: 30, Loss: 1.461132287979126\n",
      "Epoch: 1, Batch: 31, Loss: 1.6543550491333008\n",
      "Epoch: 1, Batch: 32, Loss: 1.6240074634552002\n",
      "Epoch: 1, Batch: 33, Loss: 1.867571234703064\n",
      "Epoch: 1, Batch: 34, Loss: 1.3865984678268433\n",
      "Epoch: 1, Batch: 35, Loss: 1.6268435716629028\n",
      "Epoch: 1, Batch: 36, Loss: 1.6050596237182617\n",
      "Epoch: 1, Batch: 37, Loss: 1.377988338470459\n",
      "Epoch: 1, Batch: 38, Loss: 1.216625690460205\n",
      "Epoch: 1, Batch: 39, Loss: 1.2205655574798584\n",
      "Epoch: 1, Batch: 40, Loss: 1.6666271686553955\n",
      "Epoch: 1, Batch: 41, Loss: 1.2623628377914429\n",
      "Epoch: 1, Batch: 42, Loss: 1.2091141939163208\n",
      "Epoch: 1, Batch: 43, Loss: 1.1782910823822021\n",
      "Epoch: 1, Batch: 44, Loss: 1.2485431432724\n",
      "Epoch: 1, Batch: 45, Loss: 1.1205805540084839\n",
      "Epoch: 1, Batch: 46, Loss: 1.1015782356262207\n",
      "Epoch: 1, Batch: 47, Loss: 1.1048927307128906\n",
      "Epoch: 1, Batch: 48, Loss: 1.1238839626312256\n",
      "Epoch: 1, Batch: 49, Loss: 1.1148003339767456\n",
      "Epoch: 1, Batch: 50, Loss: 1.2280166149139404\n",
      "Epoch: 1, Batch: 51, Loss: 1.2401098012924194\n",
      "Epoch: 1, Batch: 52, Loss: 1.198717474937439\n",
      "Epoch: 1, Batch: 53, Loss: 1.1358587741851807\n",
      "Epoch: 1, Batch: 54, Loss: 1.2634000778198242\n",
      "Epoch: 1, Batch: 55, Loss: 1.1310889720916748\n",
      "Epoch: 1, Batch: 56, Loss: 1.0969023704528809\n",
      "Epoch: 1, Batch: 57, Loss: 1.0777268409729004\n",
      "Epoch: 1, Batch: 58, Loss: 1.1126973628997803\n",
      "Epoch: 1, Batch: 59, Loss: 1.0345518589019775\n",
      "Epoch: 1, Batch: 60, Loss: 1.0752172470092773\n",
      "Epoch: 1, Batch: 61, Loss: 1.2589375972747803\n",
      "Epoch: 1, Batch: 62, Loss: 1.1490678787231445\n",
      "Epoch: 1, Batch: 63, Loss: 1.1511023044586182\n",
      "Epoch: 1, Batch: 64, Loss: 1.1759693622589111\n",
      "Epoch: 1, Batch: 65, Loss: 1.271836280822754\n",
      "Epoch: 1, Batch: 66, Loss: 0.9694219827651978\n",
      "Epoch: 1, Batch: 67, Loss: 1.2799699306488037\n",
      "Epoch: 1, Batch: 68, Loss: 0.9238229990005493\n",
      "Epoch: 1, Batch: 69, Loss: 1.0490639209747314\n",
      "Epoch: 1, Batch: 70, Loss: 1.0256142616271973\n",
      "Epoch: 1, Batch: 71, Loss: 1.0822590589523315\n",
      "Epoch: 1, Batch: 72, Loss: 1.181968331336975\n",
      "Epoch: 1, Batch: 73, Loss: 0.9767530560493469\n",
      "Epoch: 1, Batch: 74, Loss: 1.0455117225646973\n",
      "Epoch: 1, Batch: 75, Loss: 1.0889972448349\n",
      "Epoch: 1, Batch: 76, Loss: 1.0080389976501465\n",
      "Epoch: 1, Batch: 77, Loss: 1.060031771659851\n",
      "Epoch: 1, Batch: 78, Loss: 0.9649679064750671\n",
      "Epoch: 1, Batch: 79, Loss: 1.1476938724517822\n",
      "Epoch: 1, Batch: 80, Loss: 1.1567182540893555\n",
      "Epoch: 1, Batch: 81, Loss: 1.0435314178466797\n",
      "Epoch: 1, Batch: 82, Loss: 1.1831568479537964\n",
      "Epoch: 1, Batch: 83, Loss: 1.029160976409912\n",
      "Epoch: 1, Batch: 84, Loss: 1.094801425933838\n",
      "Epoch: 1, Batch: 85, Loss: 1.1378268003463745\n",
      "Epoch: 1, Batch: 86, Loss: 0.9040535092353821\n",
      "Epoch: 1, Batch: 87, Loss: 1.0130292177200317\n",
      "Epoch: 1, Batch: 88, Loss: 1.080673098564148\n",
      "Epoch: 1, Batch: 89, Loss: 1.2049633264541626\n",
      "Epoch: 1, Batch: 90, Loss: 1.0513651371002197\n",
      "Epoch: 1, Batch: 91, Loss: 0.9963611364364624\n",
      "Epoch: 1, Batch: 92, Loss: 1.1737492084503174\n",
      "Epoch: 1, Batch: 93, Loss: 1.0694565773010254\n",
      "Epoch: 1, Batch: 94, Loss: 1.080554723739624\n",
      "Epoch: 1, Batch: 95, Loss: 1.0363140106201172\n",
      "Epoch: 1, Batch: 96, Loss: 0.9943283796310425\n",
      "Epoch: 1, Batch: 97, Loss: 1.0522899627685547\n",
      "Epoch: 1, Batch: 98, Loss: 0.9903120994567871\n",
      "Epoch: 1, Batch: 99, Loss: 1.1953606605529785\n",
      "Epoch: 1, Batch: 100, Loss: 1.1915855407714844\n",
      "Epoch: 1, Batch: 101, Loss: 1.0905414819717407\n",
      "Epoch: 1, Batch: 102, Loss: 1.0440475940704346\n",
      "Epoch: 1, Batch: 103, Loss: 0.9893344640731812\n",
      "Epoch: 1, Batch: 104, Loss: 1.0458108186721802\n",
      "Epoch: 1, Batch: 105, Loss: 1.057570457458496\n",
      "Epoch: 1, Batch: 106, Loss: 1.0285372734069824\n",
      "Epoch: 1, Batch: 107, Loss: 1.044222116470337\n",
      "Epoch: 1, Batch: 108, Loss: 0.9969254732131958\n",
      "Epoch: 1, Batch: 109, Loss: 1.0042637586593628\n",
      "Epoch: 1, Batch: 110, Loss: 1.0702441930770874\n",
      "Epoch: 1, Batch: 111, Loss: 0.9981665015220642\n",
      "Epoch: 1, Batch: 112, Loss: 0.9343224763870239\n",
      "Epoch: 1, Batch: 113, Loss: 1.149505376815796\n",
      "Epoch: 1, Batch: 114, Loss: 1.0030391216278076\n",
      "Epoch: 1, Batch: 115, Loss: 1.099696397781372\n",
      "Epoch: 1, Batch: 116, Loss: 1.1061670780181885\n",
      "Epoch: 1, Batch: 117, Loss: 1.0169479846954346\n",
      "Epoch: 1, Batch: 118, Loss: 1.094167709350586\n",
      "Epoch: 1, Batch: 119, Loss: 1.0874333381652832\n",
      "Epoch: 1, Batch: 120, Loss: 1.125704050064087\n",
      "Epoch: 1, Batch: 121, Loss: 1.1879098415374756\n",
      "Epoch: 1, Batch: 122, Loss: 1.1572370529174805\n",
      "Epoch: 1, Batch: 123, Loss: 0.9904295206069946\n",
      "Epoch: 1, Batch: 124, Loss: 0.9375572204589844\n",
      "Epoch: 1, Batch: 125, Loss: 1.1457654237747192\n",
      "Epoch: 1, Batch: 126, Loss: 1.0254380702972412\n",
      "Epoch: 1, Batch: 127, Loss: 1.0274114608764648\n",
      "Epoch: 1, Batch: 128, Loss: 1.0005919933319092\n",
      "Epoch: 1, Batch: 129, Loss: 0.9973080158233643\n",
      "Epoch: 1, Batch: 130, Loss: 0.8744701743125916\n",
      "Epoch: 1, Batch: 131, Loss: 1.1060963869094849\n",
      "Epoch: 1, Batch: 132, Loss: 1.1233452558517456\n",
      "Epoch: 1, Batch: 133, Loss: 0.9590736627578735\n",
      "Epoch: 1, Batch: 134, Loss: 0.9799518585205078\n",
      "Epoch: 1, Batch: 135, Loss: 1.1932501792907715\n",
      "Epoch: 1, Batch: 136, Loss: 1.0413349866867065\n",
      "Epoch: 1, Batch: 137, Loss: 1.0753276348114014\n",
      "Epoch: 1, Batch: 138, Loss: 1.135272741317749\n",
      "Epoch: 1, Batch: 139, Loss: 1.1742089986801147\n",
      "Epoch: 1, Batch: 140, Loss: 1.0409586429595947\n",
      "Epoch: 1, Batch: 141, Loss: 0.9466917514801025\n",
      "Epoch: 1, Batch: 142, Loss: 0.9567067623138428\n",
      "Epoch: 1, Batch: 143, Loss: 1.1522300243377686\n",
      "Epoch: 1, Batch: 144, Loss: 1.0401623249053955\n",
      "Epoch: 1, Batch: 145, Loss: 1.0170717239379883\n",
      "Epoch: 1, Batch: 146, Loss: 1.120682716369629\n",
      "Epoch: 1, Batch: 147, Loss: 1.0159004926681519\n",
      "Epoch: 1, Batch: 148, Loss: 1.0297517776489258\n",
      "Epoch: 1, Batch: 149, Loss: 0.9558879137039185\n",
      "Epoch: 1, Batch: 150, Loss: 1.1215741634368896\n",
      "Epoch: 1, Batch: 151, Loss: 1.1634962558746338\n",
      "Epoch: 1, Batch: 152, Loss: 0.950634777545929\n",
      "Epoch: 1, Batch: 153, Loss: 1.0691354274749756\n",
      "Epoch: 1, Batch: 154, Loss: 1.0533424615859985\n",
      "Epoch: 1, Batch: 155, Loss: 1.070541501045227\n",
      "Epoch: 1, Batch: 156, Loss: 1.0070199966430664\n",
      "Epoch: 1, Batch: 157, Loss: 1.041667103767395\n",
      "Epoch: 1, Batch: 158, Loss: 1.0713443756103516\n",
      "Epoch: 1, Batch: 159, Loss: 1.015528678894043\n",
      "Epoch: 1, Batch: 160, Loss: 0.9475535154342651\n",
      "Epoch: 1, Batch: 161, Loss: 1.0360480546951294\n",
      "Epoch: 1, Batch: 162, Loss: 1.0735387802124023\n",
      "Epoch: 1, Batch: 163, Loss: 1.1219024658203125\n",
      "Epoch: 1, Batch: 164, Loss: 1.0964869260787964\n",
      "Epoch: 1, Batch: 165, Loss: 1.0120275020599365\n",
      "Epoch: 1, Batch: 166, Loss: 1.102270483970642\n",
      "Epoch: 1, Batch: 167, Loss: 1.0698522329330444\n",
      "Epoch: 1, Batch: 168, Loss: 0.9290344715118408\n",
      "Epoch: 1, Batch: 169, Loss: 0.9706522822380066\n",
      "Epoch: 1, Batch: 170, Loss: 0.892534613609314\n",
      "Epoch: 1, Batch: 171, Loss: 1.2108619213104248\n",
      "Epoch: 1, Batch: 172, Loss: 1.1242108345031738\n",
      "Epoch: 1, Batch: 173, Loss: 0.9392998218536377\n",
      "Epoch: 1, Batch: 174, Loss: 0.9575105905532837\n",
      "Epoch: 1, Batch: 175, Loss: 1.0910594463348389\n",
      "Epoch: 1, Batch: 176, Loss: 0.9518375992774963\n",
      "Epoch: 1, Batch: 177, Loss: 0.8878216743469238\n",
      "Epoch: 1, Batch: 178, Loss: 0.9303451776504517\n",
      "Epoch: 1, Batch: 179, Loss: 1.034677505493164\n",
      "Epoch: 1, Batch: 180, Loss: 0.981107234954834\n",
      "Epoch: 1, Batch: 181, Loss: 0.9993551969528198\n",
      "Epoch: 1, Batch: 182, Loss: 1.0521745681762695\n",
      "Epoch: 1, Batch: 183, Loss: 1.0261483192443848\n",
      "Epoch: 1, Batch: 184, Loss: 1.1119754314422607\n",
      "Epoch: 1, Batch: 185, Loss: 1.0135712623596191\n",
      "Epoch: 1, Batch: 186, Loss: 1.0121304988861084\n",
      "Epoch: 1, Batch: 187, Loss: 0.9035619497299194\n",
      "Epoch: 1, Batch: 188, Loss: 0.9384081363677979\n",
      "Epoch: 1, Batch: 189, Loss: 0.8748157024383545\n",
      "Epoch: 1, Batch: 190, Loss: 0.96333909034729\n",
      "Epoch: 1, Batch: 191, Loss: 1.0208112001419067\n",
      "Epoch: 1, Batch: 192, Loss: 0.9988927841186523\n",
      "Epoch: 1, Batch: 193, Loss: 1.072110652923584\n",
      "Epoch: 1, Batch: 194, Loss: 1.0572844743728638\n",
      "Epoch: 1, Batch: 195, Loss: 0.9843423366546631\n",
      "Epoch: 1, Batch: 196, Loss: 1.2009997367858887\n",
      "Epoch: 1, Batch: 197, Loss: 1.1638141870498657\n",
      "Epoch: 1, Batch: 198, Loss: 0.9336052536964417\n",
      "Epoch: 1, Batch: 199, Loss: 1.0686125755310059\n",
      "Epoch: 1, Batch: 200, Loss: 1.0097006559371948\n",
      "Epoch: 1, Batch: 201, Loss: 1.0016286373138428\n",
      "Epoch: 1, Batch: 202, Loss: 1.0082651376724243\n",
      "Epoch: 1, Batch: 203, Loss: 1.027978539466858\n",
      "Epoch: 1, Batch: 204, Loss: 1.030972957611084\n",
      "Epoch: 1, Batch: 205, Loss: 0.9792042374610901\n",
      "Epoch: 1, Batch: 206, Loss: 0.9326215386390686\n",
      "Epoch: 1, Batch: 207, Loss: 1.1352699995040894\n",
      "Epoch: 1, Batch: 208, Loss: 0.9704107046127319\n",
      "Epoch: 1, Batch: 209, Loss: 0.9741621017456055\n",
      "Epoch: 1, Batch: 210, Loss: 1.0343002080917358\n",
      "Epoch: 1, Batch: 211, Loss: 0.9803896546363831\n",
      "Epoch: 1, Batch: 212, Loss: 1.029445767402649\n",
      "Epoch: 1, Batch: 213, Loss: 1.1070036888122559\n",
      "Epoch: 1, Batch: 214, Loss: 1.1292495727539062\n",
      "Epoch: 1, Batch: 215, Loss: 1.1084797382354736\n",
      "Epoch: 1, Batch: 216, Loss: 1.106865644454956\n",
      "Epoch: 1, Batch: 217, Loss: 0.8615140914916992\n",
      "Epoch: 1, Batch: 218, Loss: 0.959176778793335\n",
      "Epoch: 1, Batch: 219, Loss: 1.0743504762649536\n",
      "Epoch: 1, Batch: 220, Loss: 0.8925000429153442\n",
      "Epoch: 1, Batch: 221, Loss: 1.162428379058838\n",
      "Epoch: 1, Batch: 222, Loss: 1.0809776782989502\n",
      "Epoch: 1, Batch: 223, Loss: 1.1327166557312012\n",
      "Epoch: 1, Batch: 224, Loss: 1.0793488025665283\n",
      "Epoch: 1, Batch: 225, Loss: 1.090487003326416\n",
      "Epoch: 1, Batch: 226, Loss: 1.0850541591644287\n",
      "Epoch: 1, Batch: 227, Loss: 1.0044463872909546\n",
      "Epoch: 1, Batch: 228, Loss: 0.9440405964851379\n",
      "Epoch: 1, Batch: 229, Loss: 1.1014653444290161\n",
      "Epoch: 1, Batch: 230, Loss: 0.8771425485610962\n",
      "Epoch: 1, Batch: 231, Loss: 1.0790488719940186\n",
      "Epoch: 1, Batch: 232, Loss: 1.1136027574539185\n",
      "Epoch: 1, Batch: 233, Loss: 1.035330057144165\n",
      "Epoch: 1, Batch: 234, Loss: 1.0505132675170898\n",
      "Epoch: 1, Batch: 235, Loss: 1.0573872327804565\n",
      "Epoch: 1, Batch: 236, Loss: 0.9775721430778503\n",
      "Epoch: 1, Batch: 237, Loss: 0.9964134693145752\n",
      "Epoch: 1, Batch: 238, Loss: 0.9886296987533569\n",
      "Epoch: 1, Batch: 239, Loss: 1.0576438903808594\n",
      "Epoch: 1, Batch: 240, Loss: 1.1601381301879883\n",
      "Epoch: 1, Batch: 241, Loss: 0.917751669883728\n",
      "Epoch: 1, Batch: 242, Loss: 0.9375295639038086\n",
      "Epoch: 1, Batch: 243, Loss: 1.1553292274475098\n",
      "Epoch: 1, Batch: 244, Loss: 1.0489521026611328\n",
      "Epoch: 1, Batch: 245, Loss: 0.9720430374145508\n",
      "Epoch: 1, Batch: 246, Loss: 0.9342950582504272\n",
      "Epoch: 1, Batch: 247, Loss: 1.0581648349761963\n",
      "Epoch: 1, Batch: 248, Loss: 1.0760924816131592\n",
      "Epoch: 1, Batch: 249, Loss: 0.9427245259284973\n",
      "Epoch: 1, Batch: 250, Loss: 1.009809970855713\n",
      "Epoch: 1, Batch: 251, Loss: 1.0389595031738281\n",
      "Epoch: 1, Batch: 252, Loss: 1.0189557075500488\n",
      "Epoch: 1, Batch: 253, Loss: 1.0602763891220093\n",
      "Epoch: 1, Batch: 254, Loss: 0.9258627891540527\n",
      "Epoch: 1, Batch: 255, Loss: 1.059360384941101\n",
      "Epoch: 1, Batch: 256, Loss: 0.9048178195953369\n",
      "Epoch: 1, Batch: 257, Loss: 0.964995265007019\n",
      "Epoch: 1, Batch: 258, Loss: 0.9759233593940735\n",
      "Epoch: 1, Batch: 259, Loss: 0.9833387136459351\n",
      "Epoch: 1, Batch: 260, Loss: 1.0733914375305176\n",
      "Epoch: 1, Batch: 261, Loss: 1.021226167678833\n",
      "Epoch: 1, Batch: 262, Loss: 0.962591826915741\n",
      "Epoch: 1, Batch: 263, Loss: 1.0088729858398438\n",
      "Epoch: 1, Batch: 264, Loss: 1.0843902826309204\n",
      "Epoch: 1, Batch: 265, Loss: 0.9411680698394775\n",
      "Epoch: 1, Batch: 266, Loss: 1.1047368049621582\n",
      "Epoch: 1, Batch: 267, Loss: 0.9705725908279419\n",
      "Epoch: 1, Batch: 268, Loss: 1.0067267417907715\n",
      "Epoch: 1, Batch: 269, Loss: 0.9262218475341797\n",
      "Epoch: 1, Batch: 270, Loss: 0.9539384841918945\n",
      "Epoch: 1, Batch: 271, Loss: 0.899237334728241\n",
      "Epoch: 1, Batch: 272, Loss: 1.085267424583435\n",
      "Epoch: 1, Batch: 273, Loss: 0.9534929990768433\n",
      "Epoch: 1, Batch: 274, Loss: 1.0677108764648438\n",
      "Epoch: 1, Batch: 275, Loss: 0.935295581817627\n",
      "Epoch: 1, Batch: 276, Loss: 0.8646667003631592\n",
      "Epoch: 1, Batch: 277, Loss: 1.0314966440200806\n",
      "Epoch: 1, Batch: 278, Loss: 1.0208499431610107\n",
      "Epoch: 1, Batch: 279, Loss: 0.9417917728424072\n",
      "Epoch: 1, Batch: 280, Loss: 0.9157696962356567\n",
      "Epoch: 1, Batch: 281, Loss: 1.1439919471740723\n",
      "Epoch: 1, Batch: 282, Loss: 0.9514919519424438\n",
      "Epoch: 1, Batch: 283, Loss: 0.9370654821395874\n",
      "Epoch: 1, Batch: 284, Loss: 1.0373834371566772\n",
      "Epoch: 1, Batch: 285, Loss: 1.0428712368011475\n",
      "Epoch: 1, Batch: 286, Loss: 0.9125523567199707\n",
      "Epoch: 1, Batch: 287, Loss: 1.007217526435852\n",
      "Epoch: 1, Batch: 288, Loss: 0.9577583074569702\n",
      "Epoch: 1, Batch: 289, Loss: 1.014186978340149\n",
      "Epoch: 1, Batch: 290, Loss: 1.0291774272918701\n",
      "Epoch: 1, Batch: 291, Loss: 1.077864646911621\n",
      "Epoch: 1, Batch: 292, Loss: 0.979271650314331\n",
      "Epoch: 1, Batch: 293, Loss: 1.048331379890442\n",
      "Epoch: 1, Batch: 294, Loss: 1.0981662273406982\n",
      "Epoch: 1, Batch: 295, Loss: 0.9705643057823181\n",
      "Epoch: 1, Batch: 296, Loss: 1.0503355264663696\n",
      "Epoch: 1, Batch: 297, Loss: 0.9873868227005005\n",
      "Epoch: 1, Batch: 298, Loss: 0.9926337599754333\n",
      "Epoch: 1, Batch: 299, Loss: 1.0370454788208008\n",
      "Epoch: 1, Batch: 300, Loss: 0.8429867029190063\n",
      "Epoch: 1, Batch: 301, Loss: 1.040330410003662\n",
      "Epoch: 1, Batch: 302, Loss: 1.0614609718322754\n",
      "Epoch: 1, Batch: 303, Loss: 0.9322801232337952\n",
      "Epoch: 1, Batch: 304, Loss: 0.9553205370903015\n",
      "Epoch: 1, Batch: 305, Loss: 1.0120055675506592\n",
      "Epoch: 1, Batch: 306, Loss: 1.0403344631195068\n",
      "Epoch: 1, Batch: 307, Loss: 1.0492002964019775\n",
      "Epoch: 1, Batch: 308, Loss: 1.0813355445861816\n",
      "Epoch: 1, Batch: 309, Loss: 1.0075486898422241\n",
      "Epoch: 1, Batch: 310, Loss: 0.9891348481178284\n",
      "Epoch: 1, Batch: 311, Loss: 1.0114105939865112\n",
      "Epoch: 1, Batch: 312, Loss: 1.084113597869873\n",
      "Epoch: 1, Batch: 313, Loss: 1.1194920539855957\n",
      "Epoch: 1, Batch: 314, Loss: 0.9582657217979431\n",
      "Epoch: 1, Batch: 315, Loss: 0.874060869216919\n",
      "Epoch: 1, Batch: 316, Loss: 0.9382529258728027\n",
      "Epoch: 1, Batch: 317, Loss: 1.0368732213974\n",
      "Epoch: 1, Batch: 318, Loss: 1.142964243888855\n",
      "Epoch: 1, Batch: 319, Loss: 1.109710931777954\n",
      "Epoch: 1, Batch: 320, Loss: 1.026185393333435\n",
      "Epoch: 1, Batch: 321, Loss: 1.1152101755142212\n",
      "Epoch: 1, Batch: 322, Loss: 1.108736276626587\n",
      "Epoch: 1, Batch: 323, Loss: 1.044729471206665\n",
      "Epoch: 1, Batch: 324, Loss: 0.9666838049888611\n",
      "Epoch: 1, Batch: 325, Loss: 1.1551146507263184\n",
      "Epoch: 1, Batch: 326, Loss: 0.9644492864608765\n",
      "Epoch: 1, Batch: 327, Loss: 1.0334290266036987\n",
      "Epoch: 1, Batch: 328, Loss: 1.090362787246704\n",
      "Epoch: 1, Batch: 329, Loss: 1.049506425857544\n",
      "Epoch: 1, Batch: 330, Loss: 1.0802282094955444\n",
      "Epoch: 1, Batch: 331, Loss: 0.9544795751571655\n",
      "Epoch: 1, Batch: 332, Loss: 0.9325583577156067\n",
      "Epoch: 1, Batch: 333, Loss: 1.0975998640060425\n",
      "Epoch: 1, Batch: 334, Loss: 1.1124076843261719\n",
      "Epoch: 1, Batch: 335, Loss: 0.7768294811248779\n",
      "Epoch: 1, Batch: 336, Loss: 1.0542731285095215\n",
      "Epoch: 1, Batch: 337, Loss: 1.1299643516540527\n",
      "Epoch: 1, Batch: 338, Loss: 0.9710112810134888\n",
      "Epoch: 1, Batch: 339, Loss: 0.9834064245223999\n",
      "Epoch: 1, Batch: 340, Loss: 0.9440670013427734\n",
      "Epoch: 1, Batch: 341, Loss: 0.9734339118003845\n",
      "Epoch: 1, Batch: 342, Loss: 0.8879942893981934\n",
      "Epoch: 1, Batch: 343, Loss: 0.9018538594245911\n",
      "Epoch: 1, Batch: 344, Loss: 0.9756643772125244\n",
      "Epoch: 1, Batch: 345, Loss: 1.040564775466919\n",
      "Epoch: 1, Batch: 346, Loss: 0.9673702716827393\n",
      "Epoch: 1, Batch: 347, Loss: 1.0395798683166504\n",
      "Epoch: 1, Batch: 348, Loss: 1.2162630558013916\n",
      "Epoch: 1, Batch: 349, Loss: 0.9702063202857971\n",
      "Epoch: 1, Batch: 350, Loss: 0.9739168882369995\n",
      "Epoch: 1, Batch: 351, Loss: 0.9803059697151184\n",
      "Epoch: 1, Batch: 352, Loss: 0.9695817828178406\n",
      "Epoch: 1, Batch: 353, Loss: 1.0128763914108276\n",
      "Epoch: 1, Batch: 354, Loss: 0.9874477386474609\n",
      "Epoch: 1, Batch: 355, Loss: 1.027112364768982\n",
      "Epoch: 1, Batch: 356, Loss: 0.9911539554595947\n",
      "Epoch: 1, Batch: 357, Loss: 1.0431232452392578\n",
      "Epoch: 1, Batch: 358, Loss: 1.0006566047668457\n",
      "Epoch: 1, Batch: 359, Loss: 1.0210843086242676\n",
      "Epoch: 1, Batch: 360, Loss: 0.9639096260070801\n",
      "Epoch: 1, Batch: 361, Loss: 0.8839750289916992\n",
      "Epoch: 1, Batch: 362, Loss: 0.9087313413619995\n",
      "Epoch: 1, Batch: 363, Loss: 0.9779109954833984\n",
      "Epoch: 1, Batch: 364, Loss: 1.1854736804962158\n",
      "Epoch: 1, Batch: 365, Loss: 0.9781135320663452\n",
      "Epoch: 1, Batch: 366, Loss: 1.0473132133483887\n",
      "Epoch: 1, Batch: 367, Loss: 0.956017255783081\n",
      "Epoch: 1, Batch: 368, Loss: 0.9967367649078369\n",
      "Epoch: 1, Batch: 369, Loss: 0.9617618322372437\n",
      "Epoch: 1, Batch: 370, Loss: 1.0917567014694214\n",
      "Epoch: 1, Batch: 371, Loss: 1.1290414333343506\n",
      "Epoch: 1, Batch: 372, Loss: 1.034633755683899\n",
      "Epoch: 1, Batch: 373, Loss: 1.0434545278549194\n",
      "Epoch: 1, Batch: 374, Loss: 1.0909333229064941\n",
      "Epoch: 1, Batch: 375, Loss: 0.9867684245109558\n",
      "Epoch: 1, Batch: 376, Loss: 0.8869324922561646\n",
      "Epoch: 1, Batch: 377, Loss: 0.9241083860397339\n",
      "Epoch: 1, Batch: 378, Loss: 1.0138412714004517\n",
      "Epoch: 1, Batch: 379, Loss: 0.9913995265960693\n",
      "Epoch: 1, Batch: 380, Loss: 0.9397194385528564\n",
      "Epoch: 1, Batch: 381, Loss: 1.020190954208374\n",
      "Epoch: 1, Batch: 382, Loss: 0.8924422264099121\n",
      "Epoch: 1, Batch: 383, Loss: 0.971247673034668\n",
      "Epoch: 1, Batch: 384, Loss: 0.9993033409118652\n",
      "Epoch: 1, Batch: 385, Loss: 1.0184106826782227\n",
      "Epoch: 1, Batch: 386, Loss: 1.051666259765625\n",
      "Epoch: 1, Batch: 387, Loss: 1.0133061408996582\n",
      "Epoch: 1, Batch: 388, Loss: 1.0087051391601562\n",
      "Epoch: 1, Batch: 389, Loss: 1.0280154943466187\n",
      "Epoch: 1, Batch: 390, Loss: 0.8258017301559448\n",
      "Epoch: 1, Batch: 391, Loss: 0.9988189935684204\n",
      "Epoch: 1, Batch: 392, Loss: 0.9752689003944397\n",
      "Epoch: 1, Batch: 393, Loss: 0.8593909740447998\n",
      "Epoch: 1, Batch: 394, Loss: 1.323146104812622\n",
      "Epoch: 1, Batch: 395, Loss: 1.0167310237884521\n",
      "Epoch: 1, Batch: 396, Loss: 0.9442622661590576\n",
      "Epoch: 1, Batch: 397, Loss: 0.9920710325241089\n",
      "Epoch: 1, Batch: 398, Loss: 1.035571813583374\n",
      "Epoch: 1, Batch: 399, Loss: 1.004408359527588\n",
      "Epoch: 1, Batch: 400, Loss: 1.0389328002929688\n",
      "Epoch: 1, Batch: 401, Loss: 1.0020420551300049\n",
      "Epoch: 1, Batch: 402, Loss: 1.011833906173706\n",
      "Epoch: 1, Batch: 403, Loss: 0.9829708933830261\n",
      "Epoch: 1, Batch: 404, Loss: 1.0233285427093506\n",
      "Epoch: 1, Batch: 405, Loss: 1.0924429893493652\n",
      "Epoch: 1, Batch: 406, Loss: 1.0332512855529785\n",
      "Epoch: 1, Batch: 407, Loss: 1.0283610820770264\n",
      "Epoch: 1, Batch: 408, Loss: 0.9634557962417603\n",
      "Epoch: 1, Batch: 409, Loss: 0.8811044692993164\n",
      "Epoch: 1, Batch: 410, Loss: 1.0401277542114258\n",
      "Epoch: 1, Batch: 411, Loss: 1.0205076932907104\n",
      "Epoch: 1, Batch: 412, Loss: 1.026272177696228\n",
      "Epoch: 1, Batch: 413, Loss: 0.9523558616638184\n",
      "Epoch: 1, Batch: 414, Loss: 1.1085284948349\n",
      "Epoch: 1, Batch: 415, Loss: 1.0158448219299316\n",
      "Epoch: 1, Batch: 416, Loss: 0.9684489965438843\n",
      "Epoch: 1, Batch: 417, Loss: 1.0210893154144287\n",
      "Epoch: 1, Batch: 418, Loss: 1.0746564865112305\n",
      "Epoch: 1, Batch: 419, Loss: 1.0107452869415283\n",
      "Epoch: 1, Batch: 420, Loss: 1.0465061664581299\n",
      "Epoch: 1, Batch: 421, Loss: 0.9775938987731934\n",
      "Epoch: 1, Batch: 422, Loss: 0.9864885807037354\n",
      "Epoch: 1, Batch: 423, Loss: 1.1025810241699219\n",
      "Epoch: 1, Batch: 424, Loss: 1.009762167930603\n",
      "Epoch: 1, Batch: 425, Loss: 1.1731553077697754\n",
      "Epoch: 1, Batch: 426, Loss: 0.9143617153167725\n",
      "Epoch: 1, Batch: 427, Loss: 0.8898724317550659\n",
      "Epoch: 1, Batch: 428, Loss: 1.056039810180664\n",
      "Epoch: 1, Batch: 429, Loss: 1.1036455631256104\n",
      "Epoch: 1, Batch: 430, Loss: 1.0060368776321411\n",
      "Epoch: 1, Batch: 431, Loss: 1.1086361408233643\n",
      "Epoch: 1, Batch: 432, Loss: 0.9917176365852356\n",
      "Epoch: 1, Batch: 433, Loss: 0.8491125702857971\n",
      "Epoch: 1, Batch: 434, Loss: 0.8662809133529663\n",
      "Epoch: 1, Batch: 435, Loss: 0.941923201084137\n",
      "Epoch: 1, Batch: 436, Loss: 1.0711389780044556\n",
      "Epoch: 1, Batch: 437, Loss: 0.9753298163414001\n",
      "Epoch: 1, Batch: 438, Loss: 0.9752978086471558\n",
      "Epoch: 1, Batch: 439, Loss: 0.9662478566169739\n",
      "Epoch: 1, Batch: 440, Loss: 1.0319573879241943\n",
      "Epoch: 1, Batch: 441, Loss: 1.0645184516906738\n",
      "Epoch: 1, Batch: 442, Loss: 0.9235218167304993\n",
      "Epoch: 1, Batch: 443, Loss: 1.014631986618042\n",
      "Epoch: 1, Batch: 444, Loss: 1.1640254259109497\n",
      "Epoch: 1, Batch: 445, Loss: 0.9548931121826172\n",
      "Epoch: 1, Batch: 446, Loss: 1.0637773275375366\n",
      "Epoch: 1, Batch: 447, Loss: 1.0452985763549805\n",
      "Epoch: 1, Batch: 448, Loss: 1.1293280124664307\n",
      "Epoch: 1, Batch: 449, Loss: 0.8461508750915527\n",
      "Epoch: 1, Batch: 450, Loss: 1.0630306005477905\n",
      "Epoch: 1, Batch: 451, Loss: 1.088010549545288\n",
      "Epoch: 1, Batch: 452, Loss: 1.199992060661316\n",
      "Epoch: 1, Batch: 453, Loss: 0.979095995426178\n",
      "Epoch: 1, Batch: 454, Loss: 0.9913268089294434\n",
      "Epoch: 1, Batch: 455, Loss: 1.033447504043579\n",
      "Epoch: 1, Batch: 456, Loss: 1.0498154163360596\n",
      "Epoch: 1, Batch: 457, Loss: 0.9549769163131714\n",
      "Epoch: 1, Batch: 458, Loss: 1.0151221752166748\n",
      "Epoch: 1, Batch: 459, Loss: 0.8988453149795532\n",
      "Epoch: 1, Batch: 460, Loss: 1.0646603107452393\n",
      "Epoch: 1, Batch: 461, Loss: 0.9309889078140259\n",
      "Epoch: 1, Batch: 462, Loss: 0.9252408742904663\n",
      "Epoch: 1, Batch: 463, Loss: 1.094235897064209\n",
      "Epoch: 1, Batch: 464, Loss: 1.0307866334915161\n",
      "Epoch: 1, Batch: 465, Loss: 1.123172640800476\n",
      "Epoch: 1, Batch: 466, Loss: 0.8955731391906738\n",
      "Epoch: 1, Batch: 467, Loss: 0.9368939995765686\n",
      "Epoch: 1, Batch: 468, Loss: 0.878445029258728\n",
      "Epoch: 1, Batch: 469, Loss: 0.9882318377494812\n",
      "Epoch: 1, Batch: 470, Loss: 0.9809059500694275\n",
      "Epoch: 1, Batch: 471, Loss: 0.9853262901306152\n",
      "Epoch: 1, Batch: 472, Loss: 1.0809581279754639\n",
      "Epoch: 1, Batch: 473, Loss: 0.9617194533348083\n",
      "Epoch: 1, Batch: 474, Loss: 1.0732132196426392\n",
      "Epoch: 1, Batch: 475, Loss: 1.0392979383468628\n",
      "Epoch: 1, Batch: 476, Loss: 0.9421557188034058\n",
      "Epoch: 1, Batch: 477, Loss: 1.039833426475525\n",
      "Epoch: 1, Batch: 478, Loss: 0.8759719133377075\n",
      "Epoch: 1, Batch: 479, Loss: 0.9435214400291443\n",
      "Epoch: 1, Batch: 480, Loss: 0.9984859824180603\n",
      "Epoch: 1, Batch: 481, Loss: 1.1392016410827637\n",
      "Epoch: 1, Batch: 482, Loss: 0.973600447177887\n",
      "Epoch: 1, Batch: 483, Loss: 1.0306432247161865\n",
      "Epoch: 1, Batch: 484, Loss: 1.010235071182251\n",
      "Epoch: 1, Batch: 485, Loss: 0.9938830137252808\n",
      "Epoch: 1, Batch: 486, Loss: 0.8678930997848511\n",
      "Epoch: 1, Batch: 487, Loss: 1.1093499660491943\n",
      "Epoch: 1, Batch: 488, Loss: 1.1243293285369873\n",
      "Epoch: 1, Batch: 489, Loss: 0.977057933807373\n",
      "Epoch: 1, Batch: 490, Loss: 1.1177196502685547\n",
      "Epoch: 1, Batch: 491, Loss: 1.0370079278945923\n",
      "Epoch: 1, Batch: 492, Loss: 1.0041286945343018\n",
      "Epoch: 1, Batch: 493, Loss: 1.0433651208877563\n",
      "Epoch: 1, Batch: 494, Loss: 1.032217025756836\n",
      "Epoch: 1, Batch: 495, Loss: 0.9904302358627319\n",
      "Epoch: 1, Batch: 496, Loss: 1.0741734504699707\n",
      "Epoch: 1, Batch: 497, Loss: 0.9413264989852905\n",
      "Epoch: 1, Batch: 498, Loss: 0.8960686326026917\n",
      "Epoch: 1, Batch: 499, Loss: 0.8880874514579773\n",
      "Epoch: 1, Batch: 500, Loss: 0.9419965147972107\n",
      "Epoch: 1, Batch: 501, Loss: 0.9786100387573242\n",
      "Epoch: 1, Batch: 502, Loss: 1.0120927095413208\n",
      "Epoch: 1, Batch: 503, Loss: 0.9825692176818848\n",
      "Epoch: 1, Batch: 504, Loss: 1.0457782745361328\n",
      "Epoch: 1, Batch: 505, Loss: 0.9379929304122925\n",
      "Epoch: 1, Batch: 506, Loss: 1.005903720855713\n",
      "Epoch: 1, Batch: 507, Loss: 0.8763800859451294\n",
      "Epoch: 1, Batch: 508, Loss: 1.0620825290679932\n",
      "Epoch: 1, Batch: 509, Loss: 0.9145652651786804\n",
      "Epoch: 1, Batch: 510, Loss: 1.009183645248413\n",
      "Epoch: 1, Batch: 511, Loss: 0.99527907371521\n",
      "Epoch: 1, Batch: 512, Loss: 1.0080913305282593\n",
      "Epoch: 1, Batch: 513, Loss: 0.97492516040802\n",
      "Epoch: 1, Batch: 514, Loss: 0.9003196954727173\n",
      "Epoch: 1, Batch: 515, Loss: 0.9604514241218567\n",
      "Epoch: 1, Batch: 516, Loss: 0.8937729001045227\n",
      "Epoch: 1, Batch: 517, Loss: 1.087408185005188\n",
      "Epoch: 1, Batch: 518, Loss: 0.9698718190193176\n",
      "Epoch: 1, Batch: 519, Loss: 0.9287070631980896\n",
      "Epoch: 1, Batch: 520, Loss: 0.9853419661521912\n",
      "Epoch: 1, Batch: 521, Loss: 0.9922236204147339\n",
      "Epoch: 1, Batch: 522, Loss: 1.053095817565918\n",
      "Epoch: 1, Batch: 523, Loss: 0.8986728191375732\n",
      "Epoch: 1, Batch: 524, Loss: 0.8858232498168945\n",
      "Epoch: 1, Batch: 525, Loss: 0.9916691184043884\n",
      "Epoch: 1, Batch: 526, Loss: 1.037471055984497\n",
      "Epoch: 1, Batch: 527, Loss: 1.1248905658721924\n",
      "Epoch: 1, Batch: 528, Loss: 0.9434295892715454\n",
      "Epoch: 1, Batch: 529, Loss: 1.008346676826477\n",
      "Epoch: 1, Batch: 530, Loss: 0.9948858618736267\n",
      "Epoch: 1, Batch: 531, Loss: 0.9393985867500305\n",
      "Epoch: 1, Batch: 532, Loss: 1.0465162992477417\n",
      "Epoch: 1, Batch: 533, Loss: 1.0972434282302856\n",
      "Epoch: 1, Batch: 534, Loss: 1.08153235912323\n",
      "Epoch: 1, Batch: 535, Loss: 0.965659499168396\n",
      "Epoch: 1, Batch: 536, Loss: 0.9705526828765869\n",
      "Epoch: 1, Batch: 537, Loss: 0.997736930847168\n",
      "Epoch: 1, Batch: 538, Loss: 1.0301470756530762\n",
      "Epoch: 1, Batch: 539, Loss: 0.9953882694244385\n",
      "Epoch: 1, Batch: 540, Loss: 1.004900336265564\n",
      "Epoch: 1, Batch: 541, Loss: 1.0728567838668823\n",
      "Epoch: 1, Batch: 542, Loss: 1.0622398853302002\n",
      "Epoch: 1, Batch: 543, Loss: 0.9232109189033508\n",
      "Epoch: 1, Batch: 544, Loss: 0.9772879481315613\n",
      "Epoch: 1, Batch: 545, Loss: 0.9300796985626221\n",
      "Epoch: 1, Batch: 546, Loss: 0.9691563248634338\n",
      "Epoch: 1, Batch: 547, Loss: 1.123311996459961\n",
      "Epoch: 1, Batch: 548, Loss: 1.0858336687088013\n",
      "Epoch: 1, Batch: 549, Loss: 1.0151214599609375\n",
      "Epoch: 1, Batch: 550, Loss: 1.0724775791168213\n",
      "Epoch: 1, Batch: 551, Loss: 1.0811835527420044\n",
      "Epoch: 1, Batch: 552, Loss: 1.0011565685272217\n",
      "Epoch: 1, Batch: 553, Loss: 1.0505750179290771\n",
      "Epoch: 1, Batch: 554, Loss: 0.9648985266685486\n",
      "Epoch: 1, Batch: 555, Loss: 1.0474097728729248\n",
      "Epoch: 1, Batch: 556, Loss: 0.9788262844085693\n",
      "Epoch: 1, Batch: 557, Loss: 1.0233049392700195\n",
      "Epoch: 1, Batch: 558, Loss: 0.9513833522796631\n",
      "Epoch: 1, Batch: 559, Loss: 1.0787845849990845\n",
      "Epoch: 1, Batch: 560, Loss: 1.0710842609405518\n",
      "Epoch: 1, Batch: 561, Loss: 1.1242623329162598\n",
      "Epoch: 1, Batch: 562, Loss: 0.8821815848350525\n",
      "Epoch: 1, Batch: 563, Loss: 0.945085883140564\n",
      "Epoch: 1, Batch: 564, Loss: 0.9609942436218262\n",
      "Epoch: 1, Batch: 565, Loss: 1.0391044616699219\n",
      "Epoch: 1, Batch: 566, Loss: 1.0942015647888184\n",
      "Epoch: 1, Batch: 567, Loss: 1.013709306716919\n",
      "Epoch: 1, Batch: 568, Loss: 0.9936743974685669\n",
      "Epoch: 1, Batch: 569, Loss: 0.8994207382202148\n",
      "Epoch: 1, Batch: 570, Loss: 0.8538221120834351\n",
      "Epoch: 1, Batch: 571, Loss: 0.9252831935882568\n",
      "Epoch: 1, Batch: 572, Loss: 1.0360944271087646\n",
      "Epoch: 1, Batch: 573, Loss: 0.9644225835800171\n",
      "Epoch: 1, Batch: 574, Loss: 1.051599144935608\n",
      "Epoch: 1, Batch: 575, Loss: 0.8277764320373535\n",
      "Epoch: 1, Batch: 576, Loss: 0.9731001853942871\n",
      "Epoch: 1, Batch: 577, Loss: 0.8746883869171143\n",
      "Epoch: 1, Batch: 578, Loss: 0.9234843254089355\n",
      "Epoch: 1, Batch: 579, Loss: 0.8752468824386597\n",
      "Epoch: 1, Batch: 580, Loss: 0.9959636330604553\n",
      "Epoch: 1, Batch: 581, Loss: 0.9128693342208862\n",
      "Epoch: 1, Batch: 582, Loss: 1.0685350894927979\n",
      "Epoch: 1, Batch: 583, Loss: 0.9896057844161987\n",
      "Epoch: 1, Batch: 584, Loss: 1.0076007843017578\n",
      "Epoch: 1, Batch: 585, Loss: 0.9750267267227173\n",
      "Epoch: 1, Batch: 586, Loss: 0.9816206693649292\n",
      "Epoch: 1, Batch: 587, Loss: 1.0726113319396973\n",
      "Epoch: 1, Batch: 588, Loss: 1.0118958950042725\n",
      "Epoch: 1, Batch: 589, Loss: 1.0496840476989746\n",
      "Epoch: 1, Batch: 590, Loss: 1.0742771625518799\n",
      "Epoch: 1, Batch: 591, Loss: 0.9317706227302551\n",
      "Epoch: 1, Batch: 592, Loss: 1.0307698249816895\n",
      "Epoch: 1, Batch: 593, Loss: 0.980312705039978\n",
      "Epoch: 1, Batch: 594, Loss: 0.9223983287811279\n",
      "Epoch: 1, Batch: 595, Loss: 0.8537930250167847\n",
      "Epoch: 1, Batch: 596, Loss: 1.0338213443756104\n",
      "Epoch: 1, Batch: 597, Loss: 1.055516004562378\n",
      "Epoch: 1, Batch: 598, Loss: 0.9826242923736572\n",
      "Epoch: 1, Batch: 599, Loss: 1.0245383977890015\n",
      "Epoch: 1, Batch: 600, Loss: 1.058379888534546\n",
      "Epoch: 1, Batch: 601, Loss: 0.9027252197265625\n",
      "Epoch: 1, Batch: 602, Loss: 1.0408328771591187\n",
      "Epoch: 1, Batch: 603, Loss: 0.975326657295227\n",
      "Epoch: 1, Batch: 604, Loss: 0.9191499948501587\n",
      "Epoch: 1, Batch: 605, Loss: 0.9728525876998901\n",
      "Epoch: 1, Batch: 606, Loss: 1.0165048837661743\n",
      "Epoch: 1, Batch: 607, Loss: 1.0488414764404297\n",
      "Epoch: 1, Batch: 608, Loss: 1.0590115785598755\n",
      "Epoch: 1, Batch: 609, Loss: 1.0410016775131226\n",
      "Epoch: 1, Batch: 610, Loss: 1.1020286083221436\n",
      "Epoch: 1, Batch: 611, Loss: 1.0082523822784424\n",
      "Epoch: 1, Batch: 612, Loss: 0.9586095809936523\n",
      "Epoch: 1, Batch: 613, Loss: 1.0849685668945312\n",
      "Epoch: 1, Batch: 614, Loss: 1.081310749053955\n",
      "Epoch: 1, Batch: 615, Loss: 1.0319819450378418\n",
      "Epoch: 1, Batch: 616, Loss: 0.9954686164855957\n",
      "Epoch: 1, Batch: 617, Loss: 0.9644353985786438\n",
      "Epoch: 1, Batch: 618, Loss: 1.0900218486785889\n",
      "Epoch: 1, Batch: 619, Loss: 1.0571744441986084\n",
      "Epoch: 1, Batch: 620, Loss: 0.9694476127624512\n",
      "Epoch: 1, Batch: 621, Loss: 0.9310895204544067\n",
      "Epoch: 1, Batch: 622, Loss: 1.0305960178375244\n",
      "Epoch: 1, Batch: 623, Loss: 0.9965928792953491\n",
      "Epoch: 1, Batch: 624, Loss: 1.0527149438858032\n",
      "Epoch: 1, Batch: 625, Loss: 0.9003999829292297\n",
      "Epoch: 1, Batch: 626, Loss: 1.1146342754364014\n",
      "Epoch: 1, Batch: 627, Loss: 1.0171666145324707\n",
      "Epoch: 1, Batch: 628, Loss: 0.9964190125465393\n",
      "Epoch: 1, Batch: 629, Loss: 1.0686945915222168\n",
      "Epoch: 1, Batch: 630, Loss: 1.0745916366577148\n",
      "Epoch: 1, Batch: 631, Loss: 0.8877673149108887\n",
      "Epoch: 1, Batch: 632, Loss: 1.0365493297576904\n",
      "Epoch: 1, Batch: 633, Loss: 0.9971998333930969\n",
      "Epoch: 1, Batch: 634, Loss: 0.9534270167350769\n",
      "Epoch: 1, Batch: 635, Loss: 1.0099014043807983\n",
      "Epoch: 1, Batch: 636, Loss: 0.8831570744514465\n",
      "Epoch: 1, Batch: 637, Loss: 1.0269315242767334\n",
      "Epoch: 1, Batch: 638, Loss: 1.0621368885040283\n",
      "Epoch: 1, Batch: 639, Loss: 0.9068074822425842\n",
      "Epoch: 1, Batch: 640, Loss: 0.970732569694519\n",
      "Epoch: 1, Batch: 641, Loss: 0.998706579208374\n",
      "Epoch: 1, Batch: 642, Loss: 0.9572921991348267\n",
      "Epoch: 1, Batch: 643, Loss: 1.0036239624023438\n",
      "Epoch: 1, Batch: 644, Loss: 1.0089516639709473\n",
      "Epoch: 1, Batch: 645, Loss: 0.9965299963951111\n",
      "Epoch: 1, Batch: 646, Loss: 0.9961896538734436\n",
      "Epoch: 1, Batch: 647, Loss: 1.0146546363830566\n",
      "Epoch: 1, Batch: 648, Loss: 1.013527750968933\n",
      "Epoch: 1, Batch: 649, Loss: 0.9739028215408325\n",
      "Epoch: 1, Batch: 650, Loss: 0.946326494216919\n",
      "Epoch: 1, Batch: 651, Loss: 0.8895058631896973\n",
      "Epoch: 1, Batch: 652, Loss: 1.0071468353271484\n",
      "Epoch: 1, Batch: 653, Loss: 0.9942600727081299\n",
      "Epoch: 1, Batch: 654, Loss: 1.0563435554504395\n",
      "Epoch: 1, Batch: 655, Loss: 1.0931907892227173\n",
      "Epoch: 1, Batch: 656, Loss: 1.0198376178741455\n",
      "Epoch: 1, Batch: 657, Loss: 0.9908141493797302\n",
      "Epoch: 1, Batch: 658, Loss: 0.9395448565483093\n",
      "Epoch: 1, Batch: 659, Loss: 1.0595539808273315\n",
      "Epoch: 1, Batch: 660, Loss: 1.0112783908843994\n",
      "Epoch: 1, Batch: 661, Loss: 0.949411153793335\n",
      "Epoch: 1, Batch: 662, Loss: 0.9330154061317444\n",
      "Epoch: 1, Batch: 663, Loss: 1.0254765748977661\n",
      "Epoch: 1, Batch: 664, Loss: 0.8177621364593506\n",
      "Epoch: 1, Batch: 665, Loss: 0.8923557996749878\n",
      "Epoch: 1, Batch: 666, Loss: 1.0567059516906738\n",
      "Epoch: 1, Batch: 667, Loss: 0.9900707006454468\n",
      "Epoch: 1, Batch: 668, Loss: 0.9476260542869568\n",
      "Epoch: 1, Batch: 669, Loss: 1.0332471132278442\n",
      "Epoch: 1, Batch: 670, Loss: 1.0161206722259521\n",
      "Epoch: 1, Batch: 671, Loss: 0.8672539591789246\n",
      "Epoch: 1, Batch: 672, Loss: 0.9804040193557739\n",
      "Epoch: 1, Batch: 673, Loss: 1.074385404586792\n",
      "Epoch: 1, Batch: 674, Loss: 0.9889474511146545\n",
      "Epoch: 1, Batch: 675, Loss: 1.1725828647613525\n",
      "Epoch: 1, Batch: 676, Loss: 0.9053162336349487\n",
      "Epoch: 1, Batch: 677, Loss: 0.8799627423286438\n",
      "Epoch: 1, Batch: 678, Loss: 1.1038496494293213\n",
      "Epoch: 1, Batch: 679, Loss: 0.8896994590759277\n",
      "Epoch: 1, Batch: 680, Loss: 0.903695285320282\n",
      "Epoch: 1, Batch: 681, Loss: 1.0689425468444824\n",
      "Epoch: 1, Batch: 682, Loss: 1.0077145099639893\n",
      "Epoch: 1, Batch: 683, Loss: 0.977647602558136\n",
      "Epoch: 1, Batch: 684, Loss: 1.0366579294204712\n",
      "Epoch: 1, Batch: 685, Loss: 1.0544123649597168\n",
      "Epoch: 1, Batch: 686, Loss: 0.9553958773612976\n",
      "Epoch: 1, Batch: 687, Loss: 0.9891446828842163\n",
      "Epoch: 1, Batch: 688, Loss: 1.016402006149292\n",
      "Epoch: 1, Batch: 689, Loss: 0.8848097324371338\n",
      "Epoch: 1, Batch: 690, Loss: 0.9536306858062744\n",
      "Epoch: 1, Batch: 691, Loss: 0.929908275604248\n",
      "Epoch: 1, Batch: 692, Loss: 1.1324334144592285\n",
      "Epoch: 1, Batch: 693, Loss: 1.0323972702026367\n",
      "Epoch: 1, Batch: 694, Loss: 1.077518105506897\n",
      "Epoch: 1, Batch: 695, Loss: 0.9103341102600098\n",
      "Epoch: 1, Batch: 696, Loss: 0.9852652549743652\n",
      "Epoch: 1, Batch: 697, Loss: 1.0413789749145508\n",
      "Epoch: 1, Batch: 698, Loss: 0.9960691928863525\n",
      "Epoch: 1, Batch: 699, Loss: 0.9754927158355713\n",
      "Epoch: 1, Batch: 700, Loss: 0.8738943934440613\n",
      "Epoch: 1, Batch: 701, Loss: 0.898741602897644\n",
      "Epoch: 1, Batch: 702, Loss: 1.0643430948257446\n",
      "Epoch: 1, Batch: 703, Loss: 1.01803457736969\n",
      "Epoch: 1, Batch: 704, Loss: 1.1352872848510742\n",
      "Epoch: 1, Batch: 705, Loss: 0.9984920024871826\n",
      "Epoch: 1, Batch: 706, Loss: 1.0096817016601562\n",
      "Epoch: 1, Batch: 707, Loss: 1.0562512874603271\n",
      "Epoch: 1, Batch: 708, Loss: 0.9790712594985962\n",
      "Epoch: 1, Batch: 709, Loss: 1.146611213684082\n",
      "Epoch: 1, Batch: 710, Loss: 1.0392348766326904\n",
      "Epoch: 1, Batch: 711, Loss: 0.9145200252532959\n",
      "Epoch: 1, Batch: 712, Loss: 0.9925709962844849\n",
      "Epoch: 1, Batch: 713, Loss: 1.0103880167007446\n",
      "Epoch: 1, Batch: 714, Loss: 0.9490204453468323\n",
      "Epoch: 1, Batch: 715, Loss: 1.0347685813903809\n",
      "Epoch: 1, Batch: 716, Loss: 0.8679265975952148\n",
      "Epoch: 1, Batch: 717, Loss: 1.0955116748809814\n",
      "Epoch: 1, Batch: 718, Loss: 0.9291952252388\n",
      "Epoch: 1, Batch: 719, Loss: 1.014817714691162\n",
      "Epoch: 1, Batch: 720, Loss: 0.9023601412773132\n",
      "Epoch: 1, Batch: 721, Loss: 1.0118498802185059\n",
      "Epoch: 1, Batch: 722, Loss: 0.9033719301223755\n",
      "Epoch: 1, Batch: 723, Loss: 0.982062816619873\n",
      "Epoch: 1, Batch: 724, Loss: 0.8992152810096741\n",
      "Epoch: 1, Batch: 725, Loss: 0.9507853388786316\n",
      "Epoch: 1, Batch: 726, Loss: 1.1471081972122192\n",
      "Epoch: 1, Batch: 727, Loss: 0.961959958076477\n",
      "Epoch: 1, Batch: 728, Loss: 1.0881258249282837\n",
      "Epoch: 1, Batch: 729, Loss: 0.9832040071487427\n",
      "Epoch: 1, Batch: 730, Loss: 0.8909503817558289\n",
      "Epoch: 1, Batch: 731, Loss: 1.0682811737060547\n",
      "Epoch: 1, Batch: 732, Loss: 1.027494192123413\n",
      "Epoch: 1, Batch: 733, Loss: 0.9520272612571716\n",
      "Epoch: 1, Batch: 734, Loss: 0.9854292869567871\n",
      "Epoch: 1, Batch: 735, Loss: 0.9304378628730774\n",
      "Epoch: 1, Batch: 736, Loss: 1.0378787517547607\n",
      "Epoch: 1, Batch: 737, Loss: 0.974230170249939\n",
      "Epoch: 1, Batch: 738, Loss: 0.8952839374542236\n",
      "Epoch: 1, Batch: 739, Loss: 0.9326164722442627\n",
      "Epoch: 1, Batch: 740, Loss: 0.9248068332672119\n",
      "Epoch: 1, Batch: 741, Loss: 0.9978080987930298\n",
      "Epoch: 1, Batch: 742, Loss: 0.9621322751045227\n",
      "Epoch: 1, Batch: 743, Loss: 0.9521427154541016\n",
      "Epoch: 1, Batch: 744, Loss: 1.0664608478546143\n",
      "Epoch: 1, Batch: 745, Loss: 0.866426944732666\n",
      "Epoch: 1, Batch: 746, Loss: 0.9168590307235718\n",
      "Epoch: 1, Batch: 747, Loss: 0.9602630734443665\n",
      "Epoch: 1, Batch: 748, Loss: 0.8843933343887329\n",
      "Epoch: 1, Batch: 749, Loss: 0.9099288582801819\n",
      "Epoch: 1, Batch: 750, Loss: 0.9165543913841248\n",
      "Epoch: 1, Batch: 751, Loss: 0.9472624659538269\n",
      "Epoch: 1, Batch: 752, Loss: 0.9509334564208984\n",
      "Epoch: 1, Batch: 753, Loss: 1.044508457183838\n",
      "Epoch: 1, Batch: 754, Loss: 1.0092136859893799\n",
      "Epoch: 1, Batch: 755, Loss: 1.0059477090835571\n",
      "Epoch: 1, Batch: 756, Loss: 0.9919485449790955\n",
      "Epoch: 1, Batch: 757, Loss: 1.0221667289733887\n",
      "Epoch: 1, Batch: 758, Loss: 1.0327963829040527\n",
      "Epoch: 1, Batch: 759, Loss: 1.0204905271530151\n",
      "Epoch: 1, Batch: 760, Loss: 1.0444992780685425\n",
      "Epoch: 1, Batch: 761, Loss: 0.8584213256835938\n",
      "Epoch: 1, Batch: 762, Loss: 0.9555936455726624\n",
      "Epoch: 1, Batch: 763, Loss: 0.94195556640625\n",
      "Epoch: 1, Batch: 764, Loss: 0.9908441305160522\n",
      "Epoch: 1, Batch: 765, Loss: 0.8547618389129639\n",
      "Epoch: 1, Batch: 766, Loss: 0.9747514128684998\n",
      "Epoch: 1, Batch: 767, Loss: 0.8729661107063293\n",
      "Epoch: 1, Batch: 768, Loss: 0.9464085698127747\n",
      "Epoch: 1, Batch: 769, Loss: 1.037553310394287\n",
      "Epoch: 1, Batch: 770, Loss: 1.0699551105499268\n",
      "Epoch: 1, Batch: 771, Loss: 0.9039092659950256\n",
      "Epoch: 1, Batch: 772, Loss: 0.9454680681228638\n",
      "Epoch: 1, Batch: 773, Loss: 0.9063229560852051\n",
      "Epoch: 1, Batch: 774, Loss: 1.1447632312774658\n",
      "Epoch: 1, Batch: 775, Loss: 0.873379647731781\n",
      "Epoch: 1, Batch: 776, Loss: 1.086845874786377\n",
      "Epoch: 1, Batch: 777, Loss: 0.976020336151123\n",
      "Epoch: 1, Batch: 778, Loss: 0.9276254177093506\n",
      "Epoch: 1, Batch: 779, Loss: 0.9061837196350098\n",
      "Epoch: 1, Batch: 780, Loss: 1.0085185766220093\n",
      "Epoch: 1, Batch: 781, Loss: 0.9967086315155029\n",
      "Epoch: 1, Batch: 782, Loss: 1.1444880962371826\n",
      "Epoch: 1, Batch: 783, Loss: 0.9103987216949463\n",
      "Epoch: 1, Batch: 784, Loss: 0.998597264289856\n",
      "Epoch: 1, Batch: 785, Loss: 1.073302149772644\n",
      "Epoch: 1, Batch: 786, Loss: 0.9895874261856079\n",
      "Epoch: 1, Batch: 787, Loss: 0.9940431714057922\n",
      "Epoch: 1, Batch: 788, Loss: 0.9838840365409851\n",
      "Epoch: 1, Batch: 789, Loss: 1.150597095489502\n",
      "Epoch: 1, Batch: 790, Loss: 1.0803731679916382\n",
      "Epoch: 1, Batch: 791, Loss: 0.9775151014328003\n",
      "Epoch: 1, Batch: 792, Loss: 0.8296380043029785\n",
      "Epoch: 1, Batch: 793, Loss: 0.8773192763328552\n",
      "Epoch: 1, Batch: 794, Loss: 1.0448880195617676\n",
      "Epoch: 1, Batch: 795, Loss: 0.9708164930343628\n",
      "Epoch: 1, Batch: 796, Loss: 0.9600507020950317\n",
      "Epoch: 1, Batch: 797, Loss: 1.050027847290039\n",
      "Epoch: 1, Batch: 798, Loss: 1.0236437320709229\n",
      "Epoch: 1, Batch: 799, Loss: 0.9202823638916016\n",
      "Epoch: 1, Batch: 800, Loss: 0.9621734619140625\n",
      "Epoch: 1, Batch: 801, Loss: 0.9787936806678772\n",
      "Epoch: 1, Batch: 802, Loss: 1.0498430728912354\n",
      "Epoch: 1, Batch: 803, Loss: 0.9040964841842651\n",
      "Epoch: 1, Batch: 804, Loss: 1.137804627418518\n",
      "Epoch: 1, Batch: 805, Loss: 1.0766241550445557\n",
      "Epoch: 1, Batch: 806, Loss: 1.028334379196167\n",
      "Epoch: 1, Batch: 807, Loss: 1.09967041015625\n",
      "Epoch: 1, Batch: 808, Loss: 1.1784288883209229\n",
      "Epoch: 1, Batch: 809, Loss: 0.9396272301673889\n",
      "Epoch: 1, Batch: 810, Loss: 1.0486953258514404\n",
      "Epoch: 1, Batch: 811, Loss: 1.134218692779541\n",
      "Epoch: 1, Batch: 812, Loss: 1.070892333984375\n",
      "Epoch: 1, Batch: 813, Loss: 0.9295036792755127\n",
      "Epoch: 1, Batch: 814, Loss: 0.9945507049560547\n",
      "Epoch: 1, Batch: 815, Loss: 1.024679183959961\n",
      "Epoch: 1, Batch: 816, Loss: 0.9357215762138367\n",
      "Epoch: 1, Batch: 817, Loss: 0.9599953889846802\n",
      "Epoch: 1, Batch: 818, Loss: 0.875869870185852\n",
      "Epoch: 1, Batch: 819, Loss: 0.9201914668083191\n",
      "Epoch: 1, Batch: 820, Loss: 1.0142333507537842\n",
      "Epoch: 1, Batch: 821, Loss: 1.043148398399353\n",
      "Epoch: 1, Batch: 822, Loss: 1.000145435333252\n",
      "Epoch: 1, Batch: 823, Loss: 0.8518209457397461\n",
      "Epoch: 1, Batch: 824, Loss: 0.8773804903030396\n",
      "Epoch: 1, Batch: 825, Loss: 1.0067291259765625\n",
      "Epoch: 1, Batch: 826, Loss: 1.0419301986694336\n",
      "Epoch: 1, Batch: 827, Loss: 0.9220802783966064\n",
      "Epoch: 1, Batch: 828, Loss: 0.9974349141120911\n",
      "Epoch: 1, Batch: 829, Loss: 1.018679141998291\n",
      "Epoch: 1, Batch: 830, Loss: 0.9222241640090942\n",
      "Epoch: 1, Batch: 831, Loss: 1.0288441181182861\n",
      "Epoch: 1, Batch: 832, Loss: 0.8910290598869324\n",
      "Epoch: 1, Batch: 833, Loss: 1.0435473918914795\n",
      "Epoch: 1, Batch: 834, Loss: 0.9438696503639221\n",
      "Epoch: 1, Batch: 835, Loss: 0.9631567001342773\n",
      "Epoch: 1, Batch: 836, Loss: 1.0387260913848877\n",
      "Epoch: 1, Batch: 837, Loss: 0.9842299818992615\n",
      "Epoch: 1, Batch: 838, Loss: 0.9258019924163818\n",
      "Epoch: 1, Batch: 839, Loss: 0.8973591327667236\n",
      "Epoch: 1, Batch: 840, Loss: 0.9087846875190735\n",
      "Epoch: 1, Batch: 841, Loss: 0.9054210186004639\n",
      "Epoch: 1, Batch: 842, Loss: 0.9177877306938171\n",
      "Epoch: 1, Batch: 843, Loss: 1.053514003753662\n",
      "Epoch: 1, Batch: 844, Loss: 1.0039005279541016\n",
      "Epoch: 1, Batch: 845, Loss: 1.0007414817810059\n",
      "Epoch: 1, Batch: 846, Loss: 1.1144092082977295\n",
      "Epoch: 1, Batch: 847, Loss: 0.974784255027771\n",
      "Epoch: 1, Batch: 848, Loss: 0.9288122057914734\n",
      "Epoch: 1, Batch: 849, Loss: 0.9634085297584534\n",
      "Epoch: 1, Batch: 850, Loss: 0.8797098398208618\n",
      "Epoch: 1, Batch: 851, Loss: 0.9564679265022278\n",
      "Epoch: 1, Batch: 852, Loss: 0.9661065936088562\n",
      "Epoch: 1, Batch: 853, Loss: 0.9585488438606262\n",
      "Epoch: 1, Batch: 854, Loss: 0.9085956811904907\n",
      "Epoch: 1, Batch: 855, Loss: 1.0153212547302246\n",
      "Epoch: 1, Batch: 856, Loss: 0.8954968452453613\n",
      "Epoch: 1, Batch: 857, Loss: 1.0501880645751953\n",
      "Epoch: 1, Batch: 858, Loss: 1.0009753704071045\n",
      "Epoch: 1, Batch: 859, Loss: 1.0625842809677124\n",
      "Epoch: 1, Batch: 860, Loss: 1.0849028825759888\n",
      "Epoch: 1, Batch: 861, Loss: 1.0891252756118774\n",
      "Epoch: 1, Batch: 862, Loss: 1.0538299083709717\n",
      "Epoch: 1, Batch: 863, Loss: 1.0343501567840576\n",
      "Epoch: 1, Batch: 864, Loss: 1.0985751152038574\n",
      "Epoch: 1, Batch: 865, Loss: 1.0579349994659424\n",
      "Epoch: 1, Batch: 866, Loss: 0.976893961429596\n",
      "Epoch: 1, Batch: 867, Loss: 0.9387168288230896\n",
      "Epoch: 1, Batch: 868, Loss: 0.9612070918083191\n",
      "Epoch: 1, Batch: 869, Loss: 1.1009314060211182\n",
      "Epoch: 1, Batch: 870, Loss: 0.9569470882415771\n",
      "Epoch: 1, Batch: 871, Loss: 0.8764608502388\n",
      "Epoch: 1, Batch: 872, Loss: 0.8768255710601807\n",
      "Epoch: 1, Batch: 873, Loss: 0.9634200930595398\n",
      "Epoch: 1, Batch: 874, Loss: 1.0729093551635742\n",
      "Epoch: 1, Batch: 875, Loss: 0.868370532989502\n",
      "Epoch: 1, Batch: 876, Loss: 0.8787624835968018\n",
      "Epoch: 1, Batch: 877, Loss: 1.0222094058990479\n",
      "Epoch: 1, Batch: 878, Loss: 1.0251224040985107\n",
      "Epoch: 1, Batch: 879, Loss: 1.0023632049560547\n",
      "Epoch: 1, Batch: 880, Loss: 0.9063000679016113\n",
      "Epoch: 1, Batch: 881, Loss: 1.0085564851760864\n",
      "Epoch: 1, Batch: 882, Loss: 0.9382590055465698\n",
      "Epoch: 1, Batch: 883, Loss: 1.022463321685791\n",
      "Epoch: 1, Batch: 884, Loss: 0.9935001134872437\n",
      "Epoch: 1, Batch: 885, Loss: 1.074906587600708\n",
      "Epoch: 1, Batch: 886, Loss: 1.0336885452270508\n",
      "Epoch: 1, Batch: 887, Loss: 1.0691862106323242\n",
      "Epoch: 1, Batch: 888, Loss: 1.0343538522720337\n",
      "Epoch: 1, Batch: 889, Loss: 0.9964050054550171\n",
      "Epoch: 1, Batch: 890, Loss: 1.0514233112335205\n",
      "Epoch: 1, Batch: 891, Loss: 0.9378450512886047\n",
      "Epoch: 1, Batch: 892, Loss: 1.0294203758239746\n",
      "Epoch: 1, Batch: 893, Loss: 1.010023832321167\n",
      "Epoch: 1, Batch: 894, Loss: 1.0398379564285278\n",
      "Epoch: 1, Batch: 895, Loss: 1.1231005191802979\n",
      "Epoch: 1, Batch: 896, Loss: 0.9078644514083862\n",
      "Epoch: 1, Batch: 897, Loss: 0.8890973329544067\n",
      "Epoch: 1, Batch: 898, Loss: 0.985821008682251\n",
      "Epoch: 1, Batch: 899, Loss: 1.0393623113632202\n",
      "Epoch: 1, Batch: 900, Loss: 0.849572479724884\n",
      "Epoch: 1, Batch: 901, Loss: 1.0206832885742188\n",
      "Epoch: 1, Batch: 902, Loss: 0.8925648927688599\n",
      "Epoch: 1, Batch: 903, Loss: 0.9115921258926392\n",
      "Epoch: 1, Batch: 904, Loss: 0.9812173843383789\n",
      "Epoch: 1, Batch: 905, Loss: 0.9095443487167358\n",
      "Epoch: 1, Batch: 906, Loss: 1.1045479774475098\n",
      "Epoch: 1, Batch: 907, Loss: 1.1244635581970215\n",
      "Epoch: 1, Batch: 908, Loss: 1.0668511390686035\n",
      "Epoch: 1, Batch: 909, Loss: 1.1368833780288696\n",
      "Epoch: 1, Batch: 910, Loss: 1.0441536903381348\n",
      "Epoch: 1, Batch: 911, Loss: 0.9894405603408813\n",
      "Epoch: 1, Batch: 912, Loss: 1.0335724353790283\n",
      "Epoch: 1, Batch: 913, Loss: 1.0090892314910889\n",
      "Epoch: 1, Batch: 914, Loss: 1.078981876373291\n",
      "Epoch: 1, Batch: 915, Loss: 0.9951707124710083\n",
      "Epoch: 1, Batch: 916, Loss: 0.9418118000030518\n",
      "Epoch: 1, Batch: 917, Loss: 1.0334209203720093\n",
      "Epoch: 1, Batch: 918, Loss: 0.9571393728256226\n",
      "Epoch: 1, Batch: 919, Loss: 0.996731162071228\n",
      "Epoch: 1, Batch: 920, Loss: 1.110910177230835\n",
      "Epoch: 1, Batch: 921, Loss: 0.97840815782547\n",
      "Epoch: 1, Batch: 922, Loss: 1.0532481670379639\n",
      "Epoch: 1, Batch: 923, Loss: 0.8656423091888428\n",
      "Epoch: 1, Batch: 924, Loss: 0.8717292547225952\n",
      "Epoch: 1, Batch: 925, Loss: 1.010255217552185\n",
      "Epoch: 1, Batch: 926, Loss: 1.021257996559143\n",
      "Epoch: 1, Batch: 927, Loss: 1.0958878993988037\n",
      "Epoch: 1, Batch: 928, Loss: 1.0043550729751587\n",
      "Epoch: 1, Batch: 929, Loss: 0.9909529089927673\n",
      "Epoch: 1, Batch: 930, Loss: 0.9450160264968872\n",
      "Epoch: 1, Batch: 931, Loss: 0.8644101023674011\n",
      "Epoch: 1, Batch: 932, Loss: 1.006686806678772\n",
      "Epoch: 1, Batch: 933, Loss: 1.0213276147842407\n",
      "Epoch: 1, Batch: 934, Loss: 0.9702498316764832\n",
      "Epoch: 1, Batch: 935, Loss: 0.9586181640625\n",
      "Epoch: 1, Batch: 936, Loss: 0.9386000037193298\n",
      "Epoch: 1, Batch: 937, Loss: 1.0091980695724487\n",
      "Epoch: 1, Batch: 938, Loss: 1.023728370666504\n",
      "Epoch: 1, Batch: 939, Loss: 0.9700645208358765\n",
      "Epoch: 1, Batch: 940, Loss: 1.0107648372650146\n",
      "Epoch: 1, Batch: 941, Loss: 0.9270038604736328\n",
      "Epoch: 1, Batch: 942, Loss: 0.9696667194366455\n",
      "Epoch: 1, Batch: 943, Loss: 0.9963994026184082\n",
      "Epoch: 1, Batch: 944, Loss: 0.887537956237793\n",
      "Epoch: 1, Batch: 945, Loss: 1.0278825759887695\n",
      "Epoch: 1, Batch: 946, Loss: 1.1338468790054321\n",
      "Epoch: 1, Batch: 947, Loss: 0.8415287733078003\n",
      "Epoch: 1, Batch: 948, Loss: 0.9583476185798645\n",
      "Epoch: 1, Batch: 949, Loss: 0.9573086500167847\n",
      "Epoch: 1, Batch: 950, Loss: 1.0573365688323975\n",
      "Epoch: 1, Batch: 951, Loss: 0.9449176788330078\n",
      "Epoch: 1, Batch: 952, Loss: 0.9413565993309021\n",
      "Epoch: 1, Batch: 953, Loss: 0.8796001672744751\n",
      "Epoch: 1, Batch: 954, Loss: 0.9838395118713379\n",
      "Epoch: 1, Batch: 955, Loss: 0.9545069932937622\n",
      "Epoch: 1, Batch: 956, Loss: 0.987521767616272\n",
      "Epoch: 1, Batch: 957, Loss: 0.9615148901939392\n",
      "Epoch: 1, Batch: 958, Loss: 0.9390978217124939\n",
      "Epoch: 1, Batch: 959, Loss: 1.0640578269958496\n",
      "Epoch: 1, Batch: 960, Loss: 0.9766730070114136\n",
      "Epoch: 1, Batch: 961, Loss: 0.9719169735908508\n",
      "Epoch: 1, Batch: 962, Loss: 0.8347871899604797\n",
      "Epoch: 1, Batch: 963, Loss: 0.9730821251869202\n",
      "Epoch: 1, Batch: 964, Loss: 0.9583762884140015\n",
      "Epoch: 1, Batch: 965, Loss: 1.1004785299301147\n",
      "Epoch: 1, Batch: 966, Loss: 1.0031712055206299\n",
      "Epoch: 1, Batch: 967, Loss: 1.0619173049926758\n",
      "Epoch: 1, Batch: 968, Loss: 0.9038791656494141\n",
      "Epoch: 1, Batch: 969, Loss: 0.8639627695083618\n",
      "Epoch: 1, Batch: 970, Loss: 0.8560445308685303\n",
      "Epoch: 1, Batch: 971, Loss: 1.0746034383773804\n",
      "Epoch: 1, Batch: 972, Loss: 0.9513450264930725\n",
      "Epoch: 1, Batch: 973, Loss: 0.9829332828521729\n",
      "Epoch: 1, Batch: 974, Loss: 1.0389988422393799\n",
      "Epoch: 1, Batch: 975, Loss: 0.9746984839439392\n",
      "Epoch: 1, Batch: 976, Loss: 1.0170217752456665\n",
      "Epoch: 1, Batch: 977, Loss: 0.9659326076507568\n",
      "Epoch: 1, Batch: 978, Loss: 1.0536015033721924\n",
      "Epoch: 1, Batch: 979, Loss: 1.0385833978652954\n",
      "Epoch: 1, Batch: 980, Loss: 0.9796400666236877\n",
      "Epoch: 1, Batch: 981, Loss: 0.9577463865280151\n",
      "Epoch: 1, Batch: 982, Loss: 0.9441015720367432\n",
      "Epoch: 1, Batch: 983, Loss: 1.0819711685180664\n",
      "Epoch: 1, Batch: 984, Loss: 0.9520212411880493\n",
      "Epoch: 1, Batch: 985, Loss: 1.0194852352142334\n",
      "Epoch: 1, Batch: 986, Loss: 1.077484369277954\n",
      "Epoch: 1, Batch: 987, Loss: 1.0486403703689575\n",
      "Epoch: 1, Batch: 988, Loss: 0.9635517597198486\n",
      "Epoch: 1, Batch: 989, Loss: 0.8786125183105469\n",
      "Epoch: 1, Batch: 990, Loss: 0.8873897790908813\n",
      "Epoch: 1, Batch: 991, Loss: 0.9492814540863037\n",
      "Epoch: 1, Batch: 992, Loss: 1.0131564140319824\n",
      "Epoch: 1, Batch: 993, Loss: 0.9525845050811768\n",
      "Epoch: 1, Batch: 994, Loss: 0.9529564380645752\n",
      "Epoch: 1, Batch: 995, Loss: 1.0589520931243896\n",
      "Epoch: 1, Batch: 996, Loss: 1.0090028047561646\n",
      "Epoch: 1, Batch: 997, Loss: 0.8945308923721313\n",
      "Epoch: 1, Batch: 998, Loss: 1.1537506580352783\n",
      "Epoch: 1, Batch: 999, Loss: 0.9763894081115723\n",
      "Epoch: 1, Batch: 1000, Loss: 0.972134530544281\n",
      "Epoch: 1, Batch: 1001, Loss: 0.9017109274864197\n",
      "Epoch: 1, Batch: 1002, Loss: 1.0177526473999023\n",
      "Epoch: 1, Batch: 1003, Loss: 1.0399961471557617\n",
      "Epoch: 1, Batch: 1004, Loss: 0.9124799966812134\n",
      "Epoch: 1, Batch: 1005, Loss: 0.943526566028595\n",
      "Epoch: 1, Batch: 1006, Loss: 1.05245041847229\n",
      "Epoch: 1, Batch: 1007, Loss: 0.9526957869529724\n",
      "Epoch: 1, Batch: 1008, Loss: 1.0618983507156372\n",
      "Epoch: 1, Batch: 1009, Loss: 1.0168133974075317\n",
      "Epoch: 1, Batch: 1010, Loss: 1.0136369466781616\n",
      "Epoch: 1, Batch: 1011, Loss: 0.9800676107406616\n",
      "Epoch: 1, Batch: 1012, Loss: 0.9855223894119263\n",
      "Epoch: 1, Batch: 1013, Loss: 0.954814076423645\n",
      "Epoch: 1, Batch: 1014, Loss: 0.9395570755004883\n",
      "Epoch: 1, Batch: 1015, Loss: 0.868095338344574\n",
      "Epoch: 1, Batch: 1016, Loss: 0.8437380790710449\n",
      "Epoch: 1, Batch: 1017, Loss: 1.1035410165786743\n",
      "Epoch: 1, Batch: 1018, Loss: 0.9256821870803833\n",
      "Epoch: 1, Batch: 1019, Loss: 0.9082857370376587\n",
      "Epoch: 1, Batch: 1020, Loss: 0.9097428321838379\n",
      "Epoch: 1, Batch: 1021, Loss: 0.7855685949325562\n",
      "Epoch: 1, Batch: 1022, Loss: 0.9429295063018799\n",
      "Epoch: 1, Batch: 1023, Loss: 0.8687855005264282\n",
      "Epoch: 1, Batch: 1024, Loss: 1.0255006551742554\n",
      "Epoch: 1, Batch: 1025, Loss: 1.0190012454986572\n",
      "Epoch: 1, Batch: 1026, Loss: 0.9786722660064697\n",
      "Epoch: 1, Batch: 1027, Loss: 1.0293327569961548\n",
      "Epoch: 1, Batch: 1028, Loss: 0.9715988039970398\n",
      "Epoch: 1, Batch: 1029, Loss: 1.0235856771469116\n",
      "Epoch: 1, Batch: 1030, Loss: 1.0496745109558105\n",
      "Epoch: 1, Batch: 1031, Loss: 1.0843453407287598\n",
      "Epoch: 1, Batch: 1032, Loss: 1.057070016860962\n",
      "Epoch: 1, Batch: 1033, Loss: 0.9855117797851562\n",
      "Epoch: 1, Batch: 1034, Loss: 1.0620465278625488\n",
      "Epoch: 1, Batch: 1035, Loss: 1.0030245780944824\n",
      "Epoch: 1, Batch: 1036, Loss: 0.9364657998085022\n",
      "Epoch: 1, Batch: 1037, Loss: 0.9307212829589844\n",
      "Epoch: 1, Batch: 1038, Loss: 0.9783380031585693\n",
      "Epoch: 1, Batch: 1039, Loss: 1.0530370473861694\n",
      "Epoch: 1, Batch: 1040, Loss: 0.775232195854187\n",
      "Epoch: 1, Batch: 1041, Loss: 0.9031796455383301\n",
      "Epoch: 1, Batch: 1042, Loss: 1.0039862394332886\n",
      "Epoch: 1, Batch: 1043, Loss: 0.8467302322387695\n",
      "Epoch: 1, Batch: 1044, Loss: 1.0125740766525269\n",
      "Epoch: 1, Batch: 1045, Loss: 0.9963099360466003\n",
      "Epoch: 1, Batch: 1046, Loss: 0.9476606845855713\n",
      "Epoch: 1, Batch: 1047, Loss: 1.0802279710769653\n",
      "Epoch: 1, Batch: 1048, Loss: 0.9933902621269226\n",
      "Epoch: 1, Batch: 1049, Loss: 1.1536808013916016\n",
      "Epoch: 1, Batch: 1050, Loss: 0.9783692955970764\n",
      "Epoch: 1, Batch: 1051, Loss: 0.9058936834335327\n",
      "Epoch: 1, Batch: 1052, Loss: 1.1229279041290283\n",
      "Epoch: 1, Batch: 1053, Loss: 1.0645496845245361\n",
      "Epoch: 1, Batch: 1054, Loss: 0.9093115925788879\n",
      "Epoch: 1, Batch: 1055, Loss: 1.0133848190307617\n",
      "Epoch: 1, Batch: 1056, Loss: 1.013471245765686\n",
      "Epoch: 1, Batch: 1057, Loss: 1.0327421426773071\n",
      "Epoch: 1, Batch: 1058, Loss: 0.9960219860076904\n",
      "Epoch: 1, Batch: 1059, Loss: 1.2573521137237549\n",
      "Epoch: 1, Batch: 1060, Loss: 0.9429451823234558\n",
      "Epoch: 1, Batch: 1061, Loss: 0.9747021794319153\n",
      "Epoch: 1, Batch: 1062, Loss: 0.8566285371780396\n",
      "Epoch: 1, Batch: 1063, Loss: 0.9242090582847595\n",
      "Epoch: 1, Batch: 1064, Loss: 1.0097978115081787\n",
      "Epoch: 1, Batch: 1065, Loss: 0.9717377424240112\n",
      "Epoch: 1, Batch: 1066, Loss: 1.0001027584075928\n",
      "Epoch: 1, Batch: 1067, Loss: 1.1690539121627808\n",
      "Epoch: 1, Batch: 1068, Loss: 0.972652018070221\n",
      "Epoch: 1, Batch: 1069, Loss: 0.9754224419593811\n",
      "Epoch: 1, Batch: 1070, Loss: 0.9760524034500122\n",
      "Epoch: 1, Batch: 1071, Loss: 0.9106253981590271\n",
      "Epoch: 1, Batch: 1072, Loss: 1.0767747163772583\n",
      "Epoch: 1, Batch: 1073, Loss: 1.0543828010559082\n",
      "Epoch: 1, Batch: 1074, Loss: 0.9309374690055847\n",
      "Epoch: 1, Batch: 1075, Loss: 0.8905822038650513\n",
      "Epoch: 1, Batch: 1076, Loss: 1.1433978080749512\n",
      "Epoch: 1, Batch: 1077, Loss: 0.8648965358734131\n",
      "Epoch: 1, Batch: 1078, Loss: 1.0214550495147705\n",
      "Epoch: 1, Batch: 1079, Loss: 0.9037538766860962\n",
      "Epoch: 1, Batch: 1080, Loss: 0.9645174741744995\n",
      "Epoch: 1, Batch: 1081, Loss: 1.027546763420105\n",
      "Epoch: 1, Batch: 1082, Loss: 1.0617258548736572\n",
      "Epoch: 1, Batch: 1083, Loss: 1.0856633186340332\n",
      "Epoch: 1, Batch: 1084, Loss: 1.0189769268035889\n",
      "Epoch: 1, Batch: 1085, Loss: 1.0270178318023682\n",
      "Epoch: 1, Batch: 1086, Loss: 0.9748831987380981\n",
      "Epoch: 1, Batch: 1087, Loss: 1.0868439674377441\n",
      "Epoch: 1, Batch: 1088, Loss: 0.9047431945800781\n",
      "Epoch: 1, Batch: 1089, Loss: 0.9849935173988342\n",
      "Epoch: 1, Batch: 1090, Loss: 0.8874210715293884\n",
      "Epoch: 1, Batch: 1091, Loss: 0.9734199047088623\n",
      "Epoch: 1, Batch: 1092, Loss: 0.9415583610534668\n",
      "Epoch: 1, Batch: 1093, Loss: 0.8993616104125977\n",
      "Epoch: 1, Batch: 1094, Loss: 0.906830370426178\n",
      "Epoch: 1, Batch: 1095, Loss: 0.845487117767334\n",
      "Epoch: 1, Batch: 1096, Loss: 0.793645977973938\n",
      "Epoch: 1, Batch: 1097, Loss: 1.0589005947113037\n",
      "Epoch: 1, Batch: 1098, Loss: 0.9329096078872681\n",
      "Epoch: 1, Batch: 1099, Loss: 0.922258734703064\n",
      "Epoch: 1, Batch: 1100, Loss: 0.966755747795105\n",
      "Epoch: 1, Batch: 1101, Loss: 1.09548020362854\n",
      "Epoch: 1, Batch: 1102, Loss: 0.936031699180603\n",
      "Epoch: 1, Batch: 1103, Loss: 0.8542792797088623\n",
      "Epoch: 1, Batch: 1104, Loss: 0.949823260307312\n",
      "Epoch: 1, Batch: 1105, Loss: 1.076594352722168\n",
      "Epoch: 1, Batch: 1106, Loss: 0.942186713218689\n",
      "Epoch: 1, Batch: 1107, Loss: 0.9808009266853333\n",
      "Epoch: 1, Batch: 1108, Loss: 1.0399436950683594\n",
      "Epoch: 1, Batch: 1109, Loss: 1.0498480796813965\n",
      "Epoch: 1, Batch: 1110, Loss: 0.8842759132385254\n",
      "Epoch: 1, Batch: 1111, Loss: 1.0864980220794678\n",
      "Epoch: 1, Batch: 1112, Loss: 0.9716565608978271\n",
      "Epoch: 1, Batch: 1113, Loss: 0.8567356467247009\n",
      "Epoch: 1, Batch: 1114, Loss: 0.9703445434570312\n",
      "Epoch: 1, Batch: 1115, Loss: 0.9840294718742371\n",
      "Epoch: 1, Batch: 1116, Loss: 0.9727209806442261\n",
      "Epoch: 1, Batch: 1117, Loss: 0.951486349105835\n",
      "Epoch: 1, Batch: 1118, Loss: 1.0477266311645508\n",
      "Epoch: 1, Batch: 1119, Loss: 1.0428876876831055\n",
      "Epoch: 1, Batch: 1120, Loss: 0.8861895799636841\n",
      "Epoch: 1, Batch: 1121, Loss: 0.880618691444397\n",
      "Epoch: 1, Batch: 1122, Loss: 0.8799067735671997\n",
      "Epoch: 1, Batch: 1123, Loss: 0.9846853613853455\n",
      "Epoch: 1, Batch: 1124, Loss: 1.0173289775848389\n",
      "Epoch: 1, Batch: 1125, Loss: 1.1424518823623657\n",
      "Epoch: 1, Batch: 1126, Loss: 1.052088737487793\n",
      "Epoch: 1, Batch: 1127, Loss: 1.0108076333999634\n",
      "Epoch: 1, Batch: 1128, Loss: 0.9276647567749023\n",
      "Epoch: 1, Batch: 1129, Loss: 0.983430802822113\n",
      "Epoch: 1, Batch: 1130, Loss: 0.9221992492675781\n",
      "Epoch: 1, Batch: 1131, Loss: 0.9619601964950562\n",
      "Epoch: 1, Batch: 1132, Loss: 0.9122943878173828\n",
      "Epoch: 1, Batch: 1133, Loss: 0.9584922790527344\n",
      "Epoch: 1, Batch: 1134, Loss: 0.9773864150047302\n",
      "Epoch: 1, Batch: 1135, Loss: 1.0072150230407715\n",
      "Epoch: 1, Batch: 1136, Loss: 0.9981188774108887\n",
      "Epoch: 1, Batch: 1137, Loss: 1.0115774869918823\n",
      "Epoch: 1, Batch: 1138, Loss: 1.0199074745178223\n",
      "Epoch: 1, Batch: 1139, Loss: 0.9921557307243347\n",
      "Epoch: 1, Batch: 1140, Loss: 1.017772912979126\n",
      "Epoch: 1, Batch: 1141, Loss: 0.8856037855148315\n",
      "Epoch: 1, Batch: 1142, Loss: 1.0825941562652588\n",
      "Epoch: 1, Batch: 1143, Loss: 0.8903804421424866\n",
      "Epoch: 1, Batch: 1144, Loss: 0.8455724716186523\n",
      "Epoch: 1, Batch: 1145, Loss: 1.0697319507598877\n",
      "Epoch: 1, Batch: 1146, Loss: 0.9278848171234131\n",
      "Epoch: 1, Batch: 1147, Loss: 0.9903594255447388\n",
      "Epoch: 1, Batch: 1148, Loss: 0.9698899984359741\n",
      "Epoch: 1, Batch: 1149, Loss: 1.0486221313476562\n",
      "Epoch: 1, Batch: 1150, Loss: 1.0091872215270996\n",
      "Epoch: 1, Batch: 1151, Loss: 0.9300734400749207\n",
      "Epoch: 1, Batch: 1152, Loss: 0.9884042739868164\n",
      "Epoch: 1, Batch: 1153, Loss: 0.9716119170188904\n",
      "Epoch: 1, Batch: 1154, Loss: 1.0308760404586792\n",
      "Epoch: 1, Batch: 1155, Loss: 0.967134952545166\n",
      "Epoch: 1, Batch: 1156, Loss: 1.0195859670639038\n",
      "Epoch: 1, Batch: 1157, Loss: 0.8663691282272339\n",
      "Epoch: 1, Batch: 1158, Loss: 0.9342371821403503\n",
      "Epoch: 1, Batch: 1159, Loss: 1.1700631380081177\n",
      "Epoch: 1, Batch: 1160, Loss: 0.9470394253730774\n",
      "Epoch: 1, Batch: 1161, Loss: 0.9470605850219727\n",
      "Epoch: 1, Batch: 1162, Loss: 1.0516762733459473\n",
      "Epoch: 1, Batch: 1163, Loss: 0.9826797246932983\n",
      "Epoch: 1, Batch: 1164, Loss: 1.0197632312774658\n",
      "Epoch: 1, Batch: 1165, Loss: 0.9782897233963013\n",
      "Epoch: 1, Batch: 1166, Loss: 0.9987580180168152\n",
      "Epoch: 1, Batch: 1167, Loss: 0.9096800088882446\n",
      "Epoch: 1, Batch: 1168, Loss: 1.0149106979370117\n",
      "Epoch: 1, Batch: 1169, Loss: 0.9578520059585571\n",
      "Epoch: 1, Batch: 1170, Loss: 0.989607036113739\n",
      "Epoch: 1, Batch: 1171, Loss: 0.9400508999824524\n",
      "Epoch: 1, Batch: 1172, Loss: 1.0880926847457886\n",
      "Epoch: 1, Batch: 1173, Loss: 1.0487719774246216\n",
      "Epoch: 1, Batch: 1174, Loss: 0.937127947807312\n",
      "Epoch: 1, Batch: 1175, Loss: 1.0550017356872559\n",
      "Epoch: 1, Batch: 1176, Loss: 0.9344368577003479\n",
      "Epoch: 1, Batch: 1177, Loss: 1.1225764751434326\n",
      "Epoch: 1, Batch: 1178, Loss: 1.0063856840133667\n",
      "Epoch: 1, Batch: 1179, Loss: 0.9698087573051453\n",
      "Epoch: 1, Batch: 1180, Loss: 0.8713829517364502\n",
      "Epoch: 1, Batch: 1181, Loss: 0.8887187242507935\n",
      "Epoch: 1, Batch: 1182, Loss: 0.998962938785553\n",
      "Epoch: 1, Batch: 1183, Loss: 1.0468100309371948\n",
      "Epoch: 1, Batch: 1184, Loss: 0.9772096276283264\n",
      "Epoch: 1, Batch: 1185, Loss: 0.902529239654541\n",
      "Epoch: 1, Batch: 1186, Loss: 0.9024347066879272\n",
      "Epoch: 1, Batch: 1187, Loss: 1.0154651403427124\n",
      "Epoch: 1, Batch: 1188, Loss: 1.077512502670288\n",
      "Epoch: 1, Batch: 1189, Loss: 1.0368555784225464\n",
      "Epoch: 1, Batch: 1190, Loss: 1.0956497192382812\n",
      "Epoch: 1, Batch: 1191, Loss: 0.9330890774726868\n",
      "Epoch: 1, Batch: 1192, Loss: 1.0390584468841553\n",
      "Epoch: 1, Batch: 1193, Loss: 1.0116106271743774\n",
      "Epoch: 1, Batch: 1194, Loss: 1.0383230447769165\n",
      "Epoch: 1, Batch: 1195, Loss: 0.9842763543128967\n",
      "Epoch: 1, Batch: 1196, Loss: 1.032371997833252\n",
      "Epoch: 1, Batch: 1197, Loss: 1.01234769821167\n",
      "Epoch: 1, Batch: 1198, Loss: 1.015113353729248\n",
      "Epoch: 1, Batch: 1199, Loss: 0.8489948511123657\n",
      "Epoch: 1, Batch: 1200, Loss: 0.8305128216743469\n",
      "Epoch: 1, Batch: 1201, Loss: 0.9565464854240417\n",
      "Epoch: 1, Batch: 1202, Loss: 1.0353643894195557\n",
      "Epoch: 1, Batch: 1203, Loss: 0.9188767671585083\n",
      "Epoch: 1, Batch: 1204, Loss: 1.058064341545105\n",
      "Epoch: 1, Batch: 1205, Loss: 0.8796271681785583\n",
      "Epoch: 1, Batch: 1206, Loss: 0.9529531002044678\n",
      "Epoch: 1, Batch: 1207, Loss: 0.8408812880516052\n",
      "Epoch: 1, Batch: 1208, Loss: 0.9095137119293213\n",
      "Epoch: 1, Batch: 1209, Loss: 0.9970327615737915\n",
      "Epoch: 1, Batch: 1210, Loss: 0.9315406084060669\n",
      "Epoch: 1, Batch: 1211, Loss: 0.9286278486251831\n",
      "Epoch: 1, Batch: 1212, Loss: 1.034551739692688\n",
      "Epoch: 1, Batch: 1213, Loss: 1.0455416440963745\n",
      "Epoch: 1, Batch: 1214, Loss: 1.0199096202850342\n",
      "Epoch: 1, Batch: 1215, Loss: 0.9017415046691895\n",
      "Epoch: 1, Batch: 1216, Loss: 0.9111635088920593\n",
      "Epoch: 1, Batch: 1217, Loss: 1.1651737689971924\n",
      "Epoch: 1, Batch: 1218, Loss: 1.030752182006836\n",
      "Epoch: 1, Batch: 1219, Loss: 0.8919442296028137\n",
      "Epoch: 1, Batch: 1220, Loss: 1.004044771194458\n",
      "Epoch: 1, Batch: 1221, Loss: 1.0828299522399902\n",
      "Epoch: 1, Batch: 1222, Loss: 1.0638197660446167\n",
      "Epoch: 1, Batch: 1223, Loss: 1.0583977699279785\n",
      "Epoch: 1, Batch: 1224, Loss: 1.0555884838104248\n",
      "Epoch: 1, Batch: 1225, Loss: 0.9287842512130737\n",
      "Epoch: 1, Batch: 1226, Loss: 0.9975110292434692\n",
      "Epoch: 1, Batch: 1227, Loss: 0.9002083539962769\n",
      "Epoch: 1, Batch: 1228, Loss: 0.8255629539489746\n",
      "Epoch: 1, Batch: 1229, Loss: 0.9997050762176514\n",
      "Epoch: 1, Batch: 1230, Loss: 0.8909759521484375\n",
      "Epoch: 1, Batch: 1231, Loss: 0.9154434204101562\n",
      "Epoch: 1, Batch: 1232, Loss: 0.902019739151001\n",
      "Epoch: 1, Batch: 1233, Loss: 0.9881455898284912\n",
      "Epoch: 1, Batch: 1234, Loss: 1.0054011344909668\n",
      "Epoch: 1, Batch: 1235, Loss: 0.9200294613838196\n",
      "Epoch: 1, Batch: 1236, Loss: 0.8842517137527466\n",
      "Epoch: 1, Batch: 1237, Loss: 1.0660192966461182\n",
      "Epoch: 1, Batch: 1238, Loss: 0.9739579558372498\n",
      "Epoch: 1, Batch: 1239, Loss: 0.9700136184692383\n",
      "Epoch: 1, Batch: 1240, Loss: 1.0007789134979248\n",
      "Epoch: 1, Batch: 1241, Loss: 0.8772765398025513\n",
      "Epoch: 1, Batch: 1242, Loss: 1.0525453090667725\n",
      "Epoch: 1, Batch: 1243, Loss: 0.9453992247581482\n",
      "Epoch: 1, Batch: 1244, Loss: 1.0297139883041382\n",
      "Epoch: 1, Batch: 1245, Loss: 0.9768414497375488\n",
      "Epoch: 1, Batch: 1246, Loss: 0.9236599206924438\n",
      "Epoch: 1, Batch: 1247, Loss: 1.049176573753357\n",
      "Epoch: 1, Batch: 1248, Loss: 0.9520416259765625\n",
      "Epoch: 1, Batch: 1249, Loss: 0.9860334992408752\n",
      "Epoch: 1, Batch: 1250, Loss: 0.9945448637008667\n",
      "Epoch: 1, Batch: 1251, Loss: 0.941847026348114\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10    # number of epochs to run\n",
    "batch_size = 512  # size of each batch\n",
    "batches_per_epoch = user_ids_train.shape[0] // batch_size\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(rec.parameters(), lr=0.001)\n",
    "lr_scheduler = CosineWarmupScheduler(optimizer, warmup=50, max_iters=batches_per_epoch*n_epochs)\n",
    "\n",
    "rec.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    indices = torch.randperm(user_ids_train.shape[0])\n",
    "\n",
    "    for i in range(batches_per_epoch):\n",
    "        optimizer.zero_grad()\n",
    "        start = i * batch_size\n",
    "        batch_indices = indices[start:start+batch_size]\n",
    "\n",
    "        user_ids_batch = user_ids_train[batch_indices].unsqueeze(1).to(device=device)\n",
    "        movie_ids_batch = movie_ids_train[batch_indices].unsqueeze(1).to(device=device)\n",
    "        genres_batch = genres_train[batch_indices].unsqueeze(1).to(dtype=torch.float32).to(device=device)\n",
    "        years_batch = years_train[batch_indices].unsqueeze(1).to(device=device)\n",
    "        ratings_batch = ratings_train[batch_indices].unsqueeze(1).to(device=device)\n",
    "\n",
    "        prev_movie_ids_batch = prev_movie_ids_train[batch_indices].to(device=device)\n",
    "        prev_movie_genres_batch = prev_movie_genres_train[batch_indices].to(dtype=torch.float32).to(device=device)\n",
    "        prev_movie_ids_years = prev_movie_years_train[batch_indices].to(device=device)\n",
    "        prev_movie_ids_ratings = prev_movie_ratings_train[batch_indices].to(device=device)\n",
    "\n",
    "        output:torch.Tensor = \\\n",
    "            rec(\n",
    "                user_ids_batch, \n",
    "                movie_ids_batch, \n",
    "                prev_movie_ids_batch, \n",
    "                prev_movie_genres_batch, \n",
    "                prev_movie_ids_years, \n",
    "                prev_movie_ids_ratings, \n",
    "                genres_batch, \n",
    "                years_batch\n",
    "            )\n",
    "        \n",
    "        loss:torch.Tensor = criterion(output.contiguous(), ratings_batch.contiguous())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4220, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(2.4288, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(2.4307, device='mps:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     18\u001b[39m prev_movie_ids_ratings = prev_movie_ratings_test[batch_indices].to(device=device)\n\u001b[32m     20\u001b[39m output:torch.Tensor = \\\n\u001b[32m     21\u001b[39m     rec(\n\u001b[32m     22\u001b[39m         user_ids_batch, \n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m         years_batch\n\u001b[32m     30\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m g = \u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m-\u001b[49m\u001b[43mratings_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.sum()/output.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(g)\n\u001b[32m     34\u001b[39m s += g\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "rec.eval()\n",
    "\n",
    "batch_size = 1000\n",
    "s = 0.0\n",
    "\n",
    "for i in range(user_ids_test.shape[0]):\n",
    "    batch_indices = list(range(i, i+batch_size))\n",
    "\n",
    "    user_ids_batch = user_ids_test[batch_indices].unsqueeze(1).to(device=device)\n",
    "    movie_ids_batch = movie_ids_test[batch_indices].unsqueeze(1).to(device=device)\n",
    "    genres_batch = genres_test[batch_indices].unsqueeze(1).to(dtype=torch.float32).to(device=device)\n",
    "    years_batch = years_test[batch_indices].unsqueeze(1).to(device=device)\n",
    "    ratings_batch = ratings_test[batch_indices].unsqueeze(1).to(device=device)\n",
    "\n",
    "    prev_movie_ids_batch = prev_movie_ids_test[batch_indices].to(device=device)\n",
    "    prev_movie_genres_batch = prev_movie_genres_test[batch_indices].to(dtype=torch.float32).to(device=device)\n",
    "    prev_movie_ids_years = prev_movie_years_test[batch_indices].to(device=device)\n",
    "    prev_movie_ids_ratings = prev_movie_ratings_test[batch_indices].to(device=device)\n",
    "\n",
    "    output:torch.Tensor = \\\n",
    "        rec(\n",
    "            user_ids_batch, \n",
    "            movie_ids_batch, \n",
    "            prev_movie_ids_batch, \n",
    "            prev_movie_genres_batch, \n",
    "            prev_movie_ids_years, \n",
    "            prev_movie_ids_ratings, \n",
    "            genres_batch, \n",
    "            years_batch\n",
    "        )\n",
    "    \n",
    "    g = (output-ratings_batch).square().sum()/output.shape[0]\n",
    "    print(g)\n",
    "    s += g\n",
    "\n",
    "    if device == 'mps':\n",
    "        torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(s/user_ids_test.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
